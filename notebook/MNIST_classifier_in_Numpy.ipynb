{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_classifier_in_Numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lXsZ4JOu53-",
        "colab_type": "text"
      },
      "source": [
        "##**IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZxU1U-kiYux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruJZ96RSvIrq",
        "colab_type": "text"
      },
      "source": [
        "##**READING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6W2UZCsh3JC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1b59b79-bf5f-482b-d2ab-0acc0ced5a5f"
      },
      "source": [
        "!unzip '/content/mnist.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/mnist.zip\n",
            "  inflating: mnist_test.csv          \n",
            "  inflating: mnist_train.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pci4sPCGioTT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77b1e773-e303-415e-d467-1745fc1a25ad"
      },
      "source": [
        "train = pd.read_csv('/content/mnist_train.csv')\n",
        "test = pd.read_csv('/content/mnist_test.csv')\n",
        "\n",
        "X_train= (train.drop(['label'], axis=1).values)/255\n",
        "X_test= (test.drop(['label'], axis=1).values)/255\n",
        "y_train= train.loc[:, 'label']\n",
        "y_test= test.loc[:, 'label'].values\n",
        "\n",
        "print(f'Number of training examples: {X_train.shape[0]}')\n",
        "print(f'Number of testing examples: {X_test.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 60000\n",
            "Number of testing examples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljfa_fIdvMc0",
        "colab_type": "text"
      },
      "source": [
        "### **Plotting few images from training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8DVWxDwirK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "efaf5fd1-b3cc-4341-cd89-f970ca0bef3f"
      },
      "source": [
        "_,axes= plt.subplots(1,5)\n",
        "random_idx= np.random.randint(0,60000,size=5)\n",
        "for i in range(5):\n",
        "  axes[i].imshow(X_train[random_idx[i],:].reshape(28,28), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnjqbiawTy-",
        "colab_type": "text"
      },
      "source": [
        "## **DEFINING NETWORK FOR CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDAHcfdynVQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net():\n",
        "  def __init__(self, layer_dim):\n",
        "    layer_in, hid_layer1, hid_layer2, layer_out= layer_dim\n",
        "    self.param_dict= {}\n",
        "    self.param_dict['w1']= np.random.randn(hid_layer1, layer_in)*np.sqrt(1./hid_layer1)\n",
        "    self.param_dict['w2']= np.random.randn(hid_layer2, hid_layer1)*np.sqrt(1./hid_layer1)\n",
        "    self.param_dict['w3']= np.random.randn(layer_out, hid_layer2)*np.sqrt(1./layer_out)\n",
        "    self.param_dict['b1'] = np.zeros(hid_layer1)\n",
        "    self.param_dict['b2'] = np.zeros(hid_layer2)\n",
        "    self.param_dict['b3'] = np.zeros(layer_out)\n",
        "    self.epochs=2000\n",
        "    self.lr= 0.001\n",
        "    self.batch_size= 512\n",
        "  \n",
        "  def full_pass(self, x, y):\n",
        "    n, dims= x.shape\n",
        "    w1, w2, w3= self.param_dict['w1'], self.param_dict['w2'], self.param_dict['w3']\n",
        "    b1, b2, b3= self.param_dict['b1'], self.param_dict['b2'], self.param_dict['b3']\n",
        "    gradient_dict= {}\n",
        "    a1, temp_act1= self.fwdprop_layer_act(w1,b1,x)\n",
        "    z2, temp2= self.fwdprop_layer_z(w2,b2,a1)\n",
        "    a2, temp_act2= self.ReLU(z2)\n",
        "    z3, temp3= self.fwdprop_layer_z(w3,b3,a2)\n",
        "    delta= z3-np.max(z3,axis=1,keepdims= True)\n",
        "    o= np.sum(np.exp(delta),axis=1,keepdims= True)\n",
        "    log_p= delta-np.log(o)\n",
        "    p= np.exp(log_p)                                \n",
        "    softmax_loss= -np.sum(log_p[np.arange(n),y])/n\n",
        "    d_output= p\n",
        "    d_output[np.arange(n), y]= d_output[np.arange(n), y]-1\n",
        "    gradient_a2,gradient_w3,gradient_b3= self.backprop_layer_z(d_output, temp3)\n",
        "    gradient_z2= self.d_ReLU(gradient_a2, temp_act2)\n",
        "    gradient_a1,gradient_w2,gradient_b2= self.backprop_layer_z(gradient_z2, temp2)\n",
        "    d_x, gradient_w1, gradient_b1= self.backprop_layer_act(gradient_a1, temp_act1)\n",
        "    gradient_dict['w1'], gradient_dict['w2'], gradient_dict['w3']= gradient_w1, gradient_w2, gradient_w3\n",
        "    gradient_dict['b1'], gradient_dict['b2'], gradient_dict['b3']= gradient_b1, gradient_b2, gradient_b3\n",
        "    return softmax_loss, gradient_dict\n",
        "  \n",
        "  def fwdprop_layer_z(self,w,b,x):\n",
        "    n,d= x.shape[0], np.prod(x.shape[1:])\n",
        "    x_reshape= np.reshape(x, (n,d))\n",
        "    layer_output= np.dot(x_reshape,w.T)+b\n",
        "    return layer_output, (x,w,b)\n",
        "\n",
        "  def backprop_layer_act(self,grad,temp_dict):\n",
        "    dz,da= temp_dict\n",
        "    d_relu= self.d_ReLU(grad,da)\n",
        "    return self.backprop_layer_z(d_relu,dz)\n",
        "\n",
        "  def backprop_layer_z(self, grad, temp_dict):\n",
        "    x,w,b= temp_dict\n",
        "    n,d= x.shape[0], np.prod(x.shape[1:])\n",
        "    x_reshape= np.reshape(x, (n,d))\n",
        "    dx_out= np.dot(grad, w)\n",
        "    dx= dx_out.reshape(x.shape)\n",
        "    db= np.sum(grad,axis=0)\n",
        "    dw= np.dot(x_reshape.T,grad)\n",
        "    return dx,dw,db\n",
        "  \n",
        "  def fwdprop_layer_act(self,w,b,x):\n",
        "    z,i= self.fwdprop_layer_z(w,b,x)\n",
        "    act,j= self.ReLU(z)\n",
        "    temp= (i,j)\n",
        "    return act, temp\n",
        "\n",
        "  def ReLU(self, x):\n",
        "    return np.maximum(0,x), x\n",
        "  \n",
        "  def d_ReLU(self, grad, x):\n",
        "    bool_x= (x>0)\n",
        "    return bool_x*grad\n",
        "    \n",
        "  def train(self, x_train, y_train, x_test, y_test):\n",
        "    train_size= x_train.shape[0]\n",
        "    losses, test_accs= [], []\n",
        "    train_accs= []\n",
        "    for epoch in range(self.epochs):\n",
        "      id= np.random.choice(train_size, self.batch_size)\n",
        "      batch_x= x_train[id]\n",
        "      batch_y= y_train[id]\n",
        "      loss, gradient_dict= self.full_pass(batch_x,batch_y)\n",
        "      losses.append(loss)\n",
        "      self.param_dict['w1']= self.param_dict['w1']-self.lr*gradient_dict['w1'].T\n",
        "      self.param_dict['w2']= self.param_dict['w2']-self.lr*gradient_dict['w2'].T\n",
        "      self.param_dict['w3']= self.param_dict['w3']-self.lr*gradient_dict['w3'].T\n",
        "      self.param_dict['b1']= self.param_dict['b1']-self.lr*gradient_dict['b1']\n",
        "      self.param_dict['b2']= self.param_dict['b2']-self.lr*gradient_dict['b2']\n",
        "      self.param_dict['b3']= self.param_dict['b3']-self.lr*gradient_dict['b3']\n",
        "      train_acc = (self.predict(batch_x)==batch_y).mean()\n",
        "      test_acc= (self.predict(x_test)==y_test).mean()\n",
        "      print(f'Epoch: {epoch}; Loss: {loss}; Train Acc: {train_acc}; Test Accuracy: {test_acc}')\n",
        "      train_accs.append(train_acc)\n",
        "      test_accs.append(test_accs)\n",
        "    return losses, train_accs, test_accs\n",
        "  \n",
        "  def predict(self, x):\n",
        "    w1, w2, w3= self.param_dict['w1'], self.param_dict['w2'], self.param_dict['w3']\n",
        "    b1, b2, b3= self.param_dict['b1'], self.param_dict['b2'], self.param_dict['b3']\n",
        "    act1, i= self.fwdprop_layer_act(w1,b1,x)\n",
        "    z2, i= self.fwdprop_layer_z(w2,b2,act1)\n",
        "    act2, i= self.ReLU(z2)\n",
        "    z3, i= self.fwdprop_layer_z(w3,b3,act2)\n",
        "    out= np.argmax(z3, axis=1)\n",
        "    return out\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDYSE0lWpPsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37926f14-c355-47d7-c7a7-b546185af7f9"
      },
      "source": [
        "MNIST_net= Net([784,256,128,10])\n",
        "losses, train_accs, test_accs= MNIST_net.train(X_train,y_train,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0; Loss: 3.061831123292322; Train Acc: 0.19921875; Test Accuracy: 0.1998\n",
            "Epoch: 1; Loss: 4.323247296134571; Train Acc: 0.16015625; Test Accuracy: 0.1324\n",
            "Epoch: 2; Loss: 6.351647992533726; Train Acc: 0.173828125; Test Accuracy: 0.2032\n",
            "Epoch: 3; Loss: 3.318966691245629; Train Acc: 0.33203125; Test Accuracy: 0.3331\n",
            "Epoch: 4; Loss: 2.089031928597893; Train Acc: 0.560546875; Test Accuracy: 0.5214\n",
            "Epoch: 5; Loss: 1.79296575373733; Train Acc: 0.576171875; Test Accuracy: 0.5362\n",
            "Epoch: 6; Loss: 1.5456508557765414; Train Acc: 0.658203125; Test Accuracy: 0.6258\n",
            "Epoch: 7; Loss: 1.2491392635440288; Train Acc: 0.49609375; Test Accuracy: 0.4748\n",
            "Epoch: 8; Loss: 1.4366385298726145; Train Acc: 0.37109375; Test Accuracy: 0.363\n",
            "Epoch: 9; Loss: 1.78196545837574; Train Acc: 0.349609375; Test Accuracy: 0.3585\n",
            "Epoch: 10; Loss: 1.983278581375133; Train Acc: 0.568359375; Test Accuracy: 0.5517\n",
            "Epoch: 11; Loss: 1.5730787756504638; Train Acc: 0.634765625; Test Accuracy: 0.6093\n",
            "Epoch: 12; Loss: 1.2797006519139091; Train Acc: 0.646484375; Test Accuracy: 0.6425\n",
            "Epoch: 13; Loss: 1.2010978123333647; Train Acc: 0.70703125; Test Accuracy: 0.6676\n",
            "Epoch: 14; Loss: 1.081066129679654; Train Acc: 0.802734375; Test Accuracy: 0.7906\n",
            "Epoch: 15; Loss: 0.8850260062841184; Train Acc: 0.74609375; Test Accuracy: 0.7666\n",
            "Epoch: 16; Loss: 0.7501690009776381; Train Acc: 0.65234375; Test Accuracy: 0.6496\n",
            "Epoch: 17; Loss: 1.0608704546420575; Train Acc: 0.6015625; Test Accuracy: 0.5779\n",
            "Epoch: 18; Loss: 1.863463956047271; Train Acc: 0.55859375; Test Accuracy: 0.5527\n",
            "Epoch: 19; Loss: 1.2468437620727526; Train Acc: 0.6796875; Test Accuracy: 0.6591\n",
            "Epoch: 20; Loss: 1.0740329340817911; Train Acc: 0.673828125; Test Accuracy: 0.6856\n",
            "Epoch: 21; Loss: 0.97390371898381; Train Acc: 0.794921875; Test Accuracy: 0.7951\n",
            "Epoch: 22; Loss: 0.7335019003772145; Train Acc: 0.80859375; Test Accuracy: 0.8053\n",
            "Epoch: 23; Loss: 0.7198763840293916; Train Acc: 0.806640625; Test Accuracy: 0.7906\n",
            "Epoch: 24; Loss: 0.7021931128756426; Train Acc: 0.814453125; Test Accuracy: 0.7938\n",
            "Epoch: 25; Loss: 0.5745648159261774; Train Acc: 0.826171875; Test Accuracy: 0.7824\n",
            "Epoch: 26; Loss: 0.7453594976519569; Train Acc: 0.818359375; Test Accuracy: 0.819\n",
            "Epoch: 27; Loss: 0.5740405788689569; Train Acc: 0.84765625; Test Accuracy: 0.8274\n",
            "Epoch: 28; Loss: 0.5408905991765751; Train Acc: 0.87109375; Test Accuracy: 0.8696\n",
            "Epoch: 29; Loss: 0.48176035025047026; Train Acc: 0.8671875; Test Accuracy: 0.8414\n",
            "Epoch: 30; Loss: 0.5657712103712427; Train Acc: 0.77734375; Test Accuracy: 0.7786\n",
            "Epoch: 31; Loss: 0.6963604116832188; Train Acc: 0.8046875; Test Accuracy: 0.7964\n",
            "Epoch: 32; Loss: 0.6530133908675217; Train Acc: 0.810546875; Test Accuracy: 0.8212\n",
            "Epoch: 33; Loss: 0.5793930737254133; Train Acc: 0.865234375; Test Accuracy: 0.8406\n",
            "Epoch: 34; Loss: 0.5519783690094937; Train Acc: 0.861328125; Test Accuracy: 0.8593\n",
            "Epoch: 35; Loss: 0.4811187941020403; Train Acc: 0.873046875; Test Accuracy: 0.8783\n",
            "Epoch: 36; Loss: 0.3943909549845187; Train Acc: 0.875; Test Accuracy: 0.8621\n",
            "Epoch: 37; Loss: 0.5004953746826515; Train Acc: 0.841796875; Test Accuracy: 0.8409\n",
            "Epoch: 38; Loss: 0.44530095790483476; Train Acc: 0.865234375; Test Accuracy: 0.8578\n",
            "Epoch: 39; Loss: 0.43056873306483395; Train Acc: 0.880859375; Test Accuracy: 0.8867\n",
            "Epoch: 40; Loss: 0.41810977281497363; Train Acc: 0.880859375; Test Accuracy: 0.8933\n",
            "Epoch: 41; Loss: 0.439085223984047; Train Acc: 0.90625; Test Accuracy: 0.8725\n",
            "Epoch: 42; Loss: 0.420004524697729; Train Acc: 0.892578125; Test Accuracy: 0.8632\n",
            "Epoch: 43; Loss: 0.36772493315628235; Train Acc: 0.91015625; Test Accuracy: 0.9062\n",
            "Epoch: 44; Loss: 0.3016385251258129; Train Acc: 0.919921875; Test Accuracy: 0.9111\n",
            "Epoch: 45; Loss: 0.3314165162379556; Train Acc: 0.927734375; Test Accuracy: 0.9094\n",
            "Epoch: 46; Loss: 0.29814744851874636; Train Acc: 0.923828125; Test Accuracy: 0.9089\n",
            "Epoch: 47; Loss: 0.2595879664785537; Train Acc: 0.9296875; Test Accuracy: 0.9159\n",
            "Epoch: 48; Loss: 0.2507833088731891; Train Acc: 0.9375; Test Accuracy: 0.913\n",
            "Epoch: 49; Loss: 0.30223868468998333; Train Acc: 0.9296875; Test Accuracy: 0.9141\n",
            "Epoch: 50; Loss: 0.25617455883954926; Train Acc: 0.9453125; Test Accuracy: 0.9141\n",
            "Epoch: 51; Loss: 0.32308652200393817; Train Acc: 0.9140625; Test Accuracy: 0.9117\n",
            "Epoch: 52; Loss: 0.24108808488479694; Train Acc: 0.943359375; Test Accuracy: 0.9193\n",
            "Epoch: 53; Loss: 0.26592319537998166; Train Acc: 0.94140625; Test Accuracy: 0.9148\n",
            "Epoch: 54; Loss: 0.2998337456283719; Train Acc: 0.923828125; Test Accuracy: 0.9115\n",
            "Epoch: 55; Loss: 0.35432725004513027; Train Acc: 0.91015625; Test Accuracy: 0.9118\n",
            "Epoch: 56; Loss: 0.2807605636856664; Train Acc: 0.931640625; Test Accuracy: 0.9189\n",
            "Epoch: 57; Loss: 0.2805723983910359; Train Acc: 0.935546875; Test Accuracy: 0.9165\n",
            "Epoch: 58; Loss: 0.2993450013604473; Train Acc: 0.9296875; Test Accuracy: 0.9066\n",
            "Epoch: 59; Loss: 0.33396480695915953; Train Acc: 0.8984375; Test Accuracy: 0.9041\n",
            "Epoch: 60; Loss: 0.3413488820794276; Train Acc: 0.888671875; Test Accuracy: 0.8772\n",
            "Epoch: 61; Loss: 0.3548212957368899; Train Acc: 0.90234375; Test Accuracy: 0.8732\n",
            "Epoch: 62; Loss: 0.43538203041318774; Train Acc: 0.87109375; Test Accuracy: 0.8698\n",
            "Epoch: 63; Loss: 0.43096677852041737; Train Acc: 0.83984375; Test Accuracy: 0.8516\n",
            "Epoch: 64; Loss: 0.4247667191958402; Train Acc: 0.900390625; Test Accuracy: 0.8946\n",
            "Epoch: 65; Loss: 0.3396557776660941; Train Acc: 0.943359375; Test Accuracy: 0.9202\n",
            "Epoch: 66; Loss: 0.24929119229944485; Train Acc: 0.943359375; Test Accuracy: 0.92\n",
            "Epoch: 67; Loss: 0.2515602508437128; Train Acc: 0.947265625; Test Accuracy: 0.9287\n",
            "Epoch: 68; Loss: 0.26337170179137764; Train Acc: 0.93359375; Test Accuracy: 0.9165\n",
            "Epoch: 69; Loss: 0.33185362155970116; Train Acc: 0.904296875; Test Accuracy: 0.9081\n",
            "Epoch: 70; Loss: 0.2974759399453303; Train Acc: 0.9375; Test Accuracy: 0.9247\n",
            "Epoch: 71; Loss: 0.22527311899811603; Train Acc: 0.955078125; Test Accuracy: 0.9307\n",
            "Epoch: 72; Loss: 0.2176991173264147; Train Acc: 0.9609375; Test Accuracy: 0.9257\n",
            "Epoch: 73; Loss: 0.20447257349758263; Train Acc: 0.9609375; Test Accuracy: 0.925\n",
            "Epoch: 74; Loss: 0.2839516899461158; Train Acc: 0.9375; Test Accuracy: 0.9212\n",
            "Epoch: 75; Loss: 0.3350529752664211; Train Acc: 0.912109375; Test Accuracy: 0.9152\n",
            "Epoch: 76; Loss: 0.3452981090913093; Train Acc: 0.91796875; Test Accuracy: 0.9162\n",
            "Epoch: 77; Loss: 0.3124471340581189; Train Acc: 0.92578125; Test Accuracy: 0.9057\n",
            "Epoch: 78; Loss: 0.32635686211583553; Train Acc: 0.904296875; Test Accuracy: 0.9068\n",
            "Epoch: 79; Loss: 0.32507400722816693; Train Acc: 0.931640625; Test Accuracy: 0.928\n",
            "Epoch: 80; Loss: 0.2622658709867503; Train Acc: 0.94140625; Test Accuracy: 0.9266\n",
            "Epoch: 81; Loss: 0.22362643710862026; Train Acc: 0.953125; Test Accuracy: 0.9313\n",
            "Epoch: 82; Loss: 0.21152866061298725; Train Acc: 0.95703125; Test Accuracy: 0.9304\n",
            "Epoch: 83; Loss: 0.2963696641143786; Train Acc: 0.935546875; Test Accuracy: 0.9345\n",
            "Epoch: 84; Loss: 0.18003593526467485; Train Acc: 0.962890625; Test Accuracy: 0.9334\n",
            "Epoch: 85; Loss: 0.1983832653811885; Train Acc: 0.95703125; Test Accuracy: 0.9313\n",
            "Epoch: 86; Loss: 0.22361400142450588; Train Acc: 0.94140625; Test Accuracy: 0.932\n",
            "Epoch: 87; Loss: 0.24652568138494207; Train Acc: 0.931640625; Test Accuracy: 0.9309\n",
            "Epoch: 88; Loss: 0.27802726253341536; Train Acc: 0.93359375; Test Accuracy: 0.9212\n",
            "Epoch: 89; Loss: 0.25272914960236026; Train Acc: 0.923828125; Test Accuracy: 0.9018\n",
            "Epoch: 90; Loss: 0.2926081454535156; Train Acc: 0.916015625; Test Accuracy: 0.902\n",
            "Epoch: 91; Loss: 0.30572684636715863; Train Acc: 0.923828125; Test Accuracy: 0.9226\n",
            "Epoch: 92; Loss: 0.2622324084252291; Train Acc: 0.939453125; Test Accuracy: 0.9351\n",
            "Epoch: 93; Loss: 0.2753308784937772; Train Acc: 0.9296875; Test Accuracy: 0.9299\n",
            "Epoch: 94; Loss: 0.23339075563024214; Train Acc: 0.9453125; Test Accuracy: 0.925\n",
            "Epoch: 95; Loss: 0.22249736795474562; Train Acc: 0.939453125; Test Accuracy: 0.9299\n",
            "Epoch: 96; Loss: 0.28636023570821234; Train Acc: 0.9375; Test Accuracy: 0.9363\n",
            "Epoch: 97; Loss: 0.20770669124689875; Train Acc: 0.955078125; Test Accuracy: 0.9357\n",
            "Epoch: 98; Loss: 0.22462014758170915; Train Acc: 0.953125; Test Accuracy: 0.9404\n",
            "Epoch: 99; Loss: 0.1985925578742092; Train Acc: 0.95703125; Test Accuracy: 0.9368\n",
            "Epoch: 100; Loss: 0.233392269860499; Train Acc: 0.947265625; Test Accuracy: 0.9369\n",
            "Epoch: 101; Loss: 0.1995639722690984; Train Acc: 0.95703125; Test Accuracy: 0.9364\n",
            "Epoch: 102; Loss: 0.22857416150767768; Train Acc: 0.94921875; Test Accuracy: 0.9281\n",
            "Epoch: 103; Loss: 0.24372062694263263; Train Acc: 0.94921875; Test Accuracy: 0.938\n",
            "Epoch: 104; Loss: 0.1822533233881744; Train Acc: 0.96484375; Test Accuracy: 0.9417\n",
            "Epoch: 105; Loss: 0.21154500274303595; Train Acc: 0.953125; Test Accuracy: 0.9407\n",
            "Epoch: 106; Loss: 0.15751091458600144; Train Acc: 0.970703125; Test Accuracy: 0.9399\n",
            "Epoch: 107; Loss: 0.19856217445144786; Train Acc: 0.958984375; Test Accuracy: 0.9418\n",
            "Epoch: 108; Loss: 0.20703849831599314; Train Acc: 0.951171875; Test Accuracy: 0.9415\n",
            "Epoch: 109; Loss: 0.1717050026248743; Train Acc: 0.9609375; Test Accuracy: 0.942\n",
            "Epoch: 110; Loss: 0.1929585410720866; Train Acc: 0.955078125; Test Accuracy: 0.9412\n",
            "Epoch: 111; Loss: 0.19829552163865982; Train Acc: 0.95703125; Test Accuracy: 0.9418\n",
            "Epoch: 112; Loss: 0.20729883338191224; Train Acc: 0.95703125; Test Accuracy: 0.9433\n",
            "Epoch: 113; Loss: 0.20404529594832468; Train Acc: 0.953125; Test Accuracy: 0.9445\n",
            "Epoch: 114; Loss: 0.17979055187277457; Train Acc: 0.96484375; Test Accuracy: 0.9391\n",
            "Epoch: 115; Loss: 0.17079526554466845; Train Acc: 0.96484375; Test Accuracy: 0.9467\n",
            "Epoch: 116; Loss: 0.17293131251587354; Train Acc: 0.966796875; Test Accuracy: 0.9385\n",
            "Epoch: 117; Loss: 0.2026281871493663; Train Acc: 0.9375; Test Accuracy: 0.9324\n",
            "Epoch: 118; Loss: 0.177155826404911; Train Acc: 0.962890625; Test Accuracy: 0.9449\n",
            "Epoch: 119; Loss: 0.199132830882701; Train Acc: 0.951171875; Test Accuracy: 0.9468\n",
            "Epoch: 120; Loss: 0.20802242819000877; Train Acc: 0.962890625; Test Accuracy: 0.9452\n",
            "Epoch: 121; Loss: 0.20991978966315422; Train Acc: 0.958984375; Test Accuracy: 0.945\n",
            "Epoch: 122; Loss: 0.18333406507196923; Train Acc: 0.962890625; Test Accuracy: 0.9417\n",
            "Epoch: 123; Loss: 0.14681007577585042; Train Acc: 0.978515625; Test Accuracy: 0.9436\n",
            "Epoch: 124; Loss: 0.20738003395624705; Train Acc: 0.955078125; Test Accuracy: 0.9427\n",
            "Epoch: 125; Loss: 0.16164214413159098; Train Acc: 0.966796875; Test Accuracy: 0.9405\n",
            "Epoch: 126; Loss: 0.251467302479055; Train Acc: 0.9453125; Test Accuracy: 0.9307\n",
            "Epoch: 127; Loss: 0.23022534803016814; Train Acc: 0.953125; Test Accuracy: 0.946\n",
            "Epoch: 128; Loss: 0.18578213384517325; Train Acc: 0.958984375; Test Accuracy: 0.9407\n",
            "Epoch: 129; Loss: 0.1960022251667583; Train Acc: 0.955078125; Test Accuracy: 0.9364\n",
            "Epoch: 130; Loss: 0.20816075685333585; Train Acc: 0.923828125; Test Accuracy: 0.9222\n",
            "Epoch: 131; Loss: 0.23367672757553368; Train Acc: 0.9140625; Test Accuracy: 0.8913\n",
            "Epoch: 132; Loss: 0.31335615817025736; Train Acc: 0.8984375; Test Accuracy: 0.8832\n",
            "Epoch: 133; Loss: 0.3542241893653531; Train Acc: 0.8984375; Test Accuracy: 0.8892\n",
            "Epoch: 134; Loss: 0.3443892427830135; Train Acc: 0.939453125; Test Accuracy: 0.9387\n",
            "Epoch: 135; Loss: 0.2132983812980755; Train Acc: 0.955078125; Test Accuracy: 0.9487\n",
            "Epoch: 136; Loss: 0.17100640684016827; Train Acc: 0.96484375; Test Accuracy: 0.9512\n",
            "Epoch: 137; Loss: 0.20805535838061584; Train Acc: 0.9453125; Test Accuracy: 0.9487\n",
            "Epoch: 138; Loss: 0.1580928280605027; Train Acc: 0.97265625; Test Accuracy: 0.9473\n",
            "Epoch: 139; Loss: 0.1603006022991616; Train Acc: 0.966796875; Test Accuracy: 0.9496\n",
            "Epoch: 140; Loss: 0.1660358596393648; Train Acc: 0.96484375; Test Accuracy: 0.9494\n",
            "Epoch: 141; Loss: 0.2024178488188746; Train Acc: 0.955078125; Test Accuracy: 0.9471\n",
            "Epoch: 142; Loss: 0.1572720039096352; Train Acc: 0.9609375; Test Accuracy: 0.947\n",
            "Epoch: 143; Loss: 0.14770370816875464; Train Acc: 0.96875; Test Accuracy: 0.9496\n",
            "Epoch: 144; Loss: 0.19388056363646228; Train Acc: 0.951171875; Test Accuracy: 0.9495\n",
            "Epoch: 145; Loss: 0.1642669270258741; Train Acc: 0.966796875; Test Accuracy: 0.9493\n",
            "Epoch: 146; Loss: 0.14925642121679683; Train Acc: 0.96875; Test Accuracy: 0.9478\n",
            "Epoch: 147; Loss: 0.1598530729401481; Train Acc: 0.96484375; Test Accuracy: 0.9482\n",
            "Epoch: 148; Loss: 0.1360631415949707; Train Acc: 0.974609375; Test Accuracy: 0.9495\n",
            "Epoch: 149; Loss: 0.1830736408607776; Train Acc: 0.966796875; Test Accuracy: 0.94\n",
            "Epoch: 150; Loss: 0.18182632858870992; Train Acc: 0.962890625; Test Accuracy: 0.9309\n",
            "Epoch: 151; Loss: 0.1883421447832315; Train Acc: 0.966796875; Test Accuracy: 0.9509\n",
            "Epoch: 152; Loss: 0.1863704769303152; Train Acc: 0.955078125; Test Accuracy: 0.9453\n",
            "Epoch: 153; Loss: 0.20179976827137613; Train Acc: 0.951171875; Test Accuracy: 0.9499\n",
            "Epoch: 154; Loss: 0.1665185864977387; Train Acc: 0.97265625; Test Accuracy: 0.9498\n",
            "Epoch: 155; Loss: 0.17930778986045076; Train Acc: 0.955078125; Test Accuracy: 0.95\n",
            "Epoch: 156; Loss: 0.15609633450352722; Train Acc: 0.96875; Test Accuracy: 0.9534\n",
            "Epoch: 157; Loss: 0.13576349525320872; Train Acc: 0.96875; Test Accuracy: 0.9478\n",
            "Epoch: 158; Loss: 0.15128713524680337; Train Acc: 0.970703125; Test Accuracy: 0.9447\n",
            "Epoch: 159; Loss: 0.1722482351413801; Train Acc: 0.955078125; Test Accuracy: 0.9484\n",
            "Epoch: 160; Loss: 0.1823319900970557; Train Acc: 0.962890625; Test Accuracy: 0.9508\n",
            "Epoch: 161; Loss: 0.16987766804685323; Train Acc: 0.970703125; Test Accuracy: 0.953\n",
            "Epoch: 162; Loss: 0.11431034184297222; Train Acc: 0.9765625; Test Accuracy: 0.9544\n",
            "Epoch: 163; Loss: 0.14456875762704516; Train Acc: 0.96875; Test Accuracy: 0.9532\n",
            "Epoch: 164; Loss: 0.14455644734817946; Train Acc: 0.970703125; Test Accuracy: 0.9532\n",
            "Epoch: 165; Loss: 0.13686397392022842; Train Acc: 0.97265625; Test Accuracy: 0.9512\n",
            "Epoch: 166; Loss: 0.14714039128888917; Train Acc: 0.9765625; Test Accuracy: 0.952\n",
            "Epoch: 167; Loss: 0.17412768800026346; Train Acc: 0.96484375; Test Accuracy: 0.954\n",
            "Epoch: 168; Loss: 0.13300098548126443; Train Acc: 0.982421875; Test Accuracy: 0.9528\n",
            "Epoch: 169; Loss: 0.15125670075899242; Train Acc: 0.98046875; Test Accuracy: 0.9518\n",
            "Epoch: 170; Loss: 0.14473857522660008; Train Acc: 0.97265625; Test Accuracy: 0.9539\n",
            "Epoch: 171; Loss: 0.1785007300748495; Train Acc: 0.96875; Test Accuracy: 0.9551\n",
            "Epoch: 172; Loss: 0.14120943414376955; Train Acc: 0.97265625; Test Accuracy: 0.9512\n",
            "Epoch: 173; Loss: 0.22238168109938747; Train Acc: 0.943359375; Test Accuracy: 0.9486\n",
            "Epoch: 174; Loss: 0.1319580663892523; Train Acc: 0.97265625; Test Accuracy: 0.9542\n",
            "Epoch: 175; Loss: 0.16825980574776345; Train Acc: 0.962890625; Test Accuracy: 0.9523\n",
            "Epoch: 176; Loss: 0.13515090204656716; Train Acc: 0.97265625; Test Accuracy: 0.9526\n",
            "Epoch: 177; Loss: 0.12874628460630788; Train Acc: 0.978515625; Test Accuracy: 0.951\n",
            "Epoch: 178; Loss: 0.1459375281249592; Train Acc: 0.9765625; Test Accuracy: 0.9524\n",
            "Epoch: 179; Loss: 0.14077235790232168; Train Acc: 0.97265625; Test Accuracy: 0.9493\n",
            "Epoch: 180; Loss: 0.12155584512639278; Train Acc: 0.97265625; Test Accuracy: 0.9515\n",
            "Epoch: 181; Loss: 0.1261015464646984; Train Acc: 0.974609375; Test Accuracy: 0.9534\n",
            "Epoch: 182; Loss: 0.21769639642165584; Train Acc: 0.953125; Test Accuracy: 0.9534\n",
            "Epoch: 183; Loss: 0.157005272908393; Train Acc: 0.962890625; Test Accuracy: 0.9562\n",
            "Epoch: 184; Loss: 0.15322652364195033; Train Acc: 0.97265625; Test Accuracy: 0.949\n",
            "Epoch: 185; Loss: 0.1626848844933581; Train Acc: 0.958984375; Test Accuracy: 0.9357\n",
            "Epoch: 186; Loss: 0.21577283097419844; Train Acc: 0.953125; Test Accuracy: 0.9486\n",
            "Epoch: 187; Loss: 0.1407364819915764; Train Acc: 0.974609375; Test Accuracy: 0.9554\n",
            "Epoch: 188; Loss: 0.15747541551614064; Train Acc: 0.966796875; Test Accuracy: 0.9508\n",
            "Epoch: 189; Loss: 0.141008623842157; Train Acc: 0.970703125; Test Accuracy: 0.9559\n",
            "Epoch: 190; Loss: 0.15242768942583268; Train Acc: 0.966796875; Test Accuracy: 0.9484\n",
            "Epoch: 191; Loss: 0.1560138246360282; Train Acc: 0.98046875; Test Accuracy: 0.9532\n",
            "Epoch: 192; Loss: 0.1668381668143128; Train Acc: 0.970703125; Test Accuracy: 0.9482\n",
            "Epoch: 193; Loss: 0.13156015386287706; Train Acc: 0.9765625; Test Accuracy: 0.9569\n",
            "Epoch: 194; Loss: 0.18760029075289358; Train Acc: 0.95703125; Test Accuracy: 0.9516\n",
            "Epoch: 195; Loss: 0.13394162482117278; Train Acc: 0.970703125; Test Accuracy: 0.9496\n",
            "Epoch: 196; Loss: 0.14951374378253562; Train Acc: 0.962890625; Test Accuracy: 0.9535\n",
            "Epoch: 197; Loss: 0.12612034023246235; Train Acc: 0.9765625; Test Accuracy: 0.9553\n",
            "Epoch: 198; Loss: 0.11947482167235622; Train Acc: 0.970703125; Test Accuracy: 0.9539\n",
            "Epoch: 199; Loss: 0.15338593387499777; Train Acc: 0.9765625; Test Accuracy: 0.9462\n",
            "Epoch: 200; Loss: 0.1356462390639678; Train Acc: 0.97265625; Test Accuracy: 0.9542\n",
            "Epoch: 201; Loss: 0.12544567425800968; Train Acc: 0.9765625; Test Accuracy: 0.9546\n",
            "Epoch: 202; Loss: 0.1263098819431515; Train Acc: 0.98046875; Test Accuracy: 0.9569\n",
            "Epoch: 203; Loss: 0.13024763805036643; Train Acc: 0.974609375; Test Accuracy: 0.9584\n",
            "Epoch: 204; Loss: 0.11018369589192911; Train Acc: 0.986328125; Test Accuracy: 0.9572\n",
            "Epoch: 205; Loss: 0.1867844401141417; Train Acc: 0.95703125; Test Accuracy: 0.9544\n",
            "Epoch: 206; Loss: 0.1449996438276058; Train Acc: 0.9765625; Test Accuracy: 0.9577\n",
            "Epoch: 207; Loss: 0.1350050357645568; Train Acc: 0.974609375; Test Accuracy: 0.9549\n",
            "Epoch: 208; Loss: 0.22170495625449171; Train Acc: 0.953125; Test Accuracy: 0.9555\n",
            "Epoch: 209; Loss: 0.17405198206735945; Train Acc: 0.955078125; Test Accuracy: 0.9567\n",
            "Epoch: 210; Loss: 0.17367744376345629; Train Acc: 0.96484375; Test Accuracy: 0.9563\n",
            "Epoch: 211; Loss: 0.16944838327339162; Train Acc: 0.95703125; Test Accuracy: 0.9495\n",
            "Epoch: 212; Loss: 0.17366249105770729; Train Acc: 0.96484375; Test Accuracy: 0.9525\n",
            "Epoch: 213; Loss: 0.17851116237888298; Train Acc: 0.9609375; Test Accuracy: 0.9547\n",
            "Epoch: 214; Loss: 0.18138028463042127; Train Acc: 0.955078125; Test Accuracy: 0.9512\n",
            "Epoch: 215; Loss: 0.14570881025601792; Train Acc: 0.96875; Test Accuracy: 0.9605\n",
            "Epoch: 216; Loss: 0.1294309274116347; Train Acc: 0.978515625; Test Accuracy: 0.9532\n",
            "Epoch: 217; Loss: 0.19818823030914184; Train Acc: 0.95703125; Test Accuracy: 0.9541\n",
            "Epoch: 218; Loss: 0.11581971302141293; Train Acc: 0.978515625; Test Accuracy: 0.9594\n",
            "Epoch: 219; Loss: 0.15787815284705248; Train Acc: 0.966796875; Test Accuracy: 0.9577\n",
            "Epoch: 220; Loss: 0.14339563320606608; Train Acc: 0.982421875; Test Accuracy: 0.9583\n",
            "Epoch: 221; Loss: 0.15629570025600706; Train Acc: 0.958984375; Test Accuracy: 0.9582\n",
            "Epoch: 222; Loss: 0.10851631442451914; Train Acc: 0.98046875; Test Accuracy: 0.9589\n",
            "Epoch: 223; Loss: 0.11691286857499801; Train Acc: 0.98046875; Test Accuracy: 0.955\n",
            "Epoch: 224; Loss: 0.15511255471291924; Train Acc: 0.974609375; Test Accuracy: 0.9596\n",
            "Epoch: 225; Loss: 0.12979731552362708; Train Acc: 0.970703125; Test Accuracy: 0.9589\n",
            "Epoch: 226; Loss: 0.13517188887392867; Train Acc: 0.974609375; Test Accuracy: 0.9552\n",
            "Epoch: 227; Loss: 0.12904013521661392; Train Acc: 0.9765625; Test Accuracy: 0.9567\n",
            "Epoch: 228; Loss: 0.14088023038033962; Train Acc: 0.974609375; Test Accuracy: 0.9546\n",
            "Epoch: 229; Loss: 0.1289321011361981; Train Acc: 0.96484375; Test Accuracy: 0.9471\n",
            "Epoch: 230; Loss: 0.1284230283524872; Train Acc: 0.9765625; Test Accuracy: 0.9585\n",
            "Epoch: 231; Loss: 0.1339595205532716; Train Acc: 0.974609375; Test Accuracy: 0.9593\n",
            "Epoch: 232; Loss: 0.1195799007924014; Train Acc: 0.9765625; Test Accuracy: 0.9605\n",
            "Epoch: 233; Loss: 0.1141484875275511; Train Acc: 0.978515625; Test Accuracy: 0.9601\n",
            "Epoch: 234; Loss: 0.13035039029449078; Train Acc: 0.98046875; Test Accuracy: 0.9538\n",
            "Epoch: 235; Loss: 0.14879621872867277; Train Acc: 0.970703125; Test Accuracy: 0.9574\n",
            "Epoch: 236; Loss: 0.1340908944843862; Train Acc: 0.982421875; Test Accuracy: 0.9562\n",
            "Epoch: 237; Loss: 0.17977467194860725; Train Acc: 0.955078125; Test Accuracy: 0.9567\n",
            "Epoch: 238; Loss: 0.1623868675452227; Train Acc: 0.96875; Test Accuracy: 0.9608\n",
            "Epoch: 239; Loss: 0.12341172341241836; Train Acc: 0.98046875; Test Accuracy: 0.9591\n",
            "Epoch: 240; Loss: 0.10847886051210198; Train Acc: 0.98046875; Test Accuracy: 0.9587\n",
            "Epoch: 241; Loss: 0.13237143249343147; Train Acc: 0.974609375; Test Accuracy: 0.9568\n",
            "Epoch: 242; Loss: 0.13813566590496693; Train Acc: 0.97265625; Test Accuracy: 0.9603\n",
            "Epoch: 243; Loss: 0.12501172263333982; Train Acc: 0.974609375; Test Accuracy: 0.9572\n",
            "Epoch: 244; Loss: 0.1820215734644306; Train Acc: 0.951171875; Test Accuracy: 0.9592\n",
            "Epoch: 245; Loss: 0.12241633906165453; Train Acc: 0.984375; Test Accuracy: 0.9586\n",
            "Epoch: 246; Loss: 0.12627632878153663; Train Acc: 0.98046875; Test Accuracy: 0.9572\n",
            "Epoch: 247; Loss: 0.12521603766023515; Train Acc: 0.97265625; Test Accuracy: 0.9603\n",
            "Epoch: 248; Loss: 0.14028274382085684; Train Acc: 0.97265625; Test Accuracy: 0.9582\n",
            "Epoch: 249; Loss: 0.13727944377031115; Train Acc: 0.9765625; Test Accuracy: 0.9587\n",
            "Epoch: 250; Loss: 0.13538275027086816; Train Acc: 0.978515625; Test Accuracy: 0.9591\n",
            "Epoch: 251; Loss: 0.1582360967517657; Train Acc: 0.970703125; Test Accuracy: 0.961\n",
            "Epoch: 252; Loss: 0.09805632228206035; Train Acc: 0.986328125; Test Accuracy: 0.9594\n",
            "Epoch: 253; Loss: 0.10992882616595535; Train Acc: 0.98046875; Test Accuracy: 0.9608\n",
            "Epoch: 254; Loss: 0.10747557153084591; Train Acc: 0.984375; Test Accuracy: 0.9618\n",
            "Epoch: 255; Loss: 0.10737083784741239; Train Acc: 0.97265625; Test Accuracy: 0.9622\n",
            "Epoch: 256; Loss: 0.1050203306612307; Train Acc: 0.974609375; Test Accuracy: 0.9622\n",
            "Epoch: 257; Loss: 0.12660575243763794; Train Acc: 0.974609375; Test Accuracy: 0.9625\n",
            "Epoch: 258; Loss: 0.1576000654545883; Train Acc: 0.97265625; Test Accuracy: 0.9615\n",
            "Epoch: 259; Loss: 0.10529076804838995; Train Acc: 0.98046875; Test Accuracy: 0.9634\n",
            "Epoch: 260; Loss: 0.13478701662717035; Train Acc: 0.982421875; Test Accuracy: 0.9598\n",
            "Epoch: 261; Loss: 0.1103256044453681; Train Acc: 0.978515625; Test Accuracy: 0.963\n",
            "Epoch: 262; Loss: 0.10363458997863345; Train Acc: 0.974609375; Test Accuracy: 0.9634\n",
            "Epoch: 263; Loss: 0.094160642271029; Train Acc: 0.990234375; Test Accuracy: 0.9622\n",
            "Epoch: 264; Loss: 0.12441297683818074; Train Acc: 0.978515625; Test Accuracy: 0.9596\n",
            "Epoch: 265; Loss: 0.12307560169914812; Train Acc: 0.974609375; Test Accuracy: 0.9623\n",
            "Epoch: 266; Loss: 0.08650925534375006; Train Acc: 0.982421875; Test Accuracy: 0.964\n",
            "Epoch: 267; Loss: 0.10660489101734184; Train Acc: 0.978515625; Test Accuracy: 0.9622\n",
            "Epoch: 268; Loss: 0.09550425501415777; Train Acc: 0.98828125; Test Accuracy: 0.9631\n",
            "Epoch: 269; Loss: 0.15311592158912696; Train Acc: 0.96875; Test Accuracy: 0.9588\n",
            "Epoch: 270; Loss: 0.13517609881973316; Train Acc: 0.966796875; Test Accuracy: 0.9628\n",
            "Epoch: 271; Loss: 0.1059962047521663; Train Acc: 0.9765625; Test Accuracy: 0.9633\n",
            "Epoch: 272; Loss: 0.13556975320955023; Train Acc: 0.97265625; Test Accuracy: 0.9588\n",
            "Epoch: 273; Loss: 0.10274273142442626; Train Acc: 0.986328125; Test Accuracy: 0.9636\n",
            "Epoch: 274; Loss: 0.08205257696477125; Train Acc: 0.982421875; Test Accuracy: 0.9629\n",
            "Epoch: 275; Loss: 0.08675471289491364; Train Acc: 0.986328125; Test Accuracy: 0.9617\n",
            "Epoch: 276; Loss: 0.1138961969658416; Train Acc: 0.97265625; Test Accuracy: 0.9547\n",
            "Epoch: 277; Loss: 0.14138989885591324; Train Acc: 0.96875; Test Accuracy: 0.9592\n",
            "Epoch: 278; Loss: 0.13683966115611235; Train Acc: 0.97265625; Test Accuracy: 0.9621\n",
            "Epoch: 279; Loss: 0.0911037752919581; Train Acc: 0.986328125; Test Accuracy: 0.9625\n",
            "Epoch: 280; Loss: 0.071276341383498; Train Acc: 0.98828125; Test Accuracy: 0.9635\n",
            "Epoch: 281; Loss: 0.13288951078293507; Train Acc: 0.974609375; Test Accuracy: 0.9627\n",
            "Epoch: 282; Loss: 0.14386709258503733; Train Acc: 0.978515625; Test Accuracy: 0.9617\n",
            "Epoch: 283; Loss: 0.10388719468130009; Train Acc: 0.984375; Test Accuracy: 0.9634\n",
            "Epoch: 284; Loss: 0.10037232992975287; Train Acc: 0.98828125; Test Accuracy: 0.9628\n",
            "Epoch: 285; Loss: 0.124178586976637; Train Acc: 0.982421875; Test Accuracy: 0.9616\n",
            "Epoch: 286; Loss: 0.12623390142645058; Train Acc: 0.97265625; Test Accuracy: 0.963\n",
            "Epoch: 287; Loss: 0.094776777422881; Train Acc: 0.978515625; Test Accuracy: 0.962\n",
            "Epoch: 288; Loss: 0.08672057089637375; Train Acc: 0.98828125; Test Accuracy: 0.9646\n",
            "Epoch: 289; Loss: 0.11771320346568218; Train Acc: 0.978515625; Test Accuracy: 0.9626\n",
            "Epoch: 290; Loss: 0.12744986932714233; Train Acc: 0.982421875; Test Accuracy: 0.9619\n",
            "Epoch: 291; Loss: 0.09069494609693353; Train Acc: 0.98828125; Test Accuracy: 0.9604\n",
            "Epoch: 292; Loss: 0.10814739594642062; Train Acc: 0.98046875; Test Accuracy: 0.9612\n",
            "Epoch: 293; Loss: 0.16506771551027372; Train Acc: 0.96875; Test Accuracy: 0.9561\n",
            "Epoch: 294; Loss: 0.17271099249932165; Train Acc: 0.96484375; Test Accuracy: 0.9548\n",
            "Epoch: 295; Loss: 0.12861349162843758; Train Acc: 0.982421875; Test Accuracy: 0.9617\n",
            "Epoch: 296; Loss: 0.09687399106154554; Train Acc: 0.984375; Test Accuracy: 0.963\n",
            "Epoch: 297; Loss: 0.13553858916760536; Train Acc: 0.970703125; Test Accuracy: 0.9608\n",
            "Epoch: 298; Loss: 0.09406188856106135; Train Acc: 0.984375; Test Accuracy: 0.965\n",
            "Epoch: 299; Loss: 0.14197172568758779; Train Acc: 0.97265625; Test Accuracy: 0.9653\n",
            "Epoch: 300; Loss: 0.1461276557569888; Train Acc: 0.96875; Test Accuracy: 0.9609\n",
            "Epoch: 301; Loss: 0.1392476895033624; Train Acc: 0.970703125; Test Accuracy: 0.961\n",
            "Epoch: 302; Loss: 0.11016717241535307; Train Acc: 0.986328125; Test Accuracy: 0.9656\n",
            "Epoch: 303; Loss: 0.10748048491380194; Train Acc: 0.98046875; Test Accuracy: 0.9657\n",
            "Epoch: 304; Loss: 0.07805980683173397; Train Acc: 0.98828125; Test Accuracy: 0.9639\n",
            "Epoch: 305; Loss: 0.09001885837708137; Train Acc: 0.986328125; Test Accuracy: 0.9659\n",
            "Epoch: 306; Loss: 0.0797533757547707; Train Acc: 0.990234375; Test Accuracy: 0.9631\n",
            "Epoch: 307; Loss: 0.09521188932224878; Train Acc: 0.986328125; Test Accuracy: 0.9628\n",
            "Epoch: 308; Loss: 0.10044460388317958; Train Acc: 0.994140625; Test Accuracy: 0.9655\n",
            "Epoch: 309; Loss: 0.08300805690145414; Train Acc: 0.990234375; Test Accuracy: 0.9641\n",
            "Epoch: 310; Loss: 0.1007750315899605; Train Acc: 0.986328125; Test Accuracy: 0.9629\n",
            "Epoch: 311; Loss: 0.10928648871045561; Train Acc: 0.982421875; Test Accuracy: 0.9634\n",
            "Epoch: 312; Loss: 0.0969487976401637; Train Acc: 0.982421875; Test Accuracy: 0.9588\n",
            "Epoch: 313; Loss: 0.08350435747705708; Train Acc: 0.98828125; Test Accuracy: 0.9639\n",
            "Epoch: 314; Loss: 0.13837224265984482; Train Acc: 0.9765625; Test Accuracy: 0.9646\n",
            "Epoch: 315; Loss: 0.09107616362020356; Train Acc: 0.982421875; Test Accuracy: 0.964\n",
            "Epoch: 316; Loss: 0.1475830446554099; Train Acc: 0.970703125; Test Accuracy: 0.9638\n",
            "Epoch: 317; Loss: 0.12423586748490545; Train Acc: 0.984375; Test Accuracy: 0.9645\n",
            "Epoch: 318; Loss: 0.12101262617658284; Train Acc: 0.98046875; Test Accuracy: 0.9616\n",
            "Epoch: 319; Loss: 0.12734490306351537; Train Acc: 0.974609375; Test Accuracy: 0.9586\n",
            "Epoch: 320; Loss: 0.1645738221314267; Train Acc: 0.953125; Test Accuracy: 0.9344\n",
            "Epoch: 321; Loss: 0.16167914129866218; Train Acc: 0.953125; Test Accuracy: 0.9435\n",
            "Epoch: 322; Loss: 0.15347261399858989; Train Acc: 0.96875; Test Accuracy: 0.9544\n",
            "Epoch: 323; Loss: 0.12522941276482663; Train Acc: 0.978515625; Test Accuracy: 0.9637\n",
            "Epoch: 324; Loss: 0.08213775241140081; Train Acc: 0.984375; Test Accuracy: 0.9639\n",
            "Epoch: 325; Loss: 0.0924854115569347; Train Acc: 0.974609375; Test Accuracy: 0.9635\n",
            "Epoch: 326; Loss: 0.1490142363960075; Train Acc: 0.970703125; Test Accuracy: 0.9577\n",
            "Epoch: 327; Loss: 0.08674119617039096; Train Acc: 0.98828125; Test Accuracy: 0.9659\n",
            "Epoch: 328; Loss: 0.08013976339663367; Train Acc: 0.986328125; Test Accuracy: 0.9629\n",
            "Epoch: 329; Loss: 0.07528686660118851; Train Acc: 0.994140625; Test Accuracy: 0.9645\n",
            "Epoch: 330; Loss: 0.0661645977738754; Train Acc: 0.994140625; Test Accuracy: 0.9658\n",
            "Epoch: 331; Loss: 0.09883008784843132; Train Acc: 0.986328125; Test Accuracy: 0.9649\n",
            "Epoch: 332; Loss: 0.11813310456926344; Train Acc: 0.978515625; Test Accuracy: 0.9657\n",
            "Epoch: 333; Loss: 0.0869538177883915; Train Acc: 0.984375; Test Accuracy: 0.9651\n",
            "Epoch: 334; Loss: 0.09666037755748551; Train Acc: 0.986328125; Test Accuracy: 0.9636\n",
            "Epoch: 335; Loss: 0.117203804960094; Train Acc: 0.982421875; Test Accuracy: 0.9631\n",
            "Epoch: 336; Loss: 0.11241699992806131; Train Acc: 0.978515625; Test Accuracy: 0.9646\n",
            "Epoch: 337; Loss: 0.09321096810890303; Train Acc: 0.982421875; Test Accuracy: 0.9645\n",
            "Epoch: 338; Loss: 0.11388366477523135; Train Acc: 0.978515625; Test Accuracy: 0.9617\n",
            "Epoch: 339; Loss: 0.12391029530843804; Train Acc: 0.982421875; Test Accuracy: 0.9638\n",
            "Epoch: 340; Loss: 0.11034640087505973; Train Acc: 0.982421875; Test Accuracy: 0.9646\n",
            "Epoch: 341; Loss: 0.09649200260945826; Train Acc: 0.984375; Test Accuracy: 0.9633\n",
            "Epoch: 342; Loss: 0.10869974173052702; Train Acc: 0.982421875; Test Accuracy: 0.9654\n",
            "Epoch: 343; Loss: 0.1412956856372184; Train Acc: 0.9765625; Test Accuracy: 0.9634\n",
            "Epoch: 344; Loss: 0.08197747544251008; Train Acc: 0.9921875; Test Accuracy: 0.9646\n",
            "Epoch: 345; Loss: 0.09390617449856307; Train Acc: 0.984375; Test Accuracy: 0.9653\n",
            "Epoch: 346; Loss: 0.11554073523687838; Train Acc: 0.984375; Test Accuracy: 0.9673\n",
            "Epoch: 347; Loss: 0.1064597562267216; Train Acc: 0.98828125; Test Accuracy: 0.9655\n",
            "Epoch: 348; Loss: 0.12185843247561899; Train Acc: 0.974609375; Test Accuracy: 0.9632\n",
            "Epoch: 349; Loss: 0.0729424513915363; Train Acc: 0.98828125; Test Accuracy: 0.9675\n",
            "Epoch: 350; Loss: 0.11682730734639513; Train Acc: 0.982421875; Test Accuracy: 0.967\n",
            "Epoch: 351; Loss: 0.07962136644113267; Train Acc: 0.990234375; Test Accuracy: 0.9648\n",
            "Epoch: 352; Loss: 0.07229050480579986; Train Acc: 0.984375; Test Accuracy: 0.9622\n",
            "Epoch: 353; Loss: 0.07819066362391681; Train Acc: 0.986328125; Test Accuracy: 0.9668\n",
            "Epoch: 354; Loss: 0.12575037660046737; Train Acc: 0.98046875; Test Accuracy: 0.9607\n",
            "Epoch: 355; Loss: 0.1075487201669503; Train Acc: 0.978515625; Test Accuracy: 0.9651\n",
            "Epoch: 356; Loss: 0.10448825463902645; Train Acc: 0.984375; Test Accuracy: 0.9639\n",
            "Epoch: 357; Loss: 0.11133729302349282; Train Acc: 0.984375; Test Accuracy: 0.9644\n",
            "Epoch: 358; Loss: 0.11374078888519791; Train Acc: 0.978515625; Test Accuracy: 0.9681\n",
            "Epoch: 359; Loss: 0.08846872116438262; Train Acc: 0.986328125; Test Accuracy: 0.9669\n",
            "Epoch: 360; Loss: 0.13205183329692344; Train Acc: 0.98046875; Test Accuracy: 0.9656\n",
            "Epoch: 361; Loss: 0.09992987121814777; Train Acc: 0.986328125; Test Accuracy: 0.9679\n",
            "Epoch: 362; Loss: 0.07390475759426532; Train Acc: 0.990234375; Test Accuracy: 0.9664\n",
            "Epoch: 363; Loss: 0.10172957786022475; Train Acc: 0.990234375; Test Accuracy: 0.9666\n",
            "Epoch: 364; Loss: 0.0756099542509269; Train Acc: 0.98828125; Test Accuracy: 0.9679\n",
            "Epoch: 365; Loss: 0.07171601261201385; Train Acc: 0.990234375; Test Accuracy: 0.9667\n",
            "Epoch: 366; Loss: 0.08294500171755602; Train Acc: 0.982421875; Test Accuracy: 0.9671\n",
            "Epoch: 367; Loss: 0.10481841907502698; Train Acc: 0.982421875; Test Accuracy: 0.9655\n",
            "Epoch: 368; Loss: 0.09082513720733529; Train Acc: 0.98828125; Test Accuracy: 0.9682\n",
            "Epoch: 369; Loss: 0.12057767550096457; Train Acc: 0.982421875; Test Accuracy: 0.9701\n",
            "Epoch: 370; Loss: 0.08762016484327467; Train Acc: 0.9921875; Test Accuracy: 0.9676\n",
            "Epoch: 371; Loss: 0.07615056983002991; Train Acc: 0.98828125; Test Accuracy: 0.9676\n",
            "Epoch: 372; Loss: 0.1402431050233326; Train Acc: 0.978515625; Test Accuracy: 0.9667\n",
            "Epoch: 373; Loss: 0.10566771830637417; Train Acc: 0.984375; Test Accuracy: 0.9632\n",
            "Epoch: 374; Loss: 0.11105270690757309; Train Acc: 0.982421875; Test Accuracy: 0.9666\n",
            "Epoch: 375; Loss: 0.10103348635605253; Train Acc: 0.9765625; Test Accuracy: 0.9647\n",
            "Epoch: 376; Loss: 0.10565693316989626; Train Acc: 0.9765625; Test Accuracy: 0.9671\n",
            "Epoch: 377; Loss: 0.09358127609202357; Train Acc: 0.982421875; Test Accuracy: 0.9666\n",
            "Epoch: 378; Loss: 0.10999786681706006; Train Acc: 0.986328125; Test Accuracy: 0.9655\n",
            "Epoch: 379; Loss: 0.07883668509653649; Train Acc: 0.990234375; Test Accuracy: 0.968\n",
            "Epoch: 380; Loss: 0.1130947980126888; Train Acc: 0.984375; Test Accuracy: 0.9633\n",
            "Epoch: 381; Loss: 0.09870144379102402; Train Acc: 0.9921875; Test Accuracy: 0.9649\n",
            "Epoch: 382; Loss: 0.06678865605143186; Train Acc: 0.994140625; Test Accuracy: 0.9687\n",
            "Epoch: 383; Loss: 0.11711620653909671; Train Acc: 0.97265625; Test Accuracy: 0.9645\n",
            "Epoch: 384; Loss: 0.1297698734541371; Train Acc: 0.97265625; Test Accuracy: 0.9664\n",
            "Epoch: 385; Loss: 0.08262873128989434; Train Acc: 0.98828125; Test Accuracy: 0.9657\n",
            "Epoch: 386; Loss: 0.14124284790035396; Train Acc: 0.978515625; Test Accuracy: 0.9659\n",
            "Epoch: 387; Loss: 0.09469685465809156; Train Acc: 0.986328125; Test Accuracy: 0.9666\n",
            "Epoch: 388; Loss: 0.10060006958171577; Train Acc: 0.982421875; Test Accuracy: 0.9651\n",
            "Epoch: 389; Loss: 0.10725147862302148; Train Acc: 0.984375; Test Accuracy: 0.964\n",
            "Epoch: 390; Loss: 0.11493505775056932; Train Acc: 0.9765625; Test Accuracy: 0.9615\n",
            "Epoch: 391; Loss: 0.11627233843344442; Train Acc: 0.982421875; Test Accuracy: 0.9639\n",
            "Epoch: 392; Loss: 0.10021553168680414; Train Acc: 0.990234375; Test Accuracy: 0.9615\n",
            "Epoch: 393; Loss: 0.1086768357811568; Train Acc: 0.974609375; Test Accuracy: 0.9656\n",
            "Epoch: 394; Loss: 0.0926846253975708; Train Acc: 0.982421875; Test Accuracy: 0.9675\n",
            "Epoch: 395; Loss: 0.11475728288056516; Train Acc: 0.98046875; Test Accuracy: 0.9677\n",
            "Epoch: 396; Loss: 0.09296654940885551; Train Acc: 0.986328125; Test Accuracy: 0.9658\n",
            "Epoch: 397; Loss: 0.08768076518423554; Train Acc: 0.98828125; Test Accuracy: 0.9682\n",
            "Epoch: 398; Loss: 0.07957598159464033; Train Acc: 0.990234375; Test Accuracy: 0.9689\n",
            "Epoch: 399; Loss: 0.07125990961532125; Train Acc: 0.990234375; Test Accuracy: 0.9644\n",
            "Epoch: 400; Loss: 0.11090328326699087; Train Acc: 0.97265625; Test Accuracy: 0.963\n",
            "Epoch: 401; Loss: 0.08247379544306911; Train Acc: 0.986328125; Test Accuracy: 0.9676\n",
            "Epoch: 402; Loss: 0.08803192221724386; Train Acc: 0.994140625; Test Accuracy: 0.9678\n",
            "Epoch: 403; Loss: 0.1188966075352872; Train Acc: 0.98046875; Test Accuracy: 0.9658\n",
            "Epoch: 404; Loss: 0.07730404827156802; Train Acc: 0.98828125; Test Accuracy: 0.9666\n",
            "Epoch: 405; Loss: 0.11978439194617241; Train Acc: 0.982421875; Test Accuracy: 0.9638\n",
            "Epoch: 406; Loss: 0.10275785043730952; Train Acc: 0.986328125; Test Accuracy: 0.9666\n",
            "Epoch: 407; Loss: 0.10140340636848336; Train Acc: 0.984375; Test Accuracy: 0.9681\n",
            "Epoch: 408; Loss: 0.0580692985715905; Train Acc: 0.994140625; Test Accuracy: 0.9702\n",
            "Epoch: 409; Loss: 0.08887041550393679; Train Acc: 0.984375; Test Accuracy: 0.9684\n",
            "Epoch: 410; Loss: 0.09545445662619706; Train Acc: 0.986328125; Test Accuracy: 0.9688\n",
            "Epoch: 411; Loss: 0.1484591215544863; Train Acc: 0.974609375; Test Accuracy: 0.966\n",
            "Epoch: 412; Loss: 0.1001707812248302; Train Acc: 0.986328125; Test Accuracy: 0.9606\n",
            "Epoch: 413; Loss: 0.09680428165271382; Train Acc: 0.982421875; Test Accuracy: 0.9681\n",
            "Epoch: 414; Loss: 0.07489170216354138; Train Acc: 0.98828125; Test Accuracy: 0.9684\n",
            "Epoch: 415; Loss: 0.0769806408143961; Train Acc: 0.990234375; Test Accuracy: 0.9684\n",
            "Epoch: 416; Loss: 0.0846580769894265; Train Acc: 0.990234375; Test Accuracy: 0.9693\n",
            "Epoch: 417; Loss: 0.10174842653955338; Train Acc: 0.986328125; Test Accuracy: 0.9674\n",
            "Epoch: 418; Loss: 0.09714153235922787; Train Acc: 0.986328125; Test Accuracy: 0.9686\n",
            "Epoch: 419; Loss: 0.10969444059066742; Train Acc: 0.986328125; Test Accuracy: 0.9693\n",
            "Epoch: 420; Loss: 0.07879530261620829; Train Acc: 0.98046875; Test Accuracy: 0.9687\n",
            "Epoch: 421; Loss: 0.07558718287789798; Train Acc: 0.9921875; Test Accuracy: 0.969\n",
            "Epoch: 422; Loss: 0.0783445898190151; Train Acc: 0.984375; Test Accuracy: 0.966\n",
            "Epoch: 423; Loss: 0.0967038871370638; Train Acc: 0.978515625; Test Accuracy: 0.9673\n",
            "Epoch: 424; Loss: 0.0766821547775319; Train Acc: 0.984375; Test Accuracy: 0.969\n",
            "Epoch: 425; Loss: 0.0762879320335776; Train Acc: 0.98828125; Test Accuracy: 0.9649\n",
            "Epoch: 426; Loss: 0.07747906382439138; Train Acc: 0.9921875; Test Accuracy: 0.9688\n",
            "Epoch: 427; Loss: 0.06118606070962791; Train Acc: 0.990234375; Test Accuracy: 0.9682\n",
            "Epoch: 428; Loss: 0.054489361768402314; Train Acc: 0.990234375; Test Accuracy: 0.9691\n",
            "Epoch: 429; Loss: 0.06876452566914579; Train Acc: 0.994140625; Test Accuracy: 0.9682\n",
            "Epoch: 430; Loss: 0.07238675590606934; Train Acc: 0.986328125; Test Accuracy: 0.9695\n",
            "Epoch: 431; Loss: 0.07252740365150011; Train Acc: 0.990234375; Test Accuracy: 0.9697\n",
            "Epoch: 432; Loss: 0.10678654288723725; Train Acc: 0.986328125; Test Accuracy: 0.9664\n",
            "Epoch: 433; Loss: 0.09538856244143462; Train Acc: 0.98828125; Test Accuracy: 0.9695\n",
            "Epoch: 434; Loss: 0.1118417567053916; Train Acc: 0.986328125; Test Accuracy: 0.9702\n",
            "Epoch: 435; Loss: 0.099265496442599; Train Acc: 0.984375; Test Accuracy: 0.9671\n",
            "Epoch: 436; Loss: 0.0724096914529692; Train Acc: 0.990234375; Test Accuracy: 0.9682\n",
            "Epoch: 437; Loss: 0.08045771964952333; Train Acc: 0.990234375; Test Accuracy: 0.9681\n",
            "Epoch: 438; Loss: 0.07707188287968006; Train Acc: 0.994140625; Test Accuracy: 0.9692\n",
            "Epoch: 439; Loss: 0.07752438523262908; Train Acc: 0.994140625; Test Accuracy: 0.9686\n",
            "Epoch: 440; Loss: 0.11673223187951397; Train Acc: 0.982421875; Test Accuracy: 0.9715\n",
            "Epoch: 441; Loss: 0.0697333413064713; Train Acc: 0.994140625; Test Accuracy: 0.9675\n",
            "Epoch: 442; Loss: 0.07028334731739125; Train Acc: 0.9921875; Test Accuracy: 0.9656\n",
            "Epoch: 443; Loss: 0.08787873030794238; Train Acc: 0.986328125; Test Accuracy: 0.9685\n",
            "Epoch: 444; Loss: 0.08260683316913606; Train Acc: 0.990234375; Test Accuracy: 0.9689\n",
            "Epoch: 445; Loss: 0.0660504961935587; Train Acc: 0.98828125; Test Accuracy: 0.9693\n",
            "Epoch: 446; Loss: 0.1372587147676136; Train Acc: 0.982421875; Test Accuracy: 0.9613\n",
            "Epoch: 447; Loss: 0.10805781220904134; Train Acc: 0.978515625; Test Accuracy: 0.9663\n",
            "Epoch: 448; Loss: 0.08162582684188455; Train Acc: 0.9921875; Test Accuracy: 0.9666\n",
            "Epoch: 449; Loss: 0.08419620921519642; Train Acc: 0.990234375; Test Accuracy: 0.9677\n",
            "Epoch: 450; Loss: 0.10031809215471706; Train Acc: 0.982421875; Test Accuracy: 0.9673\n",
            "Epoch: 451; Loss: 0.08399917500213952; Train Acc: 0.986328125; Test Accuracy: 0.9669\n",
            "Epoch: 452; Loss: 0.076415950268782; Train Acc: 0.994140625; Test Accuracy: 0.9703\n",
            "Epoch: 453; Loss: 0.04677923863451437; Train Acc: 0.994140625; Test Accuracy: 0.9692\n",
            "Epoch: 454; Loss: 0.06669260977149277; Train Acc: 0.99609375; Test Accuracy: 0.97\n",
            "Epoch: 455; Loss: 0.05459761738284315; Train Acc: 0.998046875; Test Accuracy: 0.97\n",
            "Epoch: 456; Loss: 0.06102985015954299; Train Acc: 0.99609375; Test Accuracy: 0.9707\n",
            "Epoch: 457; Loss: 0.10091929262624763; Train Acc: 0.984375; Test Accuracy: 0.9693\n",
            "Epoch: 458; Loss: 0.11558136534180449; Train Acc: 0.98046875; Test Accuracy: 0.9591\n",
            "Epoch: 459; Loss: 0.12057438843448466; Train Acc: 0.97265625; Test Accuracy: 0.9656\n",
            "Epoch: 460; Loss: 0.05472027033425291; Train Acc: 0.998046875; Test Accuracy: 0.9723\n",
            "Epoch: 461; Loss: 0.05560201224170478; Train Acc: 0.9921875; Test Accuracy: 0.9715\n",
            "Epoch: 462; Loss: 0.04703161217910169; Train Acc: 0.998046875; Test Accuracy: 0.9713\n",
            "Epoch: 463; Loss: 0.04651402484824258; Train Acc: 0.99609375; Test Accuracy: 0.971\n",
            "Epoch: 464; Loss: 0.05312460590835994; Train Acc: 0.998046875; Test Accuracy: 0.9708\n",
            "Epoch: 465; Loss: 0.08086162559109027; Train Acc: 0.9921875; Test Accuracy: 0.9699\n",
            "Epoch: 466; Loss: 0.089156367570734; Train Acc: 0.982421875; Test Accuracy: 0.9685\n",
            "Epoch: 467; Loss: 0.08589220450765171; Train Acc: 0.98828125; Test Accuracy: 0.9662\n",
            "Epoch: 468; Loss: 0.11273583149435894; Train Acc: 0.9765625; Test Accuracy: 0.967\n",
            "Epoch: 469; Loss: 0.08959241535418397; Train Acc: 0.990234375; Test Accuracy: 0.9724\n",
            "Epoch: 470; Loss: 0.08058572182842011; Train Acc: 0.9921875; Test Accuracy: 0.9692\n",
            "Epoch: 471; Loss: 0.073551209137426; Train Acc: 0.98828125; Test Accuracy: 0.9715\n",
            "Epoch: 472; Loss: 0.0443021062645631; Train Acc: 0.99609375; Test Accuracy: 0.9695\n",
            "Epoch: 473; Loss: 0.07473110501659719; Train Acc: 0.9921875; Test Accuracy: 0.9703\n",
            "Epoch: 474; Loss: 0.05508996633433282; Train Acc: 0.994140625; Test Accuracy: 0.9702\n",
            "Epoch: 475; Loss: 0.07372191828368363; Train Acc: 0.9921875; Test Accuracy: 0.971\n",
            "Epoch: 476; Loss: 0.06595289274237218; Train Acc: 0.990234375; Test Accuracy: 0.9695\n",
            "Epoch: 477; Loss: 0.09053224853382147; Train Acc: 0.986328125; Test Accuracy: 0.9698\n",
            "Epoch: 478; Loss: 0.08363880703984641; Train Acc: 0.984375; Test Accuracy: 0.9708\n",
            "Epoch: 479; Loss: 0.06666950414393849; Train Acc: 0.986328125; Test Accuracy: 0.9679\n",
            "Epoch: 480; Loss: 0.11581230694467602; Train Acc: 0.986328125; Test Accuracy: 0.9687\n",
            "Epoch: 481; Loss: 0.076090572741129; Train Acc: 0.990234375; Test Accuracy: 0.9702\n",
            "Epoch: 482; Loss: 0.09521805537201758; Train Acc: 0.984375; Test Accuracy: 0.9606\n",
            "Epoch: 483; Loss: 0.06634942752603741; Train Acc: 0.9921875; Test Accuracy: 0.9682\n",
            "Epoch: 484; Loss: 0.05727411689809661; Train Acc: 0.990234375; Test Accuracy: 0.9705\n",
            "Epoch: 485; Loss: 0.11167220000715683; Train Acc: 0.974609375; Test Accuracy: 0.9644\n",
            "Epoch: 486; Loss: 0.08151012295446369; Train Acc: 0.986328125; Test Accuracy: 0.9697\n",
            "Epoch: 487; Loss: 0.10600234641575319; Train Acc: 0.9921875; Test Accuracy: 0.9683\n",
            "Epoch: 488; Loss: 0.05958277679693322; Train Acc: 0.990234375; Test Accuracy: 0.9714\n",
            "Epoch: 489; Loss: 0.07394287241831088; Train Acc: 0.98828125; Test Accuracy: 0.9697\n",
            "Epoch: 490; Loss: 0.11948719607629839; Train Acc: 0.9765625; Test Accuracy: 0.9689\n",
            "Epoch: 491; Loss: 0.061517155562977724; Train Acc: 0.994140625; Test Accuracy: 0.9727\n",
            "Epoch: 492; Loss: 0.06994962914946425; Train Acc: 0.98828125; Test Accuracy: 0.973\n",
            "Epoch: 493; Loss: 0.06062865274038974; Train Acc: 0.994140625; Test Accuracy: 0.9732\n",
            "Epoch: 494; Loss: 0.0706570544261809; Train Acc: 0.990234375; Test Accuracy: 0.9718\n",
            "Epoch: 495; Loss: 0.06334257081269547; Train Acc: 0.9921875; Test Accuracy: 0.9697\n",
            "Epoch: 496; Loss: 0.09128263128568956; Train Acc: 0.982421875; Test Accuracy: 0.9715\n",
            "Epoch: 497; Loss: 0.08647530625038566; Train Acc: 0.98828125; Test Accuracy: 0.9686\n",
            "Epoch: 498; Loss: 0.09863593873360751; Train Acc: 0.986328125; Test Accuracy: 0.969\n",
            "Epoch: 499; Loss: 0.1004162029179005; Train Acc: 0.986328125; Test Accuracy: 0.9706\n",
            "Epoch: 500; Loss: 0.11308081591060579; Train Acc: 0.9765625; Test Accuracy: 0.9726\n",
            "Epoch: 501; Loss: 0.07061839403436822; Train Acc: 0.99609375; Test Accuracy: 0.9708\n",
            "Epoch: 502; Loss: 0.0719083446543117; Train Acc: 0.986328125; Test Accuracy: 0.9701\n",
            "Epoch: 503; Loss: 0.09922758502235327; Train Acc: 0.98828125; Test Accuracy: 0.966\n",
            "Epoch: 504; Loss: 0.11698442695469866; Train Acc: 0.978515625; Test Accuracy: 0.9663\n",
            "Epoch: 505; Loss: 0.07879040629193382; Train Acc: 0.994140625; Test Accuracy: 0.9706\n",
            "Epoch: 506; Loss: 0.08147456426771704; Train Acc: 0.986328125; Test Accuracy: 0.9713\n",
            "Epoch: 507; Loss: 0.05375873655855248; Train Acc: 0.998046875; Test Accuracy: 0.9717\n",
            "Epoch: 508; Loss: 0.08983602925342601; Train Acc: 0.98828125; Test Accuracy: 0.9704\n",
            "Epoch: 509; Loss: 0.09564290174237616; Train Acc: 0.986328125; Test Accuracy: 0.9706\n",
            "Epoch: 510; Loss: 0.09547901873793904; Train Acc: 0.98828125; Test Accuracy: 0.972\n",
            "Epoch: 511; Loss: 0.0681757471651633; Train Acc: 0.994140625; Test Accuracy: 0.966\n",
            "Epoch: 512; Loss: 0.05295112406989985; Train Acc: 0.994140625; Test Accuracy: 0.9714\n",
            "Epoch: 513; Loss: 0.08067685226549404; Train Acc: 0.98828125; Test Accuracy: 0.9715\n",
            "Epoch: 514; Loss: 0.06908414724587852; Train Acc: 0.990234375; Test Accuracy: 0.9712\n",
            "Epoch: 515; Loss: 0.09653632769137056; Train Acc: 0.990234375; Test Accuracy: 0.9698\n",
            "Epoch: 516; Loss: 0.07336668706068997; Train Acc: 0.994140625; Test Accuracy: 0.9712\n",
            "Epoch: 517; Loss: 0.04972790853593223; Train Acc: 0.994140625; Test Accuracy: 0.9701\n",
            "Epoch: 518; Loss: 0.07994914321207407; Train Acc: 0.98828125; Test Accuracy: 0.9704\n",
            "Epoch: 519; Loss: 0.06526190399778567; Train Acc: 0.990234375; Test Accuracy: 0.9708\n",
            "Epoch: 520; Loss: 0.05856007194147779; Train Acc: 0.99609375; Test Accuracy: 0.9706\n",
            "Epoch: 521; Loss: 0.055490397057959044; Train Acc: 0.99609375; Test Accuracy: 0.9726\n",
            "Epoch: 522; Loss: 0.07919840187530779; Train Acc: 0.990234375; Test Accuracy: 0.9713\n",
            "Epoch: 523; Loss: 0.09380253134399842; Train Acc: 0.986328125; Test Accuracy: 0.9717\n",
            "Epoch: 524; Loss: 0.10176047225534468; Train Acc: 0.986328125; Test Accuracy: 0.967\n",
            "Epoch: 525; Loss: 0.1073769043737769; Train Acc: 0.98828125; Test Accuracy: 0.9713\n",
            "Epoch: 526; Loss: 0.07460737306861985; Train Acc: 0.994140625; Test Accuracy: 0.9707\n",
            "Epoch: 527; Loss: 0.09942290241460403; Train Acc: 0.98828125; Test Accuracy: 0.97\n",
            "Epoch: 528; Loss: 0.058898994710327474; Train Acc: 0.994140625; Test Accuracy: 0.9716\n",
            "Epoch: 529; Loss: 0.07614283970703237; Train Acc: 0.9921875; Test Accuracy: 0.9707\n",
            "Epoch: 530; Loss: 0.05402843362577575; Train Acc: 0.9921875; Test Accuracy: 0.973\n",
            "Epoch: 531; Loss: 0.07026320328969096; Train Acc: 0.990234375; Test Accuracy: 0.9728\n",
            "Epoch: 532; Loss: 0.05576697379260959; Train Acc: 0.998046875; Test Accuracy: 0.9718\n",
            "Epoch: 533; Loss: 0.061207084981213064; Train Acc: 0.994140625; Test Accuracy: 0.9695\n",
            "Epoch: 534; Loss: 0.05817922300786623; Train Acc: 0.98828125; Test Accuracy: 0.9733\n",
            "Epoch: 535; Loss: 0.04573653938610339; Train Acc: 0.994140625; Test Accuracy: 0.9719\n",
            "Epoch: 536; Loss: 0.04289627372147505; Train Acc: 1.0; Test Accuracy: 0.9713\n",
            "Epoch: 537; Loss: 0.07454755678650357; Train Acc: 0.990234375; Test Accuracy: 0.9703\n",
            "Epoch: 538; Loss: 0.08951152949113167; Train Acc: 0.986328125; Test Accuracy: 0.9727\n",
            "Epoch: 539; Loss: 0.07524808936772745; Train Acc: 0.99609375; Test Accuracy: 0.9693\n",
            "Epoch: 540; Loss: 0.06960356455337792; Train Acc: 0.9921875; Test Accuracy: 0.9718\n",
            "Epoch: 541; Loss: 0.07826244520776504; Train Acc: 0.9921875; Test Accuracy: 0.97\n",
            "Epoch: 542; Loss: 0.03589168010745762; Train Acc: 0.99609375; Test Accuracy: 0.9708\n",
            "Epoch: 543; Loss: 0.0937914698817088; Train Acc: 0.990234375; Test Accuracy: 0.97\n",
            "Epoch: 544; Loss: 0.09145036554886407; Train Acc: 0.986328125; Test Accuracy: 0.9695\n",
            "Epoch: 545; Loss: 0.0554613649101705; Train Acc: 0.9921875; Test Accuracy: 0.9709\n",
            "Epoch: 546; Loss: 0.078231865142467; Train Acc: 0.994140625; Test Accuracy: 0.9713\n",
            "Epoch: 547; Loss: 0.041843930192954355; Train Acc: 0.99609375; Test Accuracy: 0.9729\n",
            "Epoch: 548; Loss: 0.04961174757293703; Train Acc: 0.98828125; Test Accuracy: 0.9719\n",
            "Epoch: 549; Loss: 0.06089386655924593; Train Acc: 0.9921875; Test Accuracy: 0.9721\n",
            "Epoch: 550; Loss: 0.05643467007752738; Train Acc: 0.99609375; Test Accuracy: 0.9717\n",
            "Epoch: 551; Loss: 0.04306841488044852; Train Acc: 0.998046875; Test Accuracy: 0.9723\n",
            "Epoch: 552; Loss: 0.053748602618143466; Train Acc: 0.9921875; Test Accuracy: 0.9702\n",
            "Epoch: 553; Loss: 0.06077472617535319; Train Acc: 0.9921875; Test Accuracy: 0.9713\n",
            "Epoch: 554; Loss: 0.14658757728750285; Train Acc: 0.984375; Test Accuracy: 0.9676\n",
            "Epoch: 555; Loss: 0.07181718556714535; Train Acc: 0.9921875; Test Accuracy: 0.9731\n",
            "Epoch: 556; Loss: 0.06785143647638225; Train Acc: 0.990234375; Test Accuracy: 0.9712\n",
            "Epoch: 557; Loss: 0.06937763313481829; Train Acc: 0.990234375; Test Accuracy: 0.9725\n",
            "Epoch: 558; Loss: 0.06040859325080051; Train Acc: 0.994140625; Test Accuracy: 0.9704\n",
            "Epoch: 559; Loss: 0.07557966795725037; Train Acc: 0.982421875; Test Accuracy: 0.9683\n",
            "Epoch: 560; Loss: 0.07270423682113417; Train Acc: 0.986328125; Test Accuracy: 0.9716\n",
            "Epoch: 561; Loss: 0.06694692344240813; Train Acc: 0.990234375; Test Accuracy: 0.9712\n",
            "Epoch: 562; Loss: 0.09058454146342688; Train Acc: 0.98828125; Test Accuracy: 0.9718\n",
            "Epoch: 563; Loss: 0.06647471965670956; Train Acc: 0.9921875; Test Accuracy: 0.9702\n",
            "Epoch: 564; Loss: 0.07638141391042297; Train Acc: 0.990234375; Test Accuracy: 0.9721\n",
            "Epoch: 565; Loss: 0.10780068020818047; Train Acc: 0.982421875; Test Accuracy: 0.9708\n",
            "Epoch: 566; Loss: 0.07098398404530522; Train Acc: 0.994140625; Test Accuracy: 0.9725\n",
            "Epoch: 567; Loss: 0.056627874330133846; Train Acc: 0.994140625; Test Accuracy: 0.972\n",
            "Epoch: 568; Loss: 0.06231228448716987; Train Acc: 0.99609375; Test Accuracy: 0.9723\n",
            "Epoch: 569; Loss: 0.07189991501733932; Train Acc: 0.990234375; Test Accuracy: 0.9712\n",
            "Epoch: 570; Loss: 0.07732106160620299; Train Acc: 0.986328125; Test Accuracy: 0.9713\n",
            "Epoch: 571; Loss: 0.07241893925513422; Train Acc: 0.9921875; Test Accuracy: 0.9726\n",
            "Epoch: 572; Loss: 0.07171479647789918; Train Acc: 0.994140625; Test Accuracy: 0.9726\n",
            "Epoch: 573; Loss: 0.06558828755409751; Train Acc: 0.9921875; Test Accuracy: 0.9712\n",
            "Epoch: 574; Loss: 0.09113405654961987; Train Acc: 0.994140625; Test Accuracy: 0.9694\n",
            "Epoch: 575; Loss: 0.06934791366689247; Train Acc: 0.984375; Test Accuracy: 0.9711\n",
            "Epoch: 576; Loss: 0.08700611854084857; Train Acc: 0.9921875; Test Accuracy: 0.9721\n",
            "Epoch: 577; Loss: 0.09073144674081704; Train Acc: 0.990234375; Test Accuracy: 0.9736\n",
            "Epoch: 578; Loss: 0.058088643278140284; Train Acc: 0.990234375; Test Accuracy: 0.9741\n",
            "Epoch: 579; Loss: 0.048824496902349315; Train Acc: 0.994140625; Test Accuracy: 0.9732\n",
            "Epoch: 580; Loss: 0.06543857647842867; Train Acc: 0.9921875; Test Accuracy: 0.974\n",
            "Epoch: 581; Loss: 0.06929370947525579; Train Acc: 0.98828125; Test Accuracy: 0.9714\n",
            "Epoch: 582; Loss: 0.09134269448300308; Train Acc: 0.986328125; Test Accuracy: 0.9744\n",
            "Epoch: 583; Loss: 0.0902765657313726; Train Acc: 0.9921875; Test Accuracy: 0.9708\n",
            "Epoch: 584; Loss: 0.0960920256273106; Train Acc: 0.978515625; Test Accuracy: 0.9736\n",
            "Epoch: 585; Loss: 0.07197503037121887; Train Acc: 0.9921875; Test Accuracy: 0.9752\n",
            "Epoch: 586; Loss: 0.07714328905323355; Train Acc: 0.986328125; Test Accuracy: 0.9743\n",
            "Epoch: 587; Loss: 0.05948996237178884; Train Acc: 0.99609375; Test Accuracy: 0.973\n",
            "Epoch: 588; Loss: 0.10112416996429296; Train Acc: 0.984375; Test Accuracy: 0.9714\n",
            "Epoch: 589; Loss: 0.07082935271108559; Train Acc: 0.990234375; Test Accuracy: 0.9726\n",
            "Epoch: 590; Loss: 0.0815203971278928; Train Acc: 0.990234375; Test Accuracy: 0.9736\n",
            "Epoch: 591; Loss: 0.062465648463013126; Train Acc: 0.994140625; Test Accuracy: 0.9733\n",
            "Epoch: 592; Loss: 0.047441639900942634; Train Acc: 0.99609375; Test Accuracy: 0.9733\n",
            "Epoch: 593; Loss: 0.11239188328088733; Train Acc: 0.9765625; Test Accuracy: 0.9712\n",
            "Epoch: 594; Loss: 0.08065295398794256; Train Acc: 0.9921875; Test Accuracy: 0.9749\n",
            "Epoch: 595; Loss: 0.06859712299979638; Train Acc: 0.994140625; Test Accuracy: 0.974\n",
            "Epoch: 596; Loss: 0.05495121450587838; Train Acc: 0.99609375; Test Accuracy: 0.9724\n",
            "Epoch: 597; Loss: 0.05143859068705185; Train Acc: 0.9921875; Test Accuracy: 0.9751\n",
            "Epoch: 598; Loss: 0.07488500478205606; Train Acc: 0.994140625; Test Accuracy: 0.9717\n",
            "Epoch: 599; Loss: 0.08927213220912815; Train Acc: 0.986328125; Test Accuracy: 0.9729\n",
            "Epoch: 600; Loss: 0.0723486640225465; Train Acc: 0.994140625; Test Accuracy: 0.9695\n",
            "Epoch: 601; Loss: 0.05626782160183174; Train Acc: 0.99609375; Test Accuracy: 0.972\n",
            "Epoch: 602; Loss: 0.09123944751356383; Train Acc: 0.990234375; Test Accuracy: 0.9707\n",
            "Epoch: 603; Loss: 0.056352584863975125; Train Acc: 0.994140625; Test Accuracy: 0.9715\n",
            "Epoch: 604; Loss: 0.07341920613658029; Train Acc: 0.990234375; Test Accuracy: 0.9712\n",
            "Epoch: 605; Loss: 0.053034735764229915; Train Acc: 0.99609375; Test Accuracy: 0.9732\n",
            "Epoch: 606; Loss: 0.040915724729552086; Train Acc: 0.99609375; Test Accuracy: 0.9742\n",
            "Epoch: 607; Loss: 0.07588667447495404; Train Acc: 0.990234375; Test Accuracy: 0.9721\n",
            "Epoch: 608; Loss: 0.06138828797330645; Train Acc: 0.9921875; Test Accuracy: 0.9696\n",
            "Epoch: 609; Loss: 0.06533086196016866; Train Acc: 0.98828125; Test Accuracy: 0.9731\n",
            "Epoch: 610; Loss: 0.048813688071993974; Train Acc: 0.99609375; Test Accuracy: 0.9742\n",
            "Epoch: 611; Loss: 0.03888474837609461; Train Acc: 0.99609375; Test Accuracy: 0.973\n",
            "Epoch: 612; Loss: 0.07861594450316; Train Acc: 0.9921875; Test Accuracy: 0.9735\n",
            "Epoch: 613; Loss: 0.07466041570789571; Train Acc: 0.98828125; Test Accuracy: 0.9716\n",
            "Epoch: 614; Loss: 0.09026806209946005; Train Acc: 0.978515625; Test Accuracy: 0.9703\n",
            "Epoch: 615; Loss: 0.053719642317724026; Train Acc: 0.990234375; Test Accuracy: 0.9747\n",
            "Epoch: 616; Loss: 0.038883250918107876; Train Acc: 1.0; Test Accuracy: 0.972\n",
            "Epoch: 617; Loss: 0.05480994746808471; Train Acc: 0.99609375; Test Accuracy: 0.9708\n",
            "Epoch: 618; Loss: 0.09827844474297596; Train Acc: 0.98828125; Test Accuracy: 0.9696\n",
            "Epoch: 619; Loss: 0.0879713108068439; Train Acc: 0.970703125; Test Accuracy: 0.9664\n",
            "Epoch: 620; Loss: 0.08553505320904752; Train Acc: 0.994140625; Test Accuracy: 0.9717\n",
            "Epoch: 621; Loss: 0.07180760000611158; Train Acc: 0.990234375; Test Accuracy: 0.9733\n",
            "Epoch: 622; Loss: 0.09117770754571795; Train Acc: 0.984375; Test Accuracy: 0.9718\n",
            "Epoch: 623; Loss: 0.06954604088120336; Train Acc: 0.990234375; Test Accuracy: 0.9716\n",
            "Epoch: 624; Loss: 0.057526638506780435; Train Acc: 0.99609375; Test Accuracy: 0.9729\n",
            "Epoch: 625; Loss: 0.07531518287820295; Train Acc: 0.9921875; Test Accuracy: 0.9744\n",
            "Epoch: 626; Loss: 0.057613435380478355; Train Acc: 0.998046875; Test Accuracy: 0.9718\n",
            "Epoch: 627; Loss: 0.09913428561906598; Train Acc: 0.98828125; Test Accuracy: 0.9725\n",
            "Epoch: 628; Loss: 0.04974090334504969; Train Acc: 0.998046875; Test Accuracy: 0.9741\n",
            "Epoch: 629; Loss: 0.049351595549908873; Train Acc: 0.998046875; Test Accuracy: 0.9736\n",
            "Epoch: 630; Loss: 0.041904828317303924; Train Acc: 1.0; Test Accuracy: 0.9726\n",
            "Epoch: 631; Loss: 0.07558583698358869; Train Acc: 0.990234375; Test Accuracy: 0.9739\n",
            "Epoch: 632; Loss: 0.03406821892768781; Train Acc: 1.0; Test Accuracy: 0.9745\n",
            "Epoch: 633; Loss: 0.040156159353562836; Train Acc: 0.998046875; Test Accuracy: 0.9737\n",
            "Epoch: 634; Loss: 0.0655953763543318; Train Acc: 0.990234375; Test Accuracy: 0.9724\n",
            "Epoch: 635; Loss: 0.051722634883913; Train Acc: 0.99609375; Test Accuracy: 0.9718\n",
            "Epoch: 636; Loss: 0.0700735093640642; Train Acc: 0.990234375; Test Accuracy: 0.9738\n",
            "Epoch: 637; Loss: 0.044097512972134545; Train Acc: 0.99609375; Test Accuracy: 0.9738\n",
            "Epoch: 638; Loss: 0.04278328202774701; Train Acc: 0.99609375; Test Accuracy: 0.9719\n",
            "Epoch: 639; Loss: 0.08143625005040961; Train Acc: 0.98828125; Test Accuracy: 0.9728\n",
            "Epoch: 640; Loss: 0.05467005275262282; Train Acc: 0.998046875; Test Accuracy: 0.9731\n",
            "Epoch: 641; Loss: 0.09617182222790044; Train Acc: 0.98828125; Test Accuracy: 0.9709\n",
            "Epoch: 642; Loss: 0.07286388150658342; Train Acc: 0.9921875; Test Accuracy: 0.9729\n",
            "Epoch: 643; Loss: 0.08743345392783991; Train Acc: 0.984375; Test Accuracy: 0.9733\n",
            "Epoch: 644; Loss: 0.06413656652674954; Train Acc: 0.99609375; Test Accuracy: 0.97\n",
            "Epoch: 645; Loss: 0.08830523848309006; Train Acc: 0.990234375; Test Accuracy: 0.9724\n",
            "Epoch: 646; Loss: 0.047900759618279415; Train Acc: 0.99609375; Test Accuracy: 0.9723\n",
            "Epoch: 647; Loss: 0.06378298599032517; Train Acc: 0.990234375; Test Accuracy: 0.9742\n",
            "Epoch: 648; Loss: 0.03991518205661447; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 649; Loss: 0.11928692819657585; Train Acc: 0.978515625; Test Accuracy: 0.973\n",
            "Epoch: 650; Loss: 0.08401857047519833; Train Acc: 0.9921875; Test Accuracy: 0.9713\n",
            "Epoch: 651; Loss: 0.08554981698207015; Train Acc: 0.990234375; Test Accuracy: 0.974\n",
            "Epoch: 652; Loss: 0.053931227035852985; Train Acc: 0.9921875; Test Accuracy: 0.9731\n",
            "Epoch: 653; Loss: 0.0337690258547318; Train Acc: 1.0; Test Accuracy: 0.9747\n",
            "Epoch: 654; Loss: 0.04523024665212215; Train Acc: 0.998046875; Test Accuracy: 0.9728\n",
            "Epoch: 655; Loss: 0.051706237428046295; Train Acc: 0.994140625; Test Accuracy: 0.9725\n",
            "Epoch: 656; Loss: 0.05228151285382103; Train Acc: 0.994140625; Test Accuracy: 0.9722\n",
            "Epoch: 657; Loss: 0.05935996067409301; Train Acc: 0.99609375; Test Accuracy: 0.9739\n",
            "Epoch: 658; Loss: 0.061770141272775955; Train Acc: 0.9921875; Test Accuracy: 0.9745\n",
            "Epoch: 659; Loss: 0.0514636688324675; Train Acc: 0.99609375; Test Accuracy: 0.9725\n",
            "Epoch: 660; Loss: 0.06589390490866417; Train Acc: 0.990234375; Test Accuracy: 0.973\n",
            "Epoch: 661; Loss: 0.06707801100349393; Train Acc: 0.98828125; Test Accuracy: 0.9737\n",
            "Epoch: 662; Loss: 0.052116019578558465; Train Acc: 0.99609375; Test Accuracy: 0.9739\n",
            "Epoch: 663; Loss: 0.06863994971236739; Train Acc: 0.9921875; Test Accuracy: 0.9732\n",
            "Epoch: 664; Loss: 0.05148881036715792; Train Acc: 0.998046875; Test Accuracy: 0.9735\n",
            "Epoch: 665; Loss: 0.04240866256707086; Train Acc: 0.998046875; Test Accuracy: 0.9728\n",
            "Epoch: 666; Loss: 0.04496053039684881; Train Acc: 0.998046875; Test Accuracy: 0.9728\n",
            "Epoch: 667; Loss: 0.055754359336187664; Train Acc: 0.99609375; Test Accuracy: 0.9704\n",
            "Epoch: 668; Loss: 0.06880323519420009; Train Acc: 0.990234375; Test Accuracy: 0.9708\n",
            "Epoch: 669; Loss: 0.08020977915488861; Train Acc: 0.9921875; Test Accuracy: 0.9725\n",
            "Epoch: 670; Loss: 0.0490882043069391; Train Acc: 0.998046875; Test Accuracy: 0.9727\n",
            "Epoch: 671; Loss: 0.07011517165540194; Train Acc: 0.986328125; Test Accuracy: 0.969\n",
            "Epoch: 672; Loss: 0.08960987264881097; Train Acc: 0.98828125; Test Accuracy: 0.971\n",
            "Epoch: 673; Loss: 0.07658617733603552; Train Acc: 0.994140625; Test Accuracy: 0.972\n",
            "Epoch: 674; Loss: 0.04666249223360655; Train Acc: 0.99609375; Test Accuracy: 0.9722\n",
            "Epoch: 675; Loss: 0.0425525377080465; Train Acc: 0.998046875; Test Accuracy: 0.9736\n",
            "Epoch: 676; Loss: 0.048936291397973425; Train Acc: 0.994140625; Test Accuracy: 0.9735\n",
            "Epoch: 677; Loss: 0.07990983098187443; Train Acc: 0.98828125; Test Accuracy: 0.9732\n",
            "Epoch: 678; Loss: 0.05555533503817087; Train Acc: 0.99609375; Test Accuracy: 0.9724\n",
            "Epoch: 679; Loss: 0.03664275575448422; Train Acc: 0.99609375; Test Accuracy: 0.9725\n",
            "Epoch: 680; Loss: 0.05609203476690263; Train Acc: 0.9921875; Test Accuracy: 0.9712\n",
            "Epoch: 681; Loss: 0.05243379438122987; Train Acc: 0.99609375; Test Accuracy: 0.9731\n",
            "Epoch: 682; Loss: 0.08746517359144984; Train Acc: 0.9921875; Test Accuracy: 0.9727\n",
            "Epoch: 683; Loss: 0.04987012340768627; Train Acc: 0.99609375; Test Accuracy: 0.9739\n",
            "Epoch: 684; Loss: 0.046592063958751526; Train Acc: 0.9921875; Test Accuracy: 0.9737\n",
            "Epoch: 685; Loss: 0.03374938674193574; Train Acc: 0.998046875; Test Accuracy: 0.9739\n",
            "Epoch: 686; Loss: 0.036521919250867146; Train Acc: 0.998046875; Test Accuracy: 0.9737\n",
            "Epoch: 687; Loss: 0.04597114620363224; Train Acc: 0.998046875; Test Accuracy: 0.9737\n",
            "Epoch: 688; Loss: 0.06256057246348518; Train Acc: 0.994140625; Test Accuracy: 0.9718\n",
            "Epoch: 689; Loss: 0.08169480822508399; Train Acc: 0.986328125; Test Accuracy: 0.9738\n",
            "Epoch: 690; Loss: 0.04792382414361717; Train Acc: 0.99609375; Test Accuracy: 0.972\n",
            "Epoch: 691; Loss: 0.05793593076454677; Train Acc: 0.990234375; Test Accuracy: 0.9727\n",
            "Epoch: 692; Loss: 0.07146454117803884; Train Acc: 0.994140625; Test Accuracy: 0.9709\n",
            "Epoch: 693; Loss: 0.0722429686322513; Train Acc: 0.990234375; Test Accuracy: 0.9725\n",
            "Epoch: 694; Loss: 0.03261988625470484; Train Acc: 0.99609375; Test Accuracy: 0.9737\n",
            "Epoch: 695; Loss: 0.03652511430893262; Train Acc: 0.998046875; Test Accuracy: 0.9742\n",
            "Epoch: 696; Loss: 0.04898600906286854; Train Acc: 0.99609375; Test Accuracy: 0.9745\n",
            "Epoch: 697; Loss: 0.04348253966227312; Train Acc: 0.99609375; Test Accuracy: 0.9709\n",
            "Epoch: 698; Loss: 0.06502483508695436; Train Acc: 0.990234375; Test Accuracy: 0.9734\n",
            "Epoch: 699; Loss: 0.053711137138887935; Train Acc: 0.99609375; Test Accuracy: 0.9741\n",
            "Epoch: 700; Loss: 0.050919608952166975; Train Acc: 0.994140625; Test Accuracy: 0.9744\n",
            "Epoch: 701; Loss: 0.04103041603872939; Train Acc: 1.0; Test Accuracy: 0.9747\n",
            "Epoch: 702; Loss: 0.05422759021122685; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 703; Loss: 0.04595996029337861; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 704; Loss: 0.04360000071200418; Train Acc: 0.998046875; Test Accuracy: 0.9734\n",
            "Epoch: 705; Loss: 0.09912290583661826; Train Acc: 0.986328125; Test Accuracy: 0.9743\n",
            "Epoch: 706; Loss: 0.03078951473357339; Train Acc: 1.0; Test Accuracy: 0.974\n",
            "Epoch: 707; Loss: 0.05741472154544359; Train Acc: 0.98828125; Test Accuracy: 0.9741\n",
            "Epoch: 708; Loss: 0.08906932521849097; Train Acc: 0.978515625; Test Accuracy: 0.965\n",
            "Epoch: 709; Loss: 0.10062436707430243; Train Acc: 0.984375; Test Accuracy: 0.9692\n",
            "Epoch: 710; Loss: 0.07573884887063029; Train Acc: 0.98828125; Test Accuracy: 0.974\n",
            "Epoch: 711; Loss: 0.052812765381279575; Train Acc: 0.99609375; Test Accuracy: 0.9747\n",
            "Epoch: 712; Loss: 0.055855894876376086; Train Acc: 0.9921875; Test Accuracy: 0.9717\n",
            "Epoch: 713; Loss: 0.04157909213816742; Train Acc: 0.9921875; Test Accuracy: 0.9741\n",
            "Epoch: 714; Loss: 0.058300282959021454; Train Acc: 0.998046875; Test Accuracy: 0.9747\n",
            "Epoch: 715; Loss: 0.05489209011903512; Train Acc: 0.99609375; Test Accuracy: 0.9749\n",
            "Epoch: 716; Loss: 0.061916854921014716; Train Acc: 0.986328125; Test Accuracy: 0.9679\n",
            "Epoch: 717; Loss: 0.058529513835664695; Train Acc: 0.99609375; Test Accuracy: 0.9723\n",
            "Epoch: 718; Loss: 0.044683929542619655; Train Acc: 1.0; Test Accuracy: 0.9736\n",
            "Epoch: 719; Loss: 0.048087541768550464; Train Acc: 0.998046875; Test Accuracy: 0.9736\n",
            "Epoch: 720; Loss: 0.05783341772608509; Train Acc: 0.98828125; Test Accuracy: 0.9738\n",
            "Epoch: 721; Loss: 0.05351809831575194; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 722; Loss: 0.04921312162277791; Train Acc: 0.994140625; Test Accuracy: 0.9728\n",
            "Epoch: 723; Loss: 0.0432215172368105; Train Acc: 0.99609375; Test Accuracy: 0.9754\n",
            "Epoch: 724; Loss: 0.03644405597145986; Train Acc: 0.998046875; Test Accuracy: 0.9746\n",
            "Epoch: 725; Loss: 0.043964895316841085; Train Acc: 0.99609375; Test Accuracy: 0.9748\n",
            "Epoch: 726; Loss: 0.04767719317716527; Train Acc: 0.9921875; Test Accuracy: 0.9725\n",
            "Epoch: 727; Loss: 0.07785073147629941; Train Acc: 0.98828125; Test Accuracy: 0.9727\n",
            "Epoch: 728; Loss: 0.06193527386831701; Train Acc: 0.9921875; Test Accuracy: 0.9739\n",
            "Epoch: 729; Loss: 0.035806004650133325; Train Acc: 0.998046875; Test Accuracy: 0.974\n",
            "Epoch: 730; Loss: 0.0482149648378767; Train Acc: 0.994140625; Test Accuracy: 0.9741\n",
            "Epoch: 731; Loss: 0.046556742344242195; Train Acc: 0.998046875; Test Accuracy: 0.9738\n",
            "Epoch: 732; Loss: 0.08214811336426045; Train Acc: 0.9921875; Test Accuracy: 0.9738\n",
            "Epoch: 733; Loss: 0.05342138803348494; Train Acc: 0.994140625; Test Accuracy: 0.9749\n",
            "Epoch: 734; Loss: 0.033443627823959265; Train Acc: 0.994140625; Test Accuracy: 0.9757\n",
            "Epoch: 735; Loss: 0.0588135556869743; Train Acc: 0.998046875; Test Accuracy: 0.9742\n",
            "Epoch: 736; Loss: 0.059342523322390574; Train Acc: 0.9921875; Test Accuracy: 0.9736\n",
            "Epoch: 737; Loss: 0.03588589852969026; Train Acc: 1.0; Test Accuracy: 0.9743\n",
            "Epoch: 738; Loss: 0.05205611444950127; Train Acc: 0.99609375; Test Accuracy: 0.9715\n",
            "Epoch: 739; Loss: 0.060795968673767416; Train Acc: 0.994140625; Test Accuracy: 0.9749\n",
            "Epoch: 740; Loss: 0.05579687023188046; Train Acc: 0.994140625; Test Accuracy: 0.9734\n",
            "Epoch: 741; Loss: 0.051553401622036775; Train Acc: 0.99609375; Test Accuracy: 0.9737\n",
            "Epoch: 742; Loss: 0.08353039565299471; Train Acc: 0.990234375; Test Accuracy: 0.9696\n",
            "Epoch: 743; Loss: 0.04700303413258042; Train Acc: 1.0; Test Accuracy: 0.9735\n",
            "Epoch: 744; Loss: 0.08336505477788997; Train Acc: 0.98828125; Test Accuracy: 0.973\n",
            "Epoch: 745; Loss: 0.03537991118498796; Train Acc: 0.998046875; Test Accuracy: 0.9747\n",
            "Epoch: 746; Loss: 0.031612122914937396; Train Acc: 1.0; Test Accuracy: 0.9747\n",
            "Epoch: 747; Loss: 0.04893729259389531; Train Acc: 0.998046875; Test Accuracy: 0.9727\n",
            "Epoch: 748; Loss: 0.033929961626741; Train Acc: 0.998046875; Test Accuracy: 0.9726\n",
            "Epoch: 749; Loss: 0.051013343778323464; Train Acc: 0.99609375; Test Accuracy: 0.9761\n",
            "Epoch: 750; Loss: 0.06573721085291667; Train Acc: 0.994140625; Test Accuracy: 0.9746\n",
            "Epoch: 751; Loss: 0.058584948875845845; Train Acc: 0.994140625; Test Accuracy: 0.974\n",
            "Epoch: 752; Loss: 0.038989582798850425; Train Acc: 0.998046875; Test Accuracy: 0.9725\n",
            "Epoch: 753; Loss: 0.03436035293490214; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 754; Loss: 0.043908990438092696; Train Acc: 0.998046875; Test Accuracy: 0.9753\n",
            "Epoch: 755; Loss: 0.04414166672686824; Train Acc: 0.994140625; Test Accuracy: 0.9756\n",
            "Epoch: 756; Loss: 0.03549486820669417; Train Acc: 0.998046875; Test Accuracy: 0.9735\n",
            "Epoch: 757; Loss: 0.07818263504516884; Train Acc: 0.986328125; Test Accuracy: 0.9753\n",
            "Epoch: 758; Loss: 0.06105819681774623; Train Acc: 0.99609375; Test Accuracy: 0.9741\n",
            "Epoch: 759; Loss: 0.048217776676431456; Train Acc: 0.998046875; Test Accuracy: 0.9746\n",
            "Epoch: 760; Loss: 0.027084904479839925; Train Acc: 1.0; Test Accuracy: 0.9757\n",
            "Epoch: 761; Loss: 0.051171667471296055; Train Acc: 0.998046875; Test Accuracy: 0.9738\n",
            "Epoch: 762; Loss: 0.03028153900461225; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 763; Loss: 0.05671791325698077; Train Acc: 0.9921875; Test Accuracy: 0.9744\n",
            "Epoch: 764; Loss: 0.05784307772639566; Train Acc: 0.990234375; Test Accuracy: 0.9762\n",
            "Epoch: 765; Loss: 0.04290537354019466; Train Acc: 0.998046875; Test Accuracy: 0.9762\n",
            "Epoch: 766; Loss: 0.03863830826509093; Train Acc: 0.998046875; Test Accuracy: 0.9758\n",
            "Epoch: 767; Loss: 0.03430608022784821; Train Acc: 0.99609375; Test Accuracy: 0.976\n",
            "Epoch: 768; Loss: 0.0457012467107836; Train Acc: 0.99609375; Test Accuracy: 0.9756\n",
            "Epoch: 769; Loss: 0.0715681509590682; Train Acc: 0.9921875; Test Accuracy: 0.9737\n",
            "Epoch: 770; Loss: 0.033752967511790355; Train Acc: 0.99609375; Test Accuracy: 0.9763\n",
            "Epoch: 771; Loss: 0.03410161901959196; Train Acc: 0.998046875; Test Accuracy: 0.9755\n",
            "Epoch: 772; Loss: 0.041270938997457665; Train Acc: 0.998046875; Test Accuracy: 0.9765\n",
            "Epoch: 773; Loss: 0.036078197980837376; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 774; Loss: 0.04122476766120547; Train Acc: 0.998046875; Test Accuracy: 0.9764\n",
            "Epoch: 775; Loss: 0.042833697312269256; Train Acc: 0.9921875; Test Accuracy: 0.9713\n",
            "Epoch: 776; Loss: 0.051810489307650434; Train Acc: 0.9921875; Test Accuracy: 0.9741\n",
            "Epoch: 777; Loss: 0.05030957610981099; Train Acc: 0.994140625; Test Accuracy: 0.9754\n",
            "Epoch: 778; Loss: 0.0367761386797536; Train Acc: 1.0; Test Accuracy: 0.9759\n",
            "Epoch: 779; Loss: 0.03464809303542559; Train Acc: 0.99609375; Test Accuracy: 0.9746\n",
            "Epoch: 780; Loss: 0.05766810793608304; Train Acc: 0.990234375; Test Accuracy: 0.971\n",
            "Epoch: 781; Loss: 0.058161389536832704; Train Acc: 0.994140625; Test Accuracy: 0.976\n",
            "Epoch: 782; Loss: 0.04348184229614155; Train Acc: 0.99609375; Test Accuracy: 0.9768\n",
            "Epoch: 783; Loss: 0.033341806380504324; Train Acc: 1.0; Test Accuracy: 0.9749\n",
            "Epoch: 784; Loss: 0.043684757696970396; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 785; Loss: 0.053375090673138316; Train Acc: 0.9921875; Test Accuracy: 0.9738\n",
            "Epoch: 786; Loss: 0.04413179566095412; Train Acc: 0.99609375; Test Accuracy: 0.975\n",
            "Epoch: 787; Loss: 0.05438161111660397; Train Acc: 0.994140625; Test Accuracy: 0.9761\n",
            "Epoch: 788; Loss: 0.036101188442542814; Train Acc: 0.998046875; Test Accuracy: 0.9741\n",
            "Epoch: 789; Loss: 0.05339782686176585; Train Acc: 0.994140625; Test Accuracy: 0.9754\n",
            "Epoch: 790; Loss: 0.037888612805620094; Train Acc: 0.998046875; Test Accuracy: 0.9713\n",
            "Epoch: 791; Loss: 0.07927697311717674; Train Acc: 0.986328125; Test Accuracy: 0.9691\n",
            "Epoch: 792; Loss: 0.06628901858182495; Train Acc: 0.9921875; Test Accuracy: 0.9761\n",
            "Epoch: 793; Loss: 0.037580130125167824; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 794; Loss: 0.041449674887855156; Train Acc: 0.998046875; Test Accuracy: 0.9753\n",
            "Epoch: 795; Loss: 0.026994308252046118; Train Acc: 1.0; Test Accuracy: 0.9749\n",
            "Epoch: 796; Loss: 0.037830905873919685; Train Acc: 0.99609375; Test Accuracy: 0.9758\n",
            "Epoch: 797; Loss: 0.059641563350437965; Train Acc: 0.99609375; Test Accuracy: 0.9738\n",
            "Epoch: 798; Loss: 0.07084906541617497; Train Acc: 0.9921875; Test Accuracy: 0.9747\n",
            "Epoch: 799; Loss: 0.0717235170515195; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 800; Loss: 0.03459900987424573; Train Acc: 0.998046875; Test Accuracy: 0.9749\n",
            "Epoch: 801; Loss: 0.037579256113730294; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 802; Loss: 0.07562940140018362; Train Acc: 0.9921875; Test Accuracy: 0.9737\n",
            "Epoch: 803; Loss: 0.05784519498787554; Train Acc: 0.994140625; Test Accuracy: 0.9747\n",
            "Epoch: 804; Loss: 0.0624705298286958; Train Acc: 0.990234375; Test Accuracy: 0.974\n",
            "Epoch: 805; Loss: 0.02626279040516878; Train Acc: 1.0; Test Accuracy: 0.9748\n",
            "Epoch: 806; Loss: 0.06136052143467136; Train Acc: 0.9921875; Test Accuracy: 0.973\n",
            "Epoch: 807; Loss: 0.05550982059206956; Train Acc: 0.994140625; Test Accuracy: 0.9761\n",
            "Epoch: 808; Loss: 0.0456801171949542; Train Acc: 0.998046875; Test Accuracy: 0.9748\n",
            "Epoch: 809; Loss: 0.041654091672411304; Train Acc: 1.0; Test Accuracy: 0.9756\n",
            "Epoch: 810; Loss: 0.04534401397476839; Train Acc: 0.99609375; Test Accuracy: 0.9759\n",
            "Epoch: 811; Loss: 0.04238747438554394; Train Acc: 0.998046875; Test Accuracy: 0.9764\n",
            "Epoch: 812; Loss: 0.06285733336344154; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 813; Loss: 0.06051556918206232; Train Acc: 0.990234375; Test Accuracy: 0.9754\n",
            "Epoch: 814; Loss: 0.054636616063818935; Train Acc: 0.99609375; Test Accuracy: 0.9744\n",
            "Epoch: 815; Loss: 0.03641658095468303; Train Acc: 0.998046875; Test Accuracy: 0.9749\n",
            "Epoch: 816; Loss: 0.0515563833312339; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 817; Loss: 0.041305247982530664; Train Acc: 0.994140625; Test Accuracy: 0.9736\n",
            "Epoch: 818; Loss: 0.0795699807446995; Train Acc: 0.982421875; Test Accuracy: 0.9683\n",
            "Epoch: 819; Loss: 0.04000554130198366; Train Acc: 0.998046875; Test Accuracy: 0.9758\n",
            "Epoch: 820; Loss: 0.05401646494125193; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 821; Loss: 0.04583527399650607; Train Acc: 0.998046875; Test Accuracy: 0.9744\n",
            "Epoch: 822; Loss: 0.057706419073648776; Train Acc: 0.994140625; Test Accuracy: 0.9742\n",
            "Epoch: 823; Loss: 0.057587303432016235; Train Acc: 0.990234375; Test Accuracy: 0.9742\n",
            "Epoch: 824; Loss: 0.03493653595260412; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 825; Loss: 0.05890980593080114; Train Acc: 0.99609375; Test Accuracy: 0.9719\n",
            "Epoch: 826; Loss: 0.048692698789230035; Train Acc: 0.994140625; Test Accuracy: 0.9746\n",
            "Epoch: 827; Loss: 0.03754132904478011; Train Acc: 0.998046875; Test Accuracy: 0.9745\n",
            "Epoch: 828; Loss: 0.0406819825793173; Train Acc: 0.998046875; Test Accuracy: 0.9762\n",
            "Epoch: 829; Loss: 0.02589550467107883; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 830; Loss: 0.03804014041623625; Train Acc: 0.99609375; Test Accuracy: 0.9756\n",
            "Epoch: 831; Loss: 0.02457140256360739; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 832; Loss: 0.04995465601788223; Train Acc: 0.9921875; Test Accuracy: 0.9751\n",
            "Epoch: 833; Loss: 0.04483509968579347; Train Acc: 0.994140625; Test Accuracy: 0.9754\n",
            "Epoch: 834; Loss: 0.07712851079608549; Train Acc: 0.9921875; Test Accuracy: 0.9731\n",
            "Epoch: 835; Loss: 0.04476268786950531; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 836; Loss: 0.037148275655513784; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 837; Loss: 0.034656968565785665; Train Acc: 0.99609375; Test Accuracy: 0.9767\n",
            "Epoch: 838; Loss: 0.03415636156999101; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 839; Loss: 0.04149545721847146; Train Acc: 0.994140625; Test Accuracy: 0.9768\n",
            "Epoch: 840; Loss: 0.03641309867012313; Train Acc: 0.99609375; Test Accuracy: 0.9754\n",
            "Epoch: 841; Loss: 0.058809543182475235; Train Acc: 0.994140625; Test Accuracy: 0.9692\n",
            "Epoch: 842; Loss: 0.04779640795670914; Train Acc: 0.99609375; Test Accuracy: 0.9755\n",
            "Epoch: 843; Loss: 0.03674786655915144; Train Acc: 1.0; Test Accuracy: 0.9751\n",
            "Epoch: 844; Loss: 0.07325672422300925; Train Acc: 0.98828125; Test Accuracy: 0.9742\n",
            "Epoch: 845; Loss: 0.050127644259729355; Train Acc: 0.994140625; Test Accuracy: 0.975\n",
            "Epoch: 846; Loss: 0.042476081174906505; Train Acc: 0.998046875; Test Accuracy: 0.9744\n",
            "Epoch: 847; Loss: 0.039810098221579326; Train Acc: 0.998046875; Test Accuracy: 0.9734\n",
            "Epoch: 848; Loss: 0.04544000192619963; Train Acc: 0.994140625; Test Accuracy: 0.9752\n",
            "Epoch: 849; Loss: 0.04684366661264547; Train Acc: 0.99609375; Test Accuracy: 0.9741\n",
            "Epoch: 850; Loss: 0.042297871036156016; Train Acc: 0.998046875; Test Accuracy: 0.9751\n",
            "Epoch: 851; Loss: 0.09149964315176624; Train Acc: 0.978515625; Test Accuracy: 0.9702\n",
            "Epoch: 852; Loss: 0.11062026411789858; Train Acc: 0.98046875; Test Accuracy: 0.9708\n",
            "Epoch: 853; Loss: 0.04334004747724254; Train Acc: 0.99609375; Test Accuracy: 0.975\n",
            "Epoch: 854; Loss: 0.0513124592149062; Train Acc: 0.994140625; Test Accuracy: 0.9745\n",
            "Epoch: 855; Loss: 0.06527380231871095; Train Acc: 0.990234375; Test Accuracy: 0.975\n",
            "Epoch: 856; Loss: 0.029938321044210958; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 857; Loss: 0.046729463412950284; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 858; Loss: 0.04377162328629054; Train Acc: 0.99609375; Test Accuracy: 0.9772\n",
            "Epoch: 859; Loss: 0.03238710946477347; Train Acc: 0.998046875; Test Accuracy: 0.9748\n",
            "Epoch: 860; Loss: 0.04345215028303476; Train Acc: 0.994140625; Test Accuracy: 0.9764\n",
            "Epoch: 861; Loss: 0.045969321822131454; Train Acc: 0.9921875; Test Accuracy: 0.9757\n",
            "Epoch: 862; Loss: 0.04917440001412288; Train Acc: 0.994140625; Test Accuracy: 0.9724\n",
            "Epoch: 863; Loss: 0.03234533640766921; Train Acc: 1.0; Test Accuracy: 0.9751\n",
            "Epoch: 864; Loss: 0.021346054741223797; Train Acc: 1.0; Test Accuracy: 0.975\n",
            "Epoch: 865; Loss: 0.04177479512071103; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 866; Loss: 0.029707890201577703; Train Acc: 1.0; Test Accuracy: 0.9761\n",
            "Epoch: 867; Loss: 0.07404839461719881; Train Acc: 0.9921875; Test Accuracy: 0.9745\n",
            "Epoch: 868; Loss: 0.033065267937831155; Train Acc: 1.0; Test Accuracy: 0.9747\n",
            "Epoch: 869; Loss: 0.06539533428758529; Train Acc: 0.9921875; Test Accuracy: 0.9754\n",
            "Epoch: 870; Loss: 0.05902131291366688; Train Acc: 0.9921875; Test Accuracy: 0.9676\n",
            "Epoch: 871; Loss: 0.057579808506753614; Train Acc: 0.986328125; Test Accuracy: 0.9678\n",
            "Epoch: 872; Loss: 0.06329280349799714; Train Acc: 0.994140625; Test Accuracy: 0.9745\n",
            "Epoch: 873; Loss: 0.04691207496869088; Train Acc: 0.9921875; Test Accuracy: 0.9728\n",
            "Epoch: 874; Loss: 0.04172817192319731; Train Acc: 0.99609375; Test Accuracy: 0.9744\n",
            "Epoch: 875; Loss: 0.042938459926665305; Train Acc: 0.998046875; Test Accuracy: 0.9751\n",
            "Epoch: 876; Loss: 0.034313122488243614; Train Acc: 0.99609375; Test Accuracy: 0.9741\n",
            "Epoch: 877; Loss: 0.052486193533412474; Train Acc: 0.994140625; Test Accuracy: 0.9764\n",
            "Epoch: 878; Loss: 0.06186019790342667; Train Acc: 0.994140625; Test Accuracy: 0.9756\n",
            "Epoch: 879; Loss: 0.055186730854464607; Train Acc: 0.994140625; Test Accuracy: 0.9751\n",
            "Epoch: 880; Loss: 0.04203465218446357; Train Acc: 1.0; Test Accuracy: 0.9739\n",
            "Epoch: 881; Loss: 0.0531461673704575; Train Acc: 0.994140625; Test Accuracy: 0.9716\n",
            "Epoch: 882; Loss: 0.051296627032659636; Train Acc: 0.994140625; Test Accuracy: 0.9754\n",
            "Epoch: 883; Loss: 0.05425130525363121; Train Acc: 0.99609375; Test Accuracy: 0.9748\n",
            "Epoch: 884; Loss: 0.04098649694736063; Train Acc: 0.99609375; Test Accuracy: 0.9766\n",
            "Epoch: 885; Loss: 0.05440921688134827; Train Acc: 0.99609375; Test Accuracy: 0.9749\n",
            "Epoch: 886; Loss: 0.030347988886860675; Train Acc: 1.0; Test Accuracy: 0.9744\n",
            "Epoch: 887; Loss: 0.033015955305146824; Train Acc: 0.998046875; Test Accuracy: 0.9748\n",
            "Epoch: 888; Loss: 0.06747174674482755; Train Acc: 0.994140625; Test Accuracy: 0.973\n",
            "Epoch: 889; Loss: 0.05945321479686626; Train Acc: 0.994140625; Test Accuracy: 0.9761\n",
            "Epoch: 890; Loss: 0.047292693252227624; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 891; Loss: 0.042493496572704945; Train Acc: 0.998046875; Test Accuracy: 0.9748\n",
            "Epoch: 892; Loss: 0.04012449002068649; Train Acc: 0.998046875; Test Accuracy: 0.9749\n",
            "Epoch: 893; Loss: 0.06077308294478928; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 894; Loss: 0.0536919458772433; Train Acc: 0.99609375; Test Accuracy: 0.9751\n",
            "Epoch: 895; Loss: 0.036347415448557276; Train Acc: 0.99609375; Test Accuracy: 0.9744\n",
            "Epoch: 896; Loss: 0.05422384588561403; Train Acc: 0.9921875; Test Accuracy: 0.975\n",
            "Epoch: 897; Loss: 0.03635868592402958; Train Acc: 0.998046875; Test Accuracy: 0.9754\n",
            "Epoch: 898; Loss: 0.021986194974116996; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 899; Loss: 0.039099666217556076; Train Acc: 0.99609375; Test Accuracy: 0.9773\n",
            "Epoch: 900; Loss: 0.031844820956875235; Train Acc: 1.0; Test Accuracy: 0.9759\n",
            "Epoch: 901; Loss: 0.06836040054053616; Train Acc: 0.994140625; Test Accuracy: 0.9749\n",
            "Epoch: 902; Loss: 0.058421081510977926; Train Acc: 0.994140625; Test Accuracy: 0.9747\n",
            "Epoch: 903; Loss: 0.04830485855955685; Train Acc: 0.99609375; Test Accuracy: 0.9757\n",
            "Epoch: 904; Loss: 0.04445787752461736; Train Acc: 0.994140625; Test Accuracy: 0.9769\n",
            "Epoch: 905; Loss: 0.059996535200974616; Train Acc: 0.9921875; Test Accuracy: 0.9752\n",
            "Epoch: 906; Loss: 0.03345354074221143; Train Acc: 1.0; Test Accuracy: 0.9731\n",
            "Epoch: 907; Loss: 0.04925377834632615; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 908; Loss: 0.04902795517814321; Train Acc: 1.0; Test Accuracy: 0.9738\n",
            "Epoch: 909; Loss: 0.05445912599166999; Train Acc: 0.994140625; Test Accuracy: 0.9745\n",
            "Epoch: 910; Loss: 0.036405950735082716; Train Acc: 1.0; Test Accuracy: 0.9753\n",
            "Epoch: 911; Loss: 0.06731496270191738; Train Acc: 0.9921875; Test Accuracy: 0.9754\n",
            "Epoch: 912; Loss: 0.050386492792855266; Train Acc: 0.99609375; Test Accuracy: 0.9763\n",
            "Epoch: 913; Loss: 0.04412261501033113; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 914; Loss: 0.046787741882242044; Train Acc: 0.990234375; Test Accuracy: 0.9745\n",
            "Epoch: 915; Loss: 0.0161509424233629; Train Acc: 1.0; Test Accuracy: 0.975\n",
            "Epoch: 916; Loss: 0.04982135171010091; Train Acc: 0.99609375; Test Accuracy: 0.9733\n",
            "Epoch: 917; Loss: 0.05336768086020736; Train Acc: 0.99609375; Test Accuracy: 0.9771\n",
            "Epoch: 918; Loss: 0.03953009602692588; Train Acc: 0.99609375; Test Accuracy: 0.9772\n",
            "Epoch: 919; Loss: 0.03850337487342401; Train Acc: 1.0; Test Accuracy: 0.9764\n",
            "Epoch: 920; Loss: 0.03125915609603401; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 921; Loss: 0.05371616972292563; Train Acc: 0.9921875; Test Accuracy: 0.9773\n",
            "Epoch: 922; Loss: 0.029320039690630205; Train Acc: 0.99609375; Test Accuracy: 0.9783\n",
            "Epoch: 923; Loss: 0.023503187867384916; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 924; Loss: 0.04402508339514754; Train Acc: 0.998046875; Test Accuracy: 0.9761\n",
            "Epoch: 925; Loss: 0.07610935591577328; Train Acc: 0.9921875; Test Accuracy: 0.9766\n",
            "Epoch: 926; Loss: 0.036582118623365495; Train Acc: 0.998046875; Test Accuracy: 0.9757\n",
            "Epoch: 927; Loss: 0.03499612975905224; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 928; Loss: 0.03967762582607941; Train Acc: 1.0; Test Accuracy: 0.975\n",
            "Epoch: 929; Loss: 0.03628273800582836; Train Acc: 0.99609375; Test Accuracy: 0.977\n",
            "Epoch: 930; Loss: 0.024546073061353514; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 931; Loss: 0.04248476152854096; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 932; Loss: 0.049687190029301866; Train Acc: 0.99609375; Test Accuracy: 0.9778\n",
            "Epoch: 933; Loss: 0.032465049919494726; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 934; Loss: 0.04578964229005884; Train Acc: 0.994140625; Test Accuracy: 0.9762\n",
            "Epoch: 935; Loss: 0.05111844207856597; Train Acc: 0.994140625; Test Accuracy: 0.9763\n",
            "Epoch: 936; Loss: 0.029182128065356125; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 937; Loss: 0.023855006888077138; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 938; Loss: 0.02271242679566627; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 939; Loss: 0.030881113733092252; Train Acc: 1.0; Test Accuracy: 0.9758\n",
            "Epoch: 940; Loss: 0.03812425117925461; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 941; Loss: 0.03932760784639891; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 942; Loss: 0.04044206093847171; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 943; Loss: 0.01940374725251486; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 944; Loss: 0.025697628643865128; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 945; Loss: 0.02370607908315429; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 946; Loss: 0.04154999082792628; Train Acc: 0.9921875; Test Accuracy: 0.9771\n",
            "Epoch: 947; Loss: 0.03607546079976312; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 948; Loss: 0.03459759266967303; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 949; Loss: 0.030072039874056013; Train Acc: 1.0; Test Accuracy: 0.976\n",
            "Epoch: 950; Loss: 0.04367177499439935; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 951; Loss: 0.04184378356545306; Train Acc: 0.998046875; Test Accuracy: 0.9755\n",
            "Epoch: 952; Loss: 0.05603279903135422; Train Acc: 0.990234375; Test Accuracy: 0.9765\n",
            "Epoch: 953; Loss: 0.0425411468154248; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 954; Loss: 0.031201226174843832; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 955; Loss: 0.03927361720062914; Train Acc: 0.99609375; Test Accuracy: 0.9763\n",
            "Epoch: 956; Loss: 0.03588864721887261; Train Acc: 0.99609375; Test Accuracy: 0.976\n",
            "Epoch: 957; Loss: 0.06017207228598667; Train Acc: 0.9921875; Test Accuracy: 0.9765\n",
            "Epoch: 958; Loss: 0.057518496104665646; Train Acc: 0.990234375; Test Accuracy: 0.9748\n",
            "Epoch: 959; Loss: 0.026944326842820894; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 960; Loss: 0.0435662429605472; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 961; Loss: 0.02725069459110607; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 962; Loss: 0.04214991811192228; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 963; Loss: 0.051113531687441684; Train Acc: 0.994140625; Test Accuracy: 0.9759\n",
            "Epoch: 964; Loss: 0.04249599776707409; Train Acc: 0.9921875; Test Accuracy: 0.9759\n",
            "Epoch: 965; Loss: 0.040868013810873315; Train Acc: 0.99609375; Test Accuracy: 0.9759\n",
            "Epoch: 966; Loss: 0.07432057045107854; Train Acc: 0.986328125; Test Accuracy: 0.9734\n",
            "Epoch: 967; Loss: 0.03608273457227571; Train Acc: 1.0; Test Accuracy: 0.9758\n",
            "Epoch: 968; Loss: 0.040372900407137216; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 969; Loss: 0.02818174170932551; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 970; Loss: 0.02389945175967712; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 971; Loss: 0.030630087284575815; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 972; Loss: 0.055649605132362154; Train Acc: 0.994140625; Test Accuracy: 0.9751\n",
            "Epoch: 973; Loss: 0.05058768091262601; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 974; Loss: 0.05307890656092762; Train Acc: 0.99609375; Test Accuracy: 0.9756\n",
            "Epoch: 975; Loss: 0.04117271429906198; Train Acc: 1.0; Test Accuracy: 0.9734\n",
            "Epoch: 976; Loss: 0.023459878915484112; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 977; Loss: 0.05077097452690743; Train Acc: 0.994140625; Test Accuracy: 0.9754\n",
            "Epoch: 978; Loss: 0.06594282321633652; Train Acc: 0.9921875; Test Accuracy: 0.9746\n",
            "Epoch: 979; Loss: 0.02590960922946343; Train Acc: 1.0; Test Accuracy: 0.9758\n",
            "Epoch: 980; Loss: 0.022780369933815135; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 981; Loss: 0.028505085708915127; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 982; Loss: 0.03994599553187513; Train Acc: 0.994140625; Test Accuracy: 0.9779\n",
            "Epoch: 983; Loss: 0.03048388154010685; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 984; Loss: 0.022286429498911894; Train Acc: 0.998046875; Test Accuracy: 0.9775\n",
            "Epoch: 985; Loss: 0.037081608416959205; Train Acc: 0.99609375; Test Accuracy: 0.9773\n",
            "Epoch: 986; Loss: 0.02364028698641406; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 987; Loss: 0.030350608930912123; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 988; Loss: 0.023668142397024257; Train Acc: 1.0; Test Accuracy: 0.9761\n",
            "Epoch: 989; Loss: 0.05709200623209763; Train Acc: 0.9921875; Test Accuracy: 0.9742\n",
            "Epoch: 990; Loss: 0.03951498045434735; Train Acc: 0.998046875; Test Accuracy: 0.9762\n",
            "Epoch: 991; Loss: 0.04341422881392568; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 992; Loss: 0.022030954328780283; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 993; Loss: 0.04609697245968411; Train Acc: 0.9921875; Test Accuracy: 0.9757\n",
            "Epoch: 994; Loss: 0.03223386822659026; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 995; Loss: 0.03363326273456569; Train Acc: 0.998046875; Test Accuracy: 0.9743\n",
            "Epoch: 996; Loss: 0.02913923973928744; Train Acc: 1.0; Test Accuracy: 0.9757\n",
            "Epoch: 997; Loss: 0.038334033121472294; Train Acc: 0.990234375; Test Accuracy: 0.9708\n",
            "Epoch: 998; Loss: 0.07653205005022368; Train Acc: 0.9921875; Test Accuracy: 0.9749\n",
            "Epoch: 999; Loss: 0.05225621464833852; Train Acc: 0.994140625; Test Accuracy: 0.9748\n",
            "Epoch: 1000; Loss: 0.03888993803278096; Train Acc: 0.994140625; Test Accuracy: 0.9752\n",
            "Epoch: 1001; Loss: 0.039472884692215875; Train Acc: 0.994140625; Test Accuracy: 0.9747\n",
            "Epoch: 1002; Loss: 0.043108852803034746; Train Acc: 0.99609375; Test Accuracy: 0.9743\n",
            "Epoch: 1003; Loss: 0.04198377325626375; Train Acc: 0.998046875; Test Accuracy: 0.9765\n",
            "Epoch: 1004; Loss: 0.025752134027512474; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1005; Loss: 0.025912962004591047; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1006; Loss: 0.04313627177242929; Train Acc: 1.0; Test Accuracy: 0.976\n",
            "Epoch: 1007; Loss: 0.058110098996280855; Train Acc: 0.99609375; Test Accuracy: 0.9761\n",
            "Epoch: 1008; Loss: 0.04278964277183102; Train Acc: 0.99609375; Test Accuracy: 0.9763\n",
            "Epoch: 1009; Loss: 0.04353535630420799; Train Acc: 0.994140625; Test Accuracy: 0.9773\n",
            "Epoch: 1010; Loss: 0.021800701021810427; Train Acc: 1.0; Test Accuracy: 0.9762\n",
            "Epoch: 1011; Loss: 0.04432707562746589; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1012; Loss: 0.08331961913751842; Train Acc: 0.98828125; Test Accuracy: 0.9768\n",
            "Epoch: 1013; Loss: 0.0353821182678182; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 1014; Loss: 0.03156481838141511; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 1015; Loss: 0.034292227088655675; Train Acc: 0.998046875; Test Accuracy: 0.9753\n",
            "Epoch: 1016; Loss: 0.05800821599330236; Train Acc: 0.9921875; Test Accuracy: 0.9764\n",
            "Epoch: 1017; Loss: 0.045480765396635606; Train Acc: 0.99609375; Test Accuracy: 0.9746\n",
            "Epoch: 1018; Loss: 0.039122493511177914; Train Acc: 0.998046875; Test Accuracy: 0.9762\n",
            "Epoch: 1019; Loss: 0.06026975611697978; Train Acc: 0.99609375; Test Accuracy: 0.9724\n",
            "Epoch: 1020; Loss: 0.06311643724290972; Train Acc: 0.98046875; Test Accuracy: 0.9556\n",
            "Epoch: 1021; Loss: 0.09348870334564682; Train Acc: 0.98828125; Test Accuracy: 0.9663\n",
            "Epoch: 1022; Loss: 0.0562690350236443; Train Acc: 0.99609375; Test Accuracy: 0.9707\n",
            "Epoch: 1023; Loss: 0.0274030559911613; Train Acc: 0.99609375; Test Accuracy: 0.973\n",
            "Epoch: 1024; Loss: 0.039198482515681414; Train Acc: 0.998046875; Test Accuracy: 0.9761\n",
            "Epoch: 1025; Loss: 0.05519867521323204; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1026; Loss: 0.024123789107951887; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 1027; Loss: 0.024546539966082255; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1028; Loss: 0.031226322346636297; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 1029; Loss: 0.03481289211874111; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1030; Loss: 0.03417406368116949; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 1031; Loss: 0.055459076250525784; Train Acc: 0.99609375; Test Accuracy: 0.977\n",
            "Epoch: 1032; Loss: 0.03075974156682683; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1033; Loss: 0.021044232766596734; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1034; Loss: 0.02228753594443299; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 1035; Loss: 0.030922021590241027; Train Acc: 0.998046875; Test Accuracy: 0.976\n",
            "Epoch: 1036; Loss: 0.024642953531777353; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1037; Loss: 0.04221371590497544; Train Acc: 1.0; Test Accuracy: 0.9744\n",
            "Epoch: 1038; Loss: 0.048463460294680544; Train Acc: 0.98828125; Test Accuracy: 0.9706\n",
            "Epoch: 1039; Loss: 0.042827524544354637; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 1040; Loss: 0.042735768666661324; Train Acc: 0.99609375; Test Accuracy: 0.9753\n",
            "Epoch: 1041; Loss: 0.033946710954547904; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1042; Loss: 0.023810158859385422; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1043; Loss: 0.039345705665923436; Train Acc: 1.0; Test Accuracy: 0.9747\n",
            "Epoch: 1044; Loss: 0.039102218523574636; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1045; Loss: 0.04126148821512636; Train Acc: 1.0; Test Accuracy: 0.9756\n",
            "Epoch: 1046; Loss: 0.04114211875008966; Train Acc: 0.99609375; Test Accuracy: 0.9778\n",
            "Epoch: 1047; Loss: 0.06251785135643394; Train Acc: 0.994140625; Test Accuracy: 0.9761\n",
            "Epoch: 1048; Loss: 0.026893484813738062; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1049; Loss: 0.054274561762747295; Train Acc: 0.982421875; Test Accuracy: 0.9689\n",
            "Epoch: 1050; Loss: 0.038754205389852615; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 1051; Loss: 0.02687400007174491; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1052; Loss: 0.0178782149634366; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1053; Loss: 0.02382026562351582; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1054; Loss: 0.03426737243112814; Train Acc: 0.998046875; Test Accuracy: 0.9768\n",
            "Epoch: 1055; Loss: 0.027910561446788898; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 1056; Loss: 0.027705393975797438; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 1057; Loss: 0.04778780735689976; Train Acc: 0.9921875; Test Accuracy: 0.9775\n",
            "Epoch: 1058; Loss: 0.03896507368556018; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1059; Loss: 0.03458974356072267; Train Acc: 0.99609375; Test Accuracy: 0.9762\n",
            "Epoch: 1060; Loss: 0.027171616357966225; Train Acc: 0.998046875; Test Accuracy: 0.9758\n",
            "Epoch: 1061; Loss: 0.04801018001184311; Train Acc: 0.994140625; Test Accuracy: 0.9763\n",
            "Epoch: 1062; Loss: 0.029934650698728638; Train Acc: 0.998046875; Test Accuracy: 0.9757\n",
            "Epoch: 1063; Loss: 0.052357601502688776; Train Acc: 0.998046875; Test Accuracy: 0.9728\n",
            "Epoch: 1064; Loss: 0.0338193870903856; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1065; Loss: 0.036318111528415575; Train Acc: 0.998046875; Test Accuracy: 0.9761\n",
            "Epoch: 1066; Loss: 0.03848618592768403; Train Acc: 0.99609375; Test Accuracy: 0.9767\n",
            "Epoch: 1067; Loss: 0.031009951644050578; Train Acc: 0.998046875; Test Accuracy: 0.9764\n",
            "Epoch: 1068; Loss: 0.025836842253835226; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1069; Loss: 0.03185710739565278; Train Acc: 0.99609375; Test Accuracy: 0.9762\n",
            "Epoch: 1070; Loss: 0.05091102598049613; Train Acc: 0.9921875; Test Accuracy: 0.9736\n",
            "Epoch: 1071; Loss: 0.06310241194573118; Train Acc: 0.986328125; Test Accuracy: 0.9729\n",
            "Epoch: 1072; Loss: 0.03387715803402218; Train Acc: 1.0; Test Accuracy: 0.9761\n",
            "Epoch: 1073; Loss: 0.039319873056010174; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1074; Loss: 0.028892958079588436; Train Acc: 1.0; Test Accuracy: 0.9756\n",
            "Epoch: 1075; Loss: 0.05430415679389946; Train Acc: 0.998046875; Test Accuracy: 0.9775\n",
            "Epoch: 1076; Loss: 0.03639238639850588; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 1077; Loss: 0.022559765510127824; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1078; Loss: 0.0415935515400309; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1079; Loss: 0.036636868890467474; Train Acc: 0.998046875; Test Accuracy: 0.9762\n",
            "Epoch: 1080; Loss: 0.03981545478404752; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1081; Loss: 0.026680104035557428; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1082; Loss: 0.033048383962210735; Train Acc: 0.994140625; Test Accuracy: 0.978\n",
            "Epoch: 1083; Loss: 0.029664845834156336; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 1084; Loss: 0.02602030286650363; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1085; Loss: 0.024694336079482432; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1086; Loss: 0.024272112683217523; Train Acc: 1.0; Test Accuracy: 0.9759\n",
            "Epoch: 1087; Loss: 0.03871375636184024; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 1088; Loss: 0.027571881275562485; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1089; Loss: 0.03941625343597087; Train Acc: 0.99609375; Test Accuracy: 0.9781\n",
            "Epoch: 1090; Loss: 0.03477062396717429; Train Acc: 0.99609375; Test Accuracy: 0.9774\n",
            "Epoch: 1091; Loss: 0.026684407381613505; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1092; Loss: 0.031322766832847675; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1093; Loss: 0.03454838917334539; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1094; Loss: 0.04649966107081331; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1095; Loss: 0.02917743362588872; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 1096; Loss: 0.030246384767074076; Train Acc: 0.99609375; Test Accuracy: 0.977\n",
            "Epoch: 1097; Loss: 0.028442460983679536; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1098; Loss: 0.022292575395732487; Train Acc: 1.0; Test Accuracy: 0.9765\n",
            "Epoch: 1099; Loss: 0.04347941170065506; Train Acc: 0.994140625; Test Accuracy: 0.9776\n",
            "Epoch: 1100; Loss: 0.032625721317094326; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1101; Loss: 0.04357944067391917; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1102; Loss: 0.01965754920161867; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 1103; Loss: 0.039490135517504375; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1104; Loss: 0.02430180601500084; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1105; Loss: 0.026721947233783842; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1106; Loss: 0.02553115873596971; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1107; Loss: 0.023640737349904406; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1108; Loss: 0.025612792264231955; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1109; Loss: 0.030036495642712754; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1110; Loss: 0.03428309248883532; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1111; Loss: 0.039655500388750896; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 1112; Loss: 0.02900621557702371; Train Acc: 0.99609375; Test Accuracy: 0.9754\n",
            "Epoch: 1113; Loss: 0.04020132928845586; Train Acc: 1.0; Test Accuracy: 0.9765\n",
            "Epoch: 1114; Loss: 0.04973002922076339; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 1115; Loss: 0.037112583107845126; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1116; Loss: 0.037336879920707895; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1117; Loss: 0.03307601484731013; Train Acc: 0.99609375; Test Accuracy: 0.9779\n",
            "Epoch: 1118; Loss: 0.028392216375379004; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 1119; Loss: 0.0302933137801953; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1120; Loss: 0.029010529040264556; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1121; Loss: 0.027387221461323146; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1122; Loss: 0.04179563754245152; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1123; Loss: 0.03390720315991408; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 1124; Loss: 0.03777422715832759; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1125; Loss: 0.03475396857154353; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1126; Loss: 0.024973912960019218; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1127; Loss: 0.035057597517717504; Train Acc: 0.99609375; Test Accuracy: 0.9766\n",
            "Epoch: 1128; Loss: 0.05555826445758436; Train Acc: 0.9921875; Test Accuracy: 0.975\n",
            "Epoch: 1129; Loss: 0.0353333884406329; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 1130; Loss: 0.030893691682051423; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1131; Loss: 0.03662631631867529; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1132; Loss: 0.0478182793759443; Train Acc: 0.99609375; Test Accuracy: 0.9773\n",
            "Epoch: 1133; Loss: 0.028780706674205448; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1134; Loss: 0.02579253031877867; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1135; Loss: 0.023606297950364834; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1136; Loss: 0.04302814518804509; Train Acc: 0.994140625; Test Accuracy: 0.9778\n",
            "Epoch: 1137; Loss: 0.023235340297617327; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1138; Loss: 0.032572472949410194; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1139; Loss: 0.03180778363202877; Train Acc: 0.99609375; Test Accuracy: 0.9785\n",
            "Epoch: 1140; Loss: 0.024313078917900157; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1141; Loss: 0.03431897004391751; Train Acc: 0.998046875; Test Accuracy: 0.9781\n",
            "Epoch: 1142; Loss: 0.030432538414005405; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1143; Loss: 0.03176740620393837; Train Acc: 0.998046875; Test Accuracy: 0.9775\n",
            "Epoch: 1144; Loss: 0.025888955140242368; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1145; Loss: 0.025728117547228126; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1146; Loss: 0.03206557982990505; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1147; Loss: 0.014089214060399799; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1148; Loss: 0.026607207715441325; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1149; Loss: 0.028258464937863027; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1150; Loss: 0.02852150005959401; Train Acc: 0.994140625; Test Accuracy: 0.9711\n",
            "Epoch: 1151; Loss: 0.04784342944285638; Train Acc: 0.9921875; Test Accuracy: 0.9757\n",
            "Epoch: 1152; Loss: 0.03090970114906434; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1153; Loss: 0.027875078368221604; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1154; Loss: 0.01999744422460282; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1155; Loss: 0.02312461689701211; Train Acc: 0.998046875; Test Accuracy: 0.9768\n",
            "Epoch: 1156; Loss: 0.011035947632863655; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1157; Loss: 0.02011489024754408; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 1158; Loss: 0.03290174859207894; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1159; Loss: 0.031389355598421834; Train Acc: 0.998046875; Test Accuracy: 0.9768\n",
            "Epoch: 1160; Loss: 0.0408191047597724; Train Acc: 0.99609375; Test Accuracy: 0.977\n",
            "Epoch: 1161; Loss: 0.036347838697294785; Train Acc: 0.99609375; Test Accuracy: 0.9771\n",
            "Epoch: 1162; Loss: 0.038810294278830884; Train Acc: 0.99609375; Test Accuracy: 0.976\n",
            "Epoch: 1163; Loss: 0.032207356040016975; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1164; Loss: 0.03170621162328063; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1165; Loss: 0.023198820008716047; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1166; Loss: 0.03588495712381398; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1167; Loss: 0.0347482162142177; Train Acc: 1.0; Test Accuracy: 0.9771\n",
            "Epoch: 1168; Loss: 0.023140452233350525; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 1169; Loss: 0.02326711276045883; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1170; Loss: 0.024700476596245254; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1171; Loss: 0.025575397861510196; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1172; Loss: 0.051655655271151416; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 1173; Loss: 0.04577068491522998; Train Acc: 0.99609375; Test Accuracy: 0.9779\n",
            "Epoch: 1174; Loss: 0.035209400973795574; Train Acc: 0.998046875; Test Accuracy: 0.9781\n",
            "Epoch: 1175; Loss: 0.03103575621357371; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 1176; Loss: 0.024587854296650002; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1177; Loss: 0.04416333781516495; Train Acc: 0.99609375; Test Accuracy: 0.9785\n",
            "Epoch: 1178; Loss: 0.021964036243227163; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1179; Loss: 0.03082564657016138; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1180; Loss: 0.040223048373186264; Train Acc: 0.99609375; Test Accuracy: 0.9784\n",
            "Epoch: 1181; Loss: 0.026192867286697286; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1182; Loss: 0.02699678530435334; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1183; Loss: 0.0287646384100729; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1184; Loss: 0.055443403637178326; Train Acc: 0.994140625; Test Accuracy: 0.9772\n",
            "Epoch: 1185; Loss: 0.023354003424984386; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 1186; Loss: 0.03759878177374839; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1187; Loss: 0.04254130286233009; Train Acc: 0.99609375; Test Accuracy: 0.9781\n",
            "Epoch: 1188; Loss: 0.024088224740601367; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1189; Loss: 0.03276823048777425; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1190; Loss: 0.03433878938980822; Train Acc: 0.994140625; Test Accuracy: 0.9771\n",
            "Epoch: 1191; Loss: 0.030706728570921576; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1192; Loss: 0.026720842156987692; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1193; Loss: 0.029023831046709626; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1194; Loss: 0.027826700010824372; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1195; Loss: 0.030849818495378836; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1196; Loss: 0.020393167325109623; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1197; Loss: 0.023934689154063816; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1198; Loss: 0.02989573909025741; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1199; Loss: 0.023623412551820832; Train Acc: 1.0; Test Accuracy: 0.9771\n",
            "Epoch: 1200; Loss: 0.022926560153736644; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1201; Loss: 0.04320382752656447; Train Acc: 0.99609375; Test Accuracy: 0.9765\n",
            "Epoch: 1202; Loss: 0.02274610597918729; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1203; Loss: 0.025135933093337323; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1204; Loss: 0.037754543838911345; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 1205; Loss: 0.028993000590938962; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1206; Loss: 0.03013112597726155; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1207; Loss: 0.021492730748875047; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1208; Loss: 0.0341567436231871; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1209; Loss: 0.04042403705193265; Train Acc: 0.99609375; Test Accuracy: 0.976\n",
            "Epoch: 1210; Loss: 0.02052916225791669; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1211; Loss: 0.03641396810129351; Train Acc: 0.99609375; Test Accuracy: 0.9775\n",
            "Epoch: 1212; Loss: 0.05564464895412531; Train Acc: 0.99609375; Test Accuracy: 0.9775\n",
            "Epoch: 1213; Loss: 0.04084773259658714; Train Acc: 0.99609375; Test Accuracy: 0.9779\n",
            "Epoch: 1214; Loss: 0.037942720694544116; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1215; Loss: 0.022923691523289078; Train Acc: 1.0; Test Accuracy: 0.976\n",
            "Epoch: 1216; Loss: 0.03916099402974893; Train Acc: 0.99609375; Test Accuracy: 0.9739\n",
            "Epoch: 1217; Loss: 0.0569087674705575; Train Acc: 0.994140625; Test Accuracy: 0.9768\n",
            "Epoch: 1218; Loss: 0.028344109433048645; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1219; Loss: 0.018247323510585296; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1220; Loss: 0.03982779020260562; Train Acc: 0.99609375; Test Accuracy: 0.9781\n",
            "Epoch: 1221; Loss: 0.021579614488185225; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1222; Loss: 0.022792249343762637; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1223; Loss: 0.023339923460460994; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1224; Loss: 0.04087204357209797; Train Acc: 0.994140625; Test Accuracy: 0.9779\n",
            "Epoch: 1225; Loss: 0.020784256953549813; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1226; Loss: 0.017580963205524717; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1227; Loss: 0.02900193005902759; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1228; Loss: 0.02598016524125774; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1229; Loss: 0.02469714119379918; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1230; Loss: 0.03665822727542891; Train Acc: 0.99609375; Test Accuracy: 0.9771\n",
            "Epoch: 1231; Loss: 0.03739003356182295; Train Acc: 0.998046875; Test Accuracy: 0.9757\n",
            "Epoch: 1232; Loss: 0.02640875133384328; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1233; Loss: 0.03408045214641681; Train Acc: 0.998046875; Test Accuracy: 0.9795\n",
            "Epoch: 1234; Loss: 0.02925627344567684; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1235; Loss: 0.03963371844612591; Train Acc: 0.99609375; Test Accuracy: 0.9775\n",
            "Epoch: 1236; Loss: 0.023014027211994945; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1237; Loss: 0.02031736109237218; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1238; Loss: 0.021878401555792258; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1239; Loss: 0.0418425837825491; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1240; Loss: 0.026221855780761427; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1241; Loss: 0.026450382488971674; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1242; Loss: 0.02119779052168369; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1243; Loss: 0.040705481162928776; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1244; Loss: 0.03480720889944958; Train Acc: 0.99609375; Test Accuracy: 0.9776\n",
            "Epoch: 1245; Loss: 0.02377662092289208; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 1246; Loss: 0.03166815761564101; Train Acc: 0.998046875; Test Accuracy: 0.9769\n",
            "Epoch: 1247; Loss: 0.019300267720365184; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1248; Loss: 0.014936533418195386; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1249; Loss: 0.025982604158406364; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1250; Loss: 0.019845351558789935; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1251; Loss: 0.01625515939530609; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1252; Loss: 0.020152504262521846; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1253; Loss: 0.04006749857902257; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1254; Loss: 0.02184983977958235; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1255; Loss: 0.03651994195141773; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1256; Loss: 0.03950905133386555; Train Acc: 0.994140625; Test Accuracy: 0.977\n",
            "Epoch: 1257; Loss: 0.05079210451305557; Train Acc: 0.99609375; Test Accuracy: 0.9781\n",
            "Epoch: 1258; Loss: 0.016230114031825504; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1259; Loss: 0.017304073644188116; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1260; Loss: 0.026686706436036416; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1261; Loss: 0.013202746149051023; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1262; Loss: 0.04923581157386715; Train Acc: 0.99609375; Test Accuracy: 0.9783\n",
            "Epoch: 1263; Loss: 0.016359180687305623; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1264; Loss: 0.020140320254032104; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1265; Loss: 0.025561091652897158; Train Acc: 0.998046875; Test Accuracy: 0.9782\n",
            "Epoch: 1266; Loss: 0.028861930562563264; Train Acc: 0.99609375; Test Accuracy: 0.9784\n",
            "Epoch: 1267; Loss: 0.017698660303862083; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1268; Loss: 0.03258973506047693; Train Acc: 1.0; Test Accuracy: 0.9763\n",
            "Epoch: 1269; Loss: 0.028752695801209538; Train Acc: 0.998046875; Test Accuracy: 0.9775\n",
            "Epoch: 1270; Loss: 0.022313512121645666; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1271; Loss: 0.02393052548538621; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1272; Loss: 0.03329931757742746; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1273; Loss: 0.03453709463173102; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1274; Loss: 0.04634781674901686; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1275; Loss: 0.018228342993408246; Train Acc: 1.0; Test Accuracy: 0.9762\n",
            "Epoch: 1276; Loss: 0.03488467663486468; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1277; Loss: 0.023198759413347946; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1278; Loss: 0.028731371972113284; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1279; Loss: 0.019169563977970233; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1280; Loss: 0.03180537826572701; Train Acc: 0.998046875; Test Accuracy: 0.975\n",
            "Epoch: 1281; Loss: 0.03093300630886134; Train Acc: 0.99609375; Test Accuracy: 0.9782\n",
            "Epoch: 1282; Loss: 0.04080031920974937; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1283; Loss: 0.02470970190276321; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1284; Loss: 0.02442072324073604; Train Acc: 1.0; Test Accuracy: 0.976\n",
            "Epoch: 1285; Loss: 0.018378619811717158; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1286; Loss: 0.01773423309860826; Train Acc: 1.0; Test Accuracy: 0.9757\n",
            "Epoch: 1287; Loss: 0.03757528874664276; Train Acc: 0.99609375; Test Accuracy: 0.9772\n",
            "Epoch: 1288; Loss: 0.025271715978888734; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1289; Loss: 0.022443839980307665; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1290; Loss: 0.04516665951037733; Train Acc: 0.994140625; Test Accuracy: 0.9781\n",
            "Epoch: 1291; Loss: 0.020828975575746273; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1292; Loss: 0.03278874633584723; Train Acc: 1.0; Test Accuracy: 0.9757\n",
            "Epoch: 1293; Loss: 0.04618684946870864; Train Acc: 0.99609375; Test Accuracy: 0.9775\n",
            "Epoch: 1294; Loss: 0.04948716764194408; Train Acc: 0.99609375; Test Accuracy: 0.9751\n",
            "Epoch: 1295; Loss: 0.02071998474936857; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1296; Loss: 0.03160921767420606; Train Acc: 0.998046875; Test Accuracy: 0.9757\n",
            "Epoch: 1297; Loss: 0.031328599308728694; Train Acc: 1.0; Test Accuracy: 0.975\n",
            "Epoch: 1298; Loss: 0.023212896038144727; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1299; Loss: 0.01549693180679999; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1300; Loss: 0.023334594779997817; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1301; Loss: 0.02136797878146133; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1302; Loss: 0.02816126522835998; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1303; Loss: 0.021534743351031144; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1304; Loss: 0.014730374832553239; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1305; Loss: 0.022588867106699207; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1306; Loss: 0.057769395457195546; Train Acc: 0.99609375; Test Accuracy: 0.9775\n",
            "Epoch: 1307; Loss: 0.029291511051969425; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1308; Loss: 0.03287247722578013; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1309; Loss: 0.029137658351553113; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1310; Loss: 0.021817113918373703; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1311; Loss: 0.028307556865463056; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1312; Loss: 0.012924659367489131; Train Acc: 1.0; Test Accuracy: 0.9764\n",
            "Epoch: 1313; Loss: 0.014281528924861765; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1314; Loss: 0.03568712288144057; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 1315; Loss: 0.023174294976713958; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1316; Loss: 0.0163365823275236; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1317; Loss: 0.019117044872515726; Train Acc: 1.0; Test Accuracy: 0.9765\n",
            "Epoch: 1318; Loss: 0.02485158874232083; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1319; Loss: 0.03191588560128307; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1320; Loss: 0.015183514445772452; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 1321; Loss: 0.026330063184803274; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1322; Loss: 0.048372683472423975; Train Acc: 0.99609375; Test Accuracy: 0.9758\n",
            "Epoch: 1323; Loss: 0.04955869452429443; Train Acc: 0.994140625; Test Accuracy: 0.9719\n",
            "Epoch: 1324; Loss: 0.04737768939788258; Train Acc: 0.99609375; Test Accuracy: 0.9776\n",
            "Epoch: 1325; Loss: 0.035666704986466846; Train Acc: 0.998046875; Test Accuracy: 0.9765\n",
            "Epoch: 1326; Loss: 0.01871042591021198; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1327; Loss: 0.0262537746755847; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1328; Loss: 0.017224798810001374; Train Acc: 1.0; Test Accuracy: 0.9768\n",
            "Epoch: 1329; Loss: 0.012936100592038192; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1330; Loss: 0.025332434523204297; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1331; Loss: 0.034522170386876716; Train Acc: 0.99609375; Test Accuracy: 0.9769\n",
            "Epoch: 1332; Loss: 0.027854332002508746; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1333; Loss: 0.019630364094308853; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1334; Loss: 0.02732825183334652; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1335; Loss: 0.024667111958298828; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1336; Loss: 0.03509493045540531; Train Acc: 1.0; Test Accuracy: 0.9755\n",
            "Epoch: 1337; Loss: 0.028206164467002505; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1338; Loss: 0.025216008774314923; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1339; Loss: 0.034511911888246; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1340; Loss: 0.023063947953784827; Train Acc: 0.998046875; Test Accuracy: 0.9782\n",
            "Epoch: 1341; Loss: 0.013474114381740794; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1342; Loss: 0.013039980465759028; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1343; Loss: 0.014892702110872746; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1344; Loss: 0.03515646300999159; Train Acc: 0.994140625; Test Accuracy: 0.9778\n",
            "Epoch: 1345; Loss: 0.031114286674214293; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1346; Loss: 0.023180092255486832; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1347; Loss: 0.011014352709714367; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1348; Loss: 0.02740129568678042; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1349; Loss: 0.01621567407164867; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1350; Loss: 0.015392341691717321; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1351; Loss: 0.019201731113585353; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1352; Loss: 0.015749320111332645; Train Acc: 1.0; Test Accuracy: 0.9769\n",
            "Epoch: 1353; Loss: 0.027914744790227534; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1354; Loss: 0.0159024667013525; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1355; Loss: 0.02338645302679588; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1356; Loss: 0.024314240443985155; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1357; Loss: 0.02428127617553049; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1358; Loss: 0.03538343061313021; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1359; Loss: 0.022776167273032593; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1360; Loss: 0.022700085830721292; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1361; Loss: 0.025237351013282804; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1362; Loss: 0.012369177820833386; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1363; Loss: 0.04243708265596836; Train Acc: 0.998046875; Test Accuracy: 0.976\n",
            "Epoch: 1364; Loss: 0.03200429281806258; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1365; Loss: 0.02500337057216539; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1366; Loss: 0.023466435491745697; Train Acc: 1.0; Test Accuracy: 0.9758\n",
            "Epoch: 1367; Loss: 0.026954025530587095; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1368; Loss: 0.01714264000596516; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1369; Loss: 0.022415038502797382; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1370; Loss: 0.02313682673866503; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1371; Loss: 0.020776709697876018; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1372; Loss: 0.01683570687473544; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1373; Loss: 0.030230001571943887; Train Acc: 0.998046875; Test Accuracy: 0.9781\n",
            "Epoch: 1374; Loss: 0.0129083499241892; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1375; Loss: 0.014881112893900091; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1376; Loss: 0.021863764238142853; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1377; Loss: 0.02429980582116815; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1378; Loss: 0.011333589177607944; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1379; Loss: 0.01609529136151605; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1380; Loss: 0.017659840320835576; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1381; Loss: 0.020359457082997235; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1382; Loss: 0.03095864633651061; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1383; Loss: 0.05923477380655825; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1384; Loss: 0.02267009366574583; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1385; Loss: 0.02703212131695257; Train Acc: 0.994140625; Test Accuracy: 0.978\n",
            "Epoch: 1386; Loss: 0.02976373330399173; Train Acc: 0.99609375; Test Accuracy: 0.9779\n",
            "Epoch: 1387; Loss: 0.03993452730409611; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1388; Loss: 0.024607338245631785; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1389; Loss: 0.021291361256605315; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1390; Loss: 0.020348182403258683; Train Acc: 1.0; Test Accuracy: 0.9771\n",
            "Epoch: 1391; Loss: 0.034454088337716494; Train Acc: 0.994140625; Test Accuracy: 0.976\n",
            "Epoch: 1392; Loss: 0.02199038630391054; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1393; Loss: 0.034623292494900246; Train Acc: 0.998046875; Test Accuracy: 0.9781\n",
            "Epoch: 1394; Loss: 0.018522490767650168; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1395; Loss: 0.0193151496562477; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1396; Loss: 0.027279272781523013; Train Acc: 0.998046875; Test Accuracy: 0.9767\n",
            "Epoch: 1397; Loss: 0.022545351138158594; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1398; Loss: 0.010794831715099156; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1399; Loss: 0.039791459987228314; Train Acc: 0.994140625; Test Accuracy: 0.9776\n",
            "Epoch: 1400; Loss: 0.02589086926967693; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1401; Loss: 0.011294599483301311; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1402; Loss: 0.028889808448715117; Train Acc: 0.99609375; Test Accuracy: 0.9786\n",
            "Epoch: 1403; Loss: 0.017506302943427275; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1404; Loss: 0.01841336123338841; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1405; Loss: 0.025645445621379874; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1406; Loss: 0.02604693899323459; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1407; Loss: 0.028415908488942924; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1408; Loss: 0.027433548222526152; Train Acc: 0.99609375; Test Accuracy: 0.9788\n",
            "Epoch: 1409; Loss: 0.02330170471999886; Train Acc: 1.0; Test Accuracy: 0.9771\n",
            "Epoch: 1410; Loss: 0.017456975819845903; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1411; Loss: 0.0313617637123567; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1412; Loss: 0.0433023928568869; Train Acc: 0.998046875; Test Accuracy: 0.9759\n",
            "Epoch: 1413; Loss: 0.026617239391158842; Train Acc: 0.998046875; Test Accuracy: 0.9755\n",
            "Epoch: 1414; Loss: 0.023505741682600053; Train Acc: 1.0; Test Accuracy: 0.977\n",
            "Epoch: 1415; Loss: 0.029029415375926265; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1416; Loss: 0.03829133512958753; Train Acc: 1.0; Test Accuracy: 0.9773\n",
            "Epoch: 1417; Loss: 0.01821099600136447; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1418; Loss: 0.04376877947058698; Train Acc: 0.994140625; Test Accuracy: 0.9786\n",
            "Epoch: 1419; Loss: 0.022560632117321996; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1420; Loss: 0.01854143020912436; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1421; Loss: 0.017479373669615928; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1422; Loss: 0.018533787402035713; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1423; Loss: 0.036901198795874585; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1424; Loss: 0.022024567143623053; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1425; Loss: 0.012784042677032618; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1426; Loss: 0.025041644932789667; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1427; Loss: 0.014763844180483321; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1428; Loss: 0.0221891326931103; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1429; Loss: 0.027061132399550433; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1430; Loss: 0.022080790704978873; Train Acc: 0.998046875; Test Accuracy: 0.9777\n",
            "Epoch: 1431; Loss: 0.012031278721388382; Train Acc: 1.0; Test Accuracy: 0.9762\n",
            "Epoch: 1432; Loss: 0.026444442155144698; Train Acc: 1.0; Test Accuracy: 0.9764\n",
            "Epoch: 1433; Loss: 0.049447625703930215; Train Acc: 0.99609375; Test Accuracy: 0.9761\n",
            "Epoch: 1434; Loss: 0.022160234876131238; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1435; Loss: 0.014186333116390842; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1436; Loss: 0.020676090467431914; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1437; Loss: 0.018601902286680204; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1438; Loss: 0.012392978297872604; Train Acc: 0.998046875; Test Accuracy: 0.9763\n",
            "Epoch: 1439; Loss: 0.028018429134789424; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1440; Loss: 0.03445363814958342; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1441; Loss: 0.01837832481196986; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1442; Loss: 0.016523504247310153; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1443; Loss: 0.017101130052541907; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1444; Loss: 0.024442063834455765; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1445; Loss: 0.024398591804929662; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1446; Loss: 0.014306529175493797; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1447; Loss: 0.017700071977512742; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1448; Loss: 0.022538965580252715; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1449; Loss: 0.011200272597967145; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1450; Loss: 0.020759126101469988; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1451; Loss: 0.009960505810540084; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1452; Loss: 0.016901308773928883; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1453; Loss: 0.025251544643960948; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1454; Loss: 0.021179945106786623; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1455; Loss: 0.01866387658367937; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1456; Loss: 0.0311381586742212; Train Acc: 0.99609375; Test Accuracy: 0.9787\n",
            "Epoch: 1457; Loss: 0.017022270251351796; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1458; Loss: 0.015880104945154096; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1459; Loss: 0.019440372614644125; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1460; Loss: 0.010726409734329059; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1461; Loss: 0.02355688748409126; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1462; Loss: 0.02433860000054245; Train Acc: 1.0; Test Accuracy: 0.9766\n",
            "Epoch: 1463; Loss: 0.022340257957469016; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1464; Loss: 0.026286579224403136; Train Acc: 0.99609375; Test Accuracy: 0.9782\n",
            "Epoch: 1465; Loss: 0.027036135591333187; Train Acc: 0.99609375; Test Accuracy: 0.9783\n",
            "Epoch: 1466; Loss: 0.028029128198896653; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1467; Loss: 0.01583966910832695; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1468; Loss: 0.011243878436701077; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1469; Loss: 0.03618252429350103; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1470; Loss: 0.017982392642750204; Train Acc: 0.998046875; Test Accuracy: 0.9782\n",
            "Epoch: 1471; Loss: 0.018885111242164312; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1472; Loss: 0.013242579793877267; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1473; Loss: 0.030152913379248768; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1474; Loss: 0.0498417338976399; Train Acc: 0.994140625; Test Accuracy: 0.9782\n",
            "Epoch: 1475; Loss: 0.021138307391466184; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1476; Loss: 0.017842575311175342; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1477; Loss: 0.03309619419797703; Train Acc: 0.998046875; Test Accuracy: 0.977\n",
            "Epoch: 1478; Loss: 0.02019013515902006; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1479; Loss: 0.019180934055660757; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1480; Loss: 0.028002983503482917; Train Acc: 0.998046875; Test Accuracy: 0.9793\n",
            "Epoch: 1481; Loss: 0.015239613214149034; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1482; Loss: 0.03201018529626765; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1483; Loss: 0.02603450047442707; Train Acc: 0.998046875; Test Accuracy: 0.9793\n",
            "Epoch: 1484; Loss: 0.017843194138061277; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1485; Loss: 0.017345285656402285; Train Acc: 0.998046875; Test Accuracy: 0.9797\n",
            "Epoch: 1486; Loss: 0.012204300760584633; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1487; Loss: 0.013988662669101828; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1488; Loss: 0.030496245393083637; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1489; Loss: 0.030663936718522817; Train Acc: 0.9921875; Test Accuracy: 0.9764\n",
            "Epoch: 1490; Loss: 0.03774454022365211; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1491; Loss: 0.017737489750669348; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1492; Loss: 0.027764448442980014; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1493; Loss: 0.02532254184148179; Train Acc: 0.998046875; Test Accuracy: 0.9775\n",
            "Epoch: 1494; Loss: 0.032351055562323316; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1495; Loss: 0.028316030988616778; Train Acc: 0.99609375; Test Accuracy: 0.979\n",
            "Epoch: 1496; Loss: 0.03156670696954473; Train Acc: 0.994140625; Test Accuracy: 0.9775\n",
            "Epoch: 1497; Loss: 0.0310107985431783; Train Acc: 0.99609375; Test Accuracy: 0.9748\n",
            "Epoch: 1498; Loss: 0.060349745247603684; Train Acc: 0.984375; Test Accuracy: 0.9663\n",
            "Epoch: 1499; Loss: 0.062728719048369; Train Acc: 0.990234375; Test Accuracy: 0.9756\n",
            "Epoch: 1500; Loss: 0.03484227726880387; Train Acc: 1.0; Test Accuracy: 0.9759\n",
            "Epoch: 1501; Loss: 0.016711356334613542; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1502; Loss: 0.016223072268776853; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1503; Loss: 0.013496883870849876; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1504; Loss: 0.014511679227532125; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1505; Loss: 0.012660783349417259; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1506; Loss: 0.022432639653048793; Train Acc: 0.998046875; Test Accuracy: 0.9797\n",
            "Epoch: 1507; Loss: 0.018754025396184756; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1508; Loss: 0.035856402818240526; Train Acc: 0.998046875; Test Accuracy: 0.9772\n",
            "Epoch: 1509; Loss: 0.025166868941535192; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1510; Loss: 0.018884451241279118; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1511; Loss: 0.024682926526575284; Train Acc: 0.998046875; Test Accuracy: 0.9794\n",
            "Epoch: 1512; Loss: 0.028666372525611137; Train Acc: 0.99609375; Test Accuracy: 0.9789\n",
            "Epoch: 1513; Loss: 0.024543747087556435; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1514; Loss: 0.021887552540279222; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1515; Loss: 0.015239057340969982; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1516; Loss: 0.018673910161224973; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1517; Loss: 0.010909016687091634; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1518; Loss: 0.018435684474692238; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1519; Loss: 0.02947028154506918; Train Acc: 0.9921875; Test Accuracy: 0.9739\n",
            "Epoch: 1520; Loss: 0.027465784324610127; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1521; Loss: 0.01844180541192504; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1522; Loss: 0.015210061658651072; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1523; Loss: 0.025404220144699535; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1524; Loss: 0.021373874756300752; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1525; Loss: 0.01657313499438765; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1526; Loss: 0.0221481989053623; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1527; Loss: 0.022203749062078046; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1528; Loss: 0.012213260800279577; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1529; Loss: 0.015216773797929385; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1530; Loss: 0.024542062668344786; Train Acc: 1.0; Test Accuracy: 0.9767\n",
            "Epoch: 1531; Loss: 0.02569017745084403; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1532; Loss: 0.010548800225262127; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1533; Loss: 0.03958541805544953; Train Acc: 0.99609375; Test Accuracy: 0.9764\n",
            "Epoch: 1534; Loss: 0.048201174193873486; Train Acc: 0.994140625; Test Accuracy: 0.9771\n",
            "Epoch: 1535; Loss: 0.014303091320657196; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1536; Loss: 0.007708681747959243; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1537; Loss: 0.019556850419182162; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1538; Loss: 0.013988132132415426; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1539; Loss: 0.01937284941317281; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1540; Loss: 0.016819677866037275; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1541; Loss: 0.02088458577248356; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1542; Loss: 0.02234250779786426; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1543; Loss: 0.019331742649638654; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1544; Loss: 0.01896668678466027; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1545; Loss: 0.016143572326309648; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1546; Loss: 0.014857669441800836; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1547; Loss: 0.01506879926178061; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1548; Loss: 0.020117107415257022; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1549; Loss: 0.008395706953950904; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1550; Loss: 0.04209103675291008; Train Acc: 0.99609375; Test Accuracy: 0.9787\n",
            "Epoch: 1551; Loss: 0.018572044316217215; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1552; Loss: 0.03028030663875392; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1553; Loss: 0.012178483460536573; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1554; Loss: 0.030531059577809726; Train Acc: 0.998046875; Test Accuracy: 0.9796\n",
            "Epoch: 1555; Loss: 0.009795268430009486; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1556; Loss: 0.02830437913465483; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1557; Loss: 0.023568240193504137; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1558; Loss: 0.008618016959199355; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1559; Loss: 0.025158382311672514; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1560; Loss: 0.01916648614163485; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1561; Loss: 0.012239599314551923; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1562; Loss: 0.015673866912421633; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1563; Loss: 0.020416773030870916; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1564; Loss: 0.010218681749081322; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1565; Loss: 0.010528265451307736; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1566; Loss: 0.01860545762951732; Train Acc: 0.998046875; Test Accuracy: 0.9793\n",
            "Epoch: 1567; Loss: 0.024104561706080198; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1568; Loss: 0.015187427315211537; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1569; Loss: 0.02295179837828775; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1570; Loss: 0.012537031601582324; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1571; Loss: 0.029200270534159772; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 1572; Loss: 0.011781220822849152; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1573; Loss: 0.012223157618366884; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1574; Loss: 0.028138901660083603; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1575; Loss: 0.024222003664332515; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1576; Loss: 0.012921369887556718; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1577; Loss: 0.01644116491022896; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1578; Loss: 0.021686338780443997; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1579; Loss: 0.021009017806332905; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1580; Loss: 0.014847020781164996; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1581; Loss: 0.036435610758783245; Train Acc: 0.99609375; Test Accuracy: 0.9774\n",
            "Epoch: 1582; Loss: 0.01599477075650506; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1583; Loss: 0.021329734548232476; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1584; Loss: 0.02531312002170731; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1585; Loss: 0.02312049444195776; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1586; Loss: 0.022770270944057484; Train Acc: 0.998046875; Test Accuracy: 0.9793\n",
            "Epoch: 1587; Loss: 0.017950450067167327; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1588; Loss: 0.02183154034246961; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1589; Loss: 0.013204342519661665; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1590; Loss: 0.03603197828604524; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1591; Loss: 0.02810984496939812; Train Acc: 1.0; Test Accuracy: 0.9775\n",
            "Epoch: 1592; Loss: 0.01507498353521304; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1593; Loss: 0.01852561539316952; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1594; Loss: 0.01732251290410638; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1595; Loss: 0.024510967345472506; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1596; Loss: 0.01818328890434562; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1597; Loss: 0.025531265334466608; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1598; Loss: 0.019122038366692296; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1599; Loss: 0.02477305759067412; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1600; Loss: 0.010581001840742269; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1601; Loss: 0.01627170774388874; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1602; Loss: 0.009835330152311902; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1603; Loss: 0.028550487974189416; Train Acc: 1.0; Test Accuracy: 0.9718\n",
            "Epoch: 1604; Loss: 0.024748187048084746; Train Acc: 1.0; Test Accuracy: 0.9774\n",
            "Epoch: 1605; Loss: 0.03267635713709033; Train Acc: 0.994140625; Test Accuracy: 0.9734\n",
            "Epoch: 1606; Loss: 0.023724482535878874; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1607; Loss: 0.020346799809818482; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1608; Loss: 0.022584923611029192; Train Acc: 1.0; Test Accuracy: 0.9771\n",
            "Epoch: 1609; Loss: 0.012363181468970677; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1610; Loss: 0.01709117731634039; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1611; Loss: 0.009199229647656851; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1612; Loss: 0.017033961564062026; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1613; Loss: 0.02177144449049918; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1614; Loss: 0.030345123298887763; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1615; Loss: 0.023091665479279933; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1616; Loss: 0.014667601678896476; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1617; Loss: 0.01248135626037265; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1618; Loss: 0.014856907978933687; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1619; Loss: 0.011774223140448146; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1620; Loss: 0.0073301780975869; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1621; Loss: 0.011575626047695575; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1622; Loss: 0.013149888782098686; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1623; Loss: 0.03302142354826443; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1624; Loss: 0.019940553691755555; Train Acc: 0.998046875; Test Accuracy: 0.9799\n",
            "Epoch: 1625; Loss: 0.021250353060296902; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1626; Loss: 0.015775167357807658; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1627; Loss: 0.023319747252804328; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1628; Loss: 0.026628090940768283; Train Acc: 0.998046875; Test Accuracy: 0.9773\n",
            "Epoch: 1629; Loss: 0.03630968374235803; Train Acc: 0.99609375; Test Accuracy: 0.9799\n",
            "Epoch: 1630; Loss: 0.013234194552907079; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1631; Loss: 0.022780275413528672; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1632; Loss: 0.011433993382124388; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1633; Loss: 0.011628899141056548; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1634; Loss: 0.027602617394649233; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1635; Loss: 0.012227605268329158; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1636; Loss: 0.012456093259290799; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1637; Loss: 0.01424207766789461; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1638; Loss: 0.019472061281799225; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1639; Loss: 0.018829072249404236; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1640; Loss: 0.02476000818892746; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1641; Loss: 0.016528493158384207; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1642; Loss: 0.011154381324717556; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1643; Loss: 0.018504843866718836; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1644; Loss: 0.012524488657856825; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1645; Loss: 0.012467517801355636; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1646; Loss: 0.03361392699367815; Train Acc: 0.99609375; Test Accuracy: 0.9786\n",
            "Epoch: 1647; Loss: 0.01954129601853523; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1648; Loss: 0.022591414550616536; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1649; Loss: 0.023543767957227502; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1650; Loss: 0.016521321372257227; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1651; Loss: 0.01857439191113272; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1652; Loss: 0.015313917928347676; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1653; Loss: 0.017709916867514754; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1654; Loss: 0.01652224231518281; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1655; Loss: 0.012409879537752681; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1656; Loss: 0.026927786889871304; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1657; Loss: 0.015312421159214091; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1658; Loss: 0.008758158493520216; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1659; Loss: 0.008750690012337487; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1660; Loss: 0.016474625982738862; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1661; Loss: 0.016131855031045955; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1662; Loss: 0.024687412416482375; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1663; Loss: 0.009203854114350807; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1664; Loss: 0.02237915554010107; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1665; Loss: 0.010087800142222422; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1666; Loss: 0.02194928313061595; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1667; Loss: 0.016362884567355705; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1668; Loss: 0.012844760495452686; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1669; Loss: 0.012614522301912243; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1670; Loss: 0.01955734026846405; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1671; Loss: 0.01661537689057741; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1672; Loss: 0.005833623799053882; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1673; Loss: 0.014394371352883632; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1674; Loss: 0.026712149340566534; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1675; Loss: 0.011816127718983582; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1676; Loss: 0.014145191223728773; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1677; Loss: 0.014364101573344936; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1678; Loss: 0.019013155347233118; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1679; Loss: 0.01599019891782448; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1680; Loss: 0.0402038760099115; Train Acc: 0.998046875; Test Accuracy: 0.9783\n",
            "Epoch: 1681; Loss: 0.013124928148582578; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1682; Loss: 0.014895645691050067; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1683; Loss: 0.028044592000344364; Train Acc: 0.99609375; Test Accuracy: 0.9783\n",
            "Epoch: 1684; Loss: 0.028077285378954823; Train Acc: 1.0; Test Accuracy: 0.9765\n",
            "Epoch: 1685; Loss: 0.01267350438488607; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1686; Loss: 0.011240253091639087; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1687; Loss: 0.013765169371602683; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1688; Loss: 0.02023326252226618; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1689; Loss: 0.020081265418101332; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1690; Loss: 0.024796610798406865; Train Acc: 0.998046875; Test Accuracy: 0.978\n",
            "Epoch: 1691; Loss: 0.01563631365407469; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1692; Loss: 0.013064418594673981; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1693; Loss: 0.005583456224386696; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1694; Loss: 0.013762821858787728; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1695; Loss: 0.01309999838700903; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1696; Loss: 0.011737541481262645; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1697; Loss: 0.012240443385939822; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1698; Loss: 0.014222020432004214; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1699; Loss: 0.023871648166097503; Train Acc: 1.0; Test Accuracy: 0.978\n",
            "Epoch: 1700; Loss: 0.022014220933167142; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1701; Loss: 0.012869430101531739; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1702; Loss: 0.0159616339608599; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1703; Loss: 0.029348375037292904; Train Acc: 0.998046875; Test Accuracy: 0.9776\n",
            "Epoch: 1704; Loss: 0.01809945487044211; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1705; Loss: 0.012419606392409882; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1706; Loss: 0.01265096156722131; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1707; Loss: 0.021432738232627674; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1708; Loss: 0.02367706131954362; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1709; Loss: 0.014950822562047886; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1710; Loss: 0.011677288329550535; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1711; Loss: 0.01470864501885908; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1712; Loss: 0.007034455357678676; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1713; Loss: 0.007663791995598327; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1714; Loss: 0.015351977458284398; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1715; Loss: 0.010623724928333737; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1716; Loss: 0.018517269051271018; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1717; Loss: 0.01267396424540148; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1718; Loss: 0.005734709261084306; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1719; Loss: 0.021918067981267813; Train Acc: 0.998046875; Test Accuracy: 0.9801\n",
            "Epoch: 1720; Loss: 0.014255175898822197; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1721; Loss: 0.02934412899681646; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1722; Loss: 0.011947543950335418; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1723; Loss: 0.013884212741555227; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1724; Loss: 0.0069260027607445365; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1725; Loss: 0.01586065348941719; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1726; Loss: 0.019761236892116615; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1727; Loss: 0.0099218359185208; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1728; Loss: 0.0278313873501649; Train Acc: 0.998046875; Test Accuracy: 0.9786\n",
            "Epoch: 1729; Loss: 0.010747418057539803; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1730; Loss: 0.019863357122673092; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1731; Loss: 0.012960850859367247; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1732; Loss: 0.008960312355791136; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1733; Loss: 0.012867376639228989; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1734; Loss: 0.012062590148352068; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1735; Loss: 0.01000470781292868; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1736; Loss: 0.01260785187728055; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1737; Loss: 0.016608409937106286; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1738; Loss: 0.010436822657064124; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1739; Loss: 0.012832020870195648; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1740; Loss: 0.011348920503387434; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1741; Loss: 0.02151719128875508; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1742; Loss: 0.03812711902202512; Train Acc: 0.994140625; Test Accuracy: 0.977\n",
            "Epoch: 1743; Loss: 0.02746295346994869; Train Acc: 0.99609375; Test Accuracy: 0.9766\n",
            "Epoch: 1744; Loss: 0.012291654682894532; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1745; Loss: 0.0084954233360134; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1746; Loss: 0.014239895395256236; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1747; Loss: 0.020427737412423243; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1748; Loss: 0.03265193481861661; Train Acc: 0.99609375; Test Accuracy: 0.9762\n",
            "Epoch: 1749; Loss: 0.015704020733839155; Train Acc: 1.0; Test Accuracy: 0.9784\n",
            "Epoch: 1750; Loss: 0.008483814332266727; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1751; Loss: 0.013199697319878263; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 1752; Loss: 0.014839499958049717; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1753; Loss: 0.020996497544164956; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1754; Loss: 0.024969055770674112; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1755; Loss: 0.018024687616904365; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1756; Loss: 0.011648813482097203; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1757; Loss: 0.018512154458297406; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1758; Loss: 0.016605993130900866; Train Acc: 1.0; Test Accuracy: 0.9776\n",
            "Epoch: 1759; Loss: 0.013528998797555331; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1760; Loss: 0.021887564727295958; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1761; Loss: 0.009613022290899805; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1762; Loss: 0.010898908888549616; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1763; Loss: 0.016673298370538374; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1764; Loss: 0.012625560897807956; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1765; Loss: 0.03259485328685556; Train Acc: 1.0; Test Accuracy: 0.9772\n",
            "Epoch: 1766; Loss: 0.02218101435222774; Train Acc: 0.998046875; Test Accuracy: 0.9774\n",
            "Epoch: 1767; Loss: 0.01032411972605298; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1768; Loss: 0.015776838514456836; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1769; Loss: 0.009931909310235135; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1770; Loss: 0.019308022890955116; Train Acc: 0.998046875; Test Accuracy: 0.979\n",
            "Epoch: 1771; Loss: 0.013519479301755205; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1772; Loss: 0.012277550847543808; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1773; Loss: 0.011875655152934378; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1774; Loss: 0.010609411016080386; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1775; Loss: 0.014718019343605171; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1776; Loss: 0.007537298219360071; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1777; Loss: 0.021604332464608647; Train Acc: 0.998046875; Test Accuracy: 0.981\n",
            "Epoch: 1778; Loss: 0.006262436872840586; Train Acc: 1.0; Test Accuracy: 0.9807\n",
            "Epoch: 1779; Loss: 0.01836758846756398; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1780; Loss: 0.013974205796262656; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1781; Loss: 0.014342041216946901; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1782; Loss: 0.029835968692550834; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1783; Loss: 0.014391088408761712; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1784; Loss: 0.026240584499785762; Train Acc: 0.998046875; Test Accuracy: 0.9779\n",
            "Epoch: 1785; Loss: 0.01809515464643483; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1786; Loss: 0.024602152525111713; Train Acc: 0.99609375; Test Accuracy: 0.9783\n",
            "Epoch: 1787; Loss: 0.016705810042598784; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1788; Loss: 0.00965932509116901; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1789; Loss: 0.013440202667734772; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1790; Loss: 0.011736056824451859; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1791; Loss: 0.03248328348993347; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1792; Loss: 0.01519072036428568; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1793; Loss: 0.019662909033204518; Train Acc: 0.998046875; Test Accuracy: 0.9782\n",
            "Epoch: 1794; Loss: 0.02942343128066415; Train Acc: 0.998046875; Test Accuracy: 0.9788\n",
            "Epoch: 1795; Loss: 0.019140576945217456; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1796; Loss: 0.020065268755508365; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1797; Loss: 0.013850607993199314; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1798; Loss: 0.012947831167972754; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1799; Loss: 0.017658830105916627; Train Acc: 0.998046875; Test Accuracy: 0.9802\n",
            "Epoch: 1800; Loss: 0.008250933109988569; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1801; Loss: 0.01479000584873335; Train Acc: 1.0; Test Accuracy: 0.9809\n",
            "Epoch: 1802; Loss: 0.01641868055839411; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1803; Loss: 0.013659891173700468; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1804; Loss: 0.012818483072161966; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1805; Loss: 0.013964992263823198; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1806; Loss: 0.007972333478934774; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1807; Loss: 0.015619868003603203; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1808; Loss: 0.0070595571564405; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1809; Loss: 0.006256301298981803; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1810; Loss: 0.008864804022712065; Train Acc: 1.0; Test Accuracy: 0.981\n",
            "Epoch: 1811; Loss: 0.015695951847496028; Train Acc: 0.998046875; Test Accuracy: 0.9805\n",
            "Epoch: 1812; Loss: 0.019749322399013884; Train Acc: 0.998046875; Test Accuracy: 0.98\n",
            "Epoch: 1813; Loss: 0.01317665538518751; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1814; Loss: 0.013514909581494842; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1815; Loss: 0.007721850028354782; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1816; Loss: 0.01810466302542664; Train Acc: 1.0; Test Accuracy: 0.9809\n",
            "Epoch: 1817; Loss: 0.028504033916700748; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1818; Loss: 0.015555765081108702; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1819; Loss: 0.017570189044752788; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1820; Loss: 0.013373234674721304; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1821; Loss: 0.010557950859320438; Train Acc: 1.0; Test Accuracy: 0.981\n",
            "Epoch: 1822; Loss: 0.01141662433455246; Train Acc: 1.0; Test Accuracy: 0.9806\n",
            "Epoch: 1823; Loss: 0.017838131159319995; Train Acc: 0.998046875; Test Accuracy: 0.9804\n",
            "Epoch: 1824; Loss: 0.014656712084880941; Train Acc: 0.998046875; Test Accuracy: 0.9813\n",
            "Epoch: 1825; Loss: 0.038883642187056956; Train Acc: 0.99609375; Test Accuracy: 0.9752\n",
            "Epoch: 1826; Loss: 0.025013096368920847; Train Acc: 0.998046875; Test Accuracy: 0.976\n",
            "Epoch: 1827; Loss: 0.017527528246457962; Train Acc: 0.998046875; Test Accuracy: 0.9785\n",
            "Epoch: 1828; Loss: 0.01123985166018818; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1829; Loss: 0.030066606283077082; Train Acc: 0.99609375; Test Accuracy: 0.9801\n",
            "Epoch: 1830; Loss: 0.013135507981439955; Train Acc: 0.998046875; Test Accuracy: 0.9802\n",
            "Epoch: 1831; Loss: 0.013812335377209438; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1832; Loss: 0.01721760564891852; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1833; Loss: 0.019139460309437254; Train Acc: 0.998046875; Test Accuracy: 0.9804\n",
            "Epoch: 1834; Loss: 0.009676143021199315; Train Acc: 1.0; Test Accuracy: 0.9812\n",
            "Epoch: 1835; Loss: 0.009541552672985057; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1836; Loss: 0.013717916560507264; Train Acc: 0.998046875; Test Accuracy: 0.9784\n",
            "Epoch: 1837; Loss: 0.014406757221959167; Train Acc: 1.0; Test Accuracy: 0.9803\n",
            "Epoch: 1838; Loss: 0.01658678196465583; Train Acc: 0.998046875; Test Accuracy: 0.9803\n",
            "Epoch: 1839; Loss: 0.014073652882415086; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1840; Loss: 0.010742755505689157; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1841; Loss: 0.013017190713407832; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1842; Loss: 0.015780889459341335; Train Acc: 1.0; Test Accuracy: 0.9811\n",
            "Epoch: 1843; Loss: 0.022419630122546948; Train Acc: 0.998046875; Test Accuracy: 0.9806\n",
            "Epoch: 1844; Loss: 0.010782264283621761; Train Acc: 1.0; Test Accuracy: 0.9808\n",
            "Epoch: 1845; Loss: 0.014731196936678785; Train Acc: 0.998046875; Test Accuracy: 0.9803\n",
            "Epoch: 1846; Loss: 0.012239016067640232; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1847; Loss: 0.0138485459976718; Train Acc: 1.0; Test Accuracy: 0.9808\n",
            "Epoch: 1848; Loss: 0.007518127911825687; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1849; Loss: 0.024414456026009824; Train Acc: 0.998046875; Test Accuracy: 0.9771\n",
            "Epoch: 1850; Loss: 0.025147407821958134; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1851; Loss: 0.008539512662520697; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1852; Loss: 0.01423788472239378; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1853; Loss: 0.014430550131494009; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1854; Loss: 0.011981160690423213; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1855; Loss: 0.005652986719915824; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1856; Loss: 0.01660549362168029; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1857; Loss: 0.023595058276329094; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1858; Loss: 0.021020492303814146; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1859; Loss: 0.018079775883791362; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1860; Loss: 0.02198906905599802; Train Acc: 0.998046875; Test Accuracy: 0.9789\n",
            "Epoch: 1861; Loss: 0.019325398522363514; Train Acc: 0.998046875; Test Accuracy: 0.9782\n",
            "Epoch: 1862; Loss: 0.018730059966031505; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1863; Loss: 0.01456053086722058; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1864; Loss: 0.010550936506839713; Train Acc: 0.998046875; Test Accuracy: 0.9809\n",
            "Epoch: 1865; Loss: 0.013387393240786784; Train Acc: 0.998046875; Test Accuracy: 0.9791\n",
            "Epoch: 1866; Loss: 0.009827952141058714; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1867; Loss: 0.021572275588488357; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1868; Loss: 0.006632062874403211; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1869; Loss: 0.005680070454959987; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1870; Loss: 0.005083697335988978; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1871; Loss: 0.014663177715680157; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1872; Loss: 0.01512033342948033; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1873; Loss: 0.011055311398433312; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1874; Loss: 0.013636612215420518; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1875; Loss: 0.008593776429120674; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1876; Loss: 0.007728268237453286; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1877; Loss: 0.012731945947289259; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1878; Loss: 0.009002164170016754; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1879; Loss: 0.011452669224615797; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1880; Loss: 0.011710654966667284; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1881; Loss: 0.008654646280241499; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1882; Loss: 0.006859509047559838; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1883; Loss: 0.01530639401494508; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1884; Loss: 0.02020921567286131; Train Acc: 0.998046875; Test Accuracy: 0.9793\n",
            "Epoch: 1885; Loss: 0.023100709281910034; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1886; Loss: 0.015904516788134877; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1887; Loss: 0.009155530081482514; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1888; Loss: 0.010079207480227908; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1889; Loss: 0.015047046553962756; Train Acc: 0.998046875; Test Accuracy: 0.9795\n",
            "Epoch: 1890; Loss: 0.012502899160527842; Train Acc: 0.998046875; Test Accuracy: 0.9797\n",
            "Epoch: 1891; Loss: 0.008163096444411991; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1892; Loss: 0.012009693601195488; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1893; Loss: 0.05293742460176279; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1894; Loss: 0.009513085442117642; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1895; Loss: 0.0250711491367187; Train Acc: 0.998046875; Test Accuracy: 0.9796\n",
            "Epoch: 1896; Loss: 0.005506991779850837; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1897; Loss: 0.0037095266803008004; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1898; Loss: 0.009905143591944679; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1899; Loss: 0.015539778737560128; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1900; Loss: 0.009120270404498084; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1901; Loss: 0.01280600600330965; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1902; Loss: 0.009475564246771703; Train Acc: 1.0; Test Accuracy: 0.981\n",
            "Epoch: 1903; Loss: 0.010376705592036937; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1904; Loss: 0.008165337027942077; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1905; Loss: 0.007760129225323008; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1906; Loss: 0.007402246708257021; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1907; Loss: 0.018984837603709424; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1908; Loss: 0.008130066117351793; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1909; Loss: 0.010519456939277163; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1910; Loss: 0.026420576856029353; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1911; Loss: 0.014502533543920062; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1912; Loss: 0.011677704975727482; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1913; Loss: 0.010889920797736125; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1914; Loss: 0.016176906721472047; Train Acc: 1.0; Test Accuracy: 0.9778\n",
            "Epoch: 1915; Loss: 0.01130426624363656; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1916; Loss: 0.007225608660885627; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1917; Loss: 0.010966376255853538; Train Acc: 1.0; Test Accuracy: 0.9777\n",
            "Epoch: 1918; Loss: 0.009930984486417677; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1919; Loss: 0.010540900333207693; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1920; Loss: 0.007096453439862592; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1921; Loss: 0.009907349551322097; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1922; Loss: 0.009245958483732171; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1923; Loss: 0.010416391792004404; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1924; Loss: 0.014361187908034046; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1925; Loss: 0.014673234461172378; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1926; Loss: 0.008933294685384561; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1927; Loss: 0.008782865771914547; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1928; Loss: 0.013812152876662969; Train Acc: 1.0; Test Accuracy: 0.9783\n",
            "Epoch: 1929; Loss: 0.010838142277805028; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1930; Loss: 0.015654892866338442; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1931; Loss: 0.007726791462221457; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1932; Loss: 0.011983026295880696; Train Acc: 1.0; Test Accuracy: 0.9807\n",
            "Epoch: 1933; Loss: 0.010767151846017087; Train Acc: 1.0; Test Accuracy: 0.979\n",
            "Epoch: 1934; Loss: 0.012811624452229161; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1935; Loss: 0.012100344516257024; Train Acc: 1.0; Test Accuracy: 0.9785\n",
            "Epoch: 1936; Loss: 0.012853035347747174; Train Acc: 1.0; Test Accuracy: 0.9791\n",
            "Epoch: 1937; Loss: 0.006258270616057407; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1938; Loss: 0.00626550365254391; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1939; Loss: 0.010917905341022412; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1940; Loss: 0.02056383231709672; Train Acc: 0.998046875; Test Accuracy: 0.9802\n",
            "Epoch: 1941; Loss: 0.010118475682446403; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1942; Loss: 0.00661482167938482; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1943; Loss: 0.02297378607002174; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1944; Loss: 0.008304258104279434; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1945; Loss: 0.012935003576659613; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1946; Loss: 0.01604301614179287; Train Acc: 1.0; Test Accuracy: 0.9796\n",
            "Epoch: 1947; Loss: 0.01589184936812172; Train Acc: 1.0; Test Accuracy: 0.9781\n",
            "Epoch: 1948; Loss: 0.01816724877452748; Train Acc: 1.0; Test Accuracy: 0.9779\n",
            "Epoch: 1949; Loss: 0.028849015952991896; Train Acc: 0.9921875; Test Accuracy: 0.9752\n",
            "Epoch: 1950; Loss: 0.028043894883515816; Train Acc: 0.998046875; Test Accuracy: 0.9778\n",
            "Epoch: 1951; Loss: 0.02573115357992213; Train Acc: 0.998046875; Test Accuracy: 0.9766\n",
            "Epoch: 1952; Loss: 0.011783382021986468; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1953; Loss: 0.005331041930361593; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1954; Loss: 0.02785787275950837; Train Acc: 0.998046875; Test Accuracy: 0.9803\n",
            "Epoch: 1955; Loss: 0.01328593822172601; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1956; Loss: 0.01601452135006127; Train Acc: 1.0; Test Accuracy: 0.9801\n",
            "Epoch: 1957; Loss: 0.009143757606229319; Train Acc: 1.0; Test Accuracy: 0.9809\n",
            "Epoch: 1958; Loss: 0.009698877684555992; Train Acc: 1.0; Test Accuracy: 0.9806\n",
            "Epoch: 1959; Loss: 0.0058625922273553045; Train Acc: 1.0; Test Accuracy: 0.9806\n",
            "Epoch: 1960; Loss: 0.014356395188444736; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1961; Loss: 0.00835659038840349; Train Acc: 1.0; Test Accuracy: 0.9806\n",
            "Epoch: 1962; Loss: 0.009044119918295481; Train Acc: 1.0; Test Accuracy: 0.9794\n",
            "Epoch: 1963; Loss: 0.008491405584117474; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1964; Loss: 0.009205958802458291; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1965; Loss: 0.013290066042297296; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1966; Loss: 0.011308662802208193; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1967; Loss: 0.010428737616055075; Train Acc: 1.0; Test Accuracy: 0.9798\n",
            "Epoch: 1968; Loss: 0.01055344064812845; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1969; Loss: 0.008567965169494991; Train Acc: 1.0; Test Accuracy: 0.9807\n",
            "Epoch: 1970; Loss: 0.008588712947572245; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1971; Loss: 0.007986442436889375; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1972; Loss: 0.0049511002237157; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1973; Loss: 0.019591472435343656; Train Acc: 1.0; Test Accuracy: 0.9789\n",
            "Epoch: 1974; Loss: 0.02231715515283775; Train Acc: 0.998046875; Test Accuracy: 0.9787\n",
            "Epoch: 1975; Loss: 0.017062621492724174; Train Acc: 1.0; Test Accuracy: 0.9805\n",
            "Epoch: 1976; Loss: 0.010601558359249181; Train Acc: 1.0; Test Accuracy: 0.9802\n",
            "Epoch: 1977; Loss: 0.00634440885275043; Train Acc: 1.0; Test Accuracy: 0.9804\n",
            "Epoch: 1978; Loss: 0.00529337687193214; Train Acc: 1.0; Test Accuracy: 0.9808\n",
            "Epoch: 1979; Loss: 0.012611355362603372; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1980; Loss: 0.014493217225183395; Train Acc: 1.0; Test Accuracy: 0.9787\n",
            "Epoch: 1981; Loss: 0.017744366787732398; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1982; Loss: 0.029510238228240785; Train Acc: 1.0; Test Accuracy: 0.9762\n",
            "Epoch: 1983; Loss: 0.017267108061583762; Train Acc: 1.0; Test Accuracy: 0.9786\n",
            "Epoch: 1984; Loss: 0.010108660595173007; Train Acc: 1.0; Test Accuracy: 0.9795\n",
            "Epoch: 1985; Loss: 0.013854334769263154; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1986; Loss: 0.015508054994358002; Train Acc: 0.998046875; Test Accuracy: 0.9792\n",
            "Epoch: 1987; Loss: 0.023147208670156703; Train Acc: 1.0; Test Accuracy: 0.9799\n",
            "Epoch: 1988; Loss: 0.010525643167604947; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1989; Loss: 0.011713270463088038; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1990; Loss: 0.012307568148235297; Train Acc: 0.998046875; Test Accuracy: 0.9796\n",
            "Epoch: 1991; Loss: 0.011276526740908469; Train Acc: 1.0; Test Accuracy: 0.9793\n",
            "Epoch: 1992; Loss: 0.014043215888227129; Train Acc: 1.0; Test Accuracy: 0.9788\n",
            "Epoch: 1993; Loss: 0.012986206374258319; Train Acc: 0.998046875; Test Accuracy: 0.9798\n",
            "Epoch: 1994; Loss: 0.013361139096929141; Train Acc: 1.0; Test Accuracy: 0.9792\n",
            "Epoch: 1995; Loss: 0.022001687769846463; Train Acc: 1.0; Test Accuracy: 0.9797\n",
            "Epoch: 1996; Loss: 0.02391557482425376; Train Acc: 1.0; Test Accuracy: 0.9782\n",
            "Epoch: 1997; Loss: 0.01425826675728544; Train Acc: 1.0; Test Accuracy: 0.9808\n",
            "Epoch: 1998; Loss: 0.012426735121795408; Train Acc: 1.0; Test Accuracy: 0.98\n",
            "Epoch: 1999; Loss: 0.010977582870034953; Train Acc: 1.0; Test Accuracy: 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDnTbsxKwjF9",
        "colab_type": "text"
      },
      "source": [
        "#### **Losses over iterations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeT98E4su2xE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "dbf1bb99-0c4c-40ab-efcd-1b4d6c89fa4e"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.title('Loss vs Iteration')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdM43VMw05i",
        "colab_type": "text"
      },
      "source": [
        "#### **Checking predictions of random test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcImAFglqbHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d43d7124-ba7f-46ae-91a6-4764f44a388f"
      },
      "source": [
        "y_pred= MNIST_net.predict(X_test)\n",
        "test_imgs= X_test.reshape(-1,28,28)\n",
        "random_idx= np.random.choice(10000,5)\n",
        "_,axes= plt.subplots(1,5)\n",
        "for i in range(5):\n",
        "  img= test_imgs[random_idx[i]]\n",
        "  axes[i].imshow(img.reshape(28,28), cmap='gray')\n",
        "  axes[i].set_title(y_pred[random_idx[i]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABpCAYAAAAnQqjlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eWzj2X3g+XniJYqHSFEkdd9HVUl1d1dVd1e13d3udLcdr+20PekgSLLrGFk48GaymBlMMNgFBthdrBfYTTCYwS7iTbLjsQ04STvp8dGJ+6quy113q7pUkqpK1EkdpERJvERJPH77h+o9SyWpTkkUq34fQJBEUeT7ffne9/d93+sJTdPQ0dHR0SlcivI9AB0dHR2dx0NX5Do6OjoFjq7IdXR0dAocXZHr6OjoFDi6ItfR0dEpcHRFrqOjo1Pg6IpcR0dHp8B5YhW5EGK3EOIjIURUCNEvhPhavse0ExBCvCWE6BVCJIUQASHEiXyPKZ/o82Rj9LmyGiHED4UQE0KImBDilhDiW/kek+SJVORCCCPwX4GfA2XAHwE/FEK05XVgeUYI8SrwfwD/HeAAXgQG8jqoPKLPk43R58q6/O9Ag6ZpTuC/Af5XIcThPI8JAPEkVnYKITqB84BDu3OBQoj3gAuapv3PeR1cHhFC/Ar4a03T/jrfY9kJ6PNkY/S5cm+EEO3Ax8C/1DTt7/I8nCfTIt8AAXTmexD5QghhAJ4BvHdcCEEhxH8SQljzPbYdxlM9T0CfK/dCCPF/CyHmgT5gAng3z0MCnlxFfhMIA/9GCGESQvwG8DmgJL/Dyit+wAR8HTgBHAAOAv9TPgeVZ/R5sj76XNkATdP+mGVX0wngH4DF/I5omSdSkWualga+CnwJmAT+FfB3QDCf48ozqTvf/6OmaROapk0Dfw58MY9jyiv6PNkQfa7cA03TspqmnQVqgG/nezwAxnwPYKvQNO0zlq0rQPn8vp+/EeUXTdNmhRBBYGVQ5MkLkDwk+jxZiz5XHhgj0JzvQcATapEDCCH2CSGKhRAlQoh/DVQC/znPw8o3/x/wPwghfEIIN/A/spyx8dSiz5MN0efKCu7I4S0hhF0IYRBCvAb8DvBhvscGT7AiB36P5WBEGHgFeFXTtB3hz8oj/wtwCbgF9AKfAv9bXkeUf/R5sj76XFmNxrIbJQjMAv8n8Keapv00r6O6wxOZfqijo6PzNPEkW+Q6Ojo6TwW6ItfR0dEpcB5LkQshXhdC3LxTNPBnmzWoQkaXyfroclmLLpO16DJ5NB7ZR36n+usW8CrLAYBLwO9omtazecMrLHSZrI8ul7XoMlmLLpNH53Es8iNAv6ZpA5qmLQE/Br6yOcMqWHSZrI8ul7XoMlmLLpNH5HEKgqqB0RW/B4Gj9/oHIcRTkSIjhJjSNM2LLpOVLKz4+Z5y0WWyPk+RXCS6TH7N9B2dsi5bXtkphPgjltuDPk0M3+uPT6lMEvf6oy6T9XlK5XJPnlKZ3FOnPI4iHwNqV/xec+exVWia9j3ge/BU3T0lukx+jXnFz2vkostEnyvroMvkAXkcH/kloFUI0SiEMANvATuiymkHYNZlsoZifa6sQZfJOugyeXge2SLXNC0jhPgO8EvAAPyNpmk3Nm1khU0by2XNukx+zQj6XLkbXSbro8vkIXksH7mmae+yQxqr7zC6NU17Jt+D2GFEdZmsQZfJOmia9tQftfew6JWdOjo6OgXOE9uPXOfREUIAYDAYEEJQVFSkHgPQNI1sNksulyOXy+VrmDp5RM6HoqKiVV+apqn5kclk8jzKpwddkeusQghBeXk5NpuN3bt3U1tbS21tLZWVleo5gUCAvr4+BgcHuXbtGnoHzacHeWP3eDyUlJTQ0tJCeXk5e/bsoaamhkQiQSqV4uTJk3z88cdkMhldoW8DuiLXURgMBgwGA6WlpbhcLvbs2cPu3bvp7OykpaVFWVtXrlwBIJVKce3atTyPWmc7KSoqwmg04nK5cLlctLa2Ul9fz4svvsiuXbuYm5sjFosRDAY5d+4cmqbpinwb0BX5U44QgpKSEmw2G1/+8pdpamrC4/Fgs9loaGjA6/Xidrux2WyMjo4SDAbp6uri0qVLBINP+9GWTw9CCKxWKy+88AJVVVUcO3aMyspKPB4PDoeDiooKSkpKMBqNOJ1OysrKMJvNZLPZfA/9qeCpVOR3+3ufZoqKiiguLsbpdPLiiy9y5MgR7HY7xcXF2Gw2iouLlYwSiQQjIyMMDQ0xMDBALBYrOPmt/OwfhEK7vq3CYDBQXFxMR0cHu3fv5rXXXqOurm7N88zm5RqnkpISTCYTS0tL2z3Up5KnQpEXFRVht9uxWq3s3r0bl8tFeXk5JpNJWZbRaJRUKnX/F3tCsFgsdHZ2UlZWxq5du/B6vXR2duLz+TCbzRQVFREOh4lGowQCAYLBIIFAgMHBQUZGRohEIiwuFsaJaBaLBYfDQUtLC1/96lcxm81KoWuaRi6XI5VKkclkiEajpNNphBBkMhm6u7uZmZkhGAwSi8XyfCXbi9FoxO1243A4OHz4MBUVFbz66qvU1NTgdrvX/Z/Z2VlisRiRSISFhQXdrbJNPDWK3GazUVpaysGDB6mrq6O5uZmSkhIVnFlcXHzqFPmuXbuoq6vj+eefp7KykqamJkpLS4FlBTczM8PQ0BCnTp3iypUrTE5OEgqFWFpaKhglDstWYmlpKXv37uWP//iPsdvtqxR5JpMhFouRSqUIBoMsLCwghGBxcRGz2UwgEFC+36cJg8GA2+3G5/Px+c9/noaGBo4cOYLH49nwf2KxGBMTE+qGqLtWtocnUpEbDAacTic2m43W1lZcLhednZ243W52796N2+3G7XZjNBp544032LNnDx988AE3btyguLgYi8XCwsICCwsLJJNJEon79jYqGGw2GwcPHsTr9bJ37158Ph91dXWUl5cDywuxq6uL8fFxBgcHCYVCXL9+nZGRERKJBIuLiwWzOO12u/L3l5eX43Q6iUajwLIchBBomoYQAovFgtFopLq6mkwmoyzyV199lf379+NwOLh9+zbDw8NMT0/n+cq2lpKSEtra2vB4PDz//PP4fD4OHTqkMlUAcrmckh1AJBIhHo9z9epV+vr66O/vZ2lpacfPlaKiInXDqqysZHFxkWQySTqdVsaKTK2Uc8Rut2M0LqvOXC636vmZTIZ4PL7tLqUnUpEbjUZKS0vx+/289NJL1NTU8PnPf57y8nKsVisGgwFYtsZcLheJRIKZmRkikQgulwuHw6G2iFNTU0+cIj9+/DjV1dU0NDRQWlpKXV2dUnLxeJwzZ85w9epVde1jY2NMTU3le+gPjcPhoL6+HqvVqj7XeDxOUVERVquVoqLlejgZyANWWesA1dXVpFIpFhYWcDgcJBKJJ16R22w29u/fT319PW+++SY+n4+ysjJMJpN6TjabXaXIw+EwExMTXL58mYsXLzI4OEg6nc7XJTwwMgvH5/Oxb98+EokEExMTLCwsqB2Y0WhUQdzi4mIqKytVLCCbzRIOh5mfn2dubo6FhQUWFxd1Rf44mEwm7HY7fr+fN954A7/fz4EDB3C73ZSWlmIymdTiheUFXFxcjBCC559/nrKyMux2OyUlJYTDYcLhMFevXmVycrLggl4y37e4uFjdwCoqKvD7/ezfvx+v14vf76e4uJhsNsvs7CynT59meHiYy5cvEwgESCQSLC0tMT8/n+/LeSgMBgNGo5HGxkZee+01TCaT8pOPjIzg8XjweDyr5sK9XstsNtPR0YHT6cRut7Nnzx56e3sZHh4mnU4/MX5gk8mE0+mkurqaQ4cOUVNTQ1lZGSUlJcr4yWazpNNpurq6mJqaUjGGmzdvMj4+Tm9vL6OjozvGDWW327FYLJSWlqrsrOLiYsrKynC5XCqo7/V6qa2tZXFxkVgsRjqdVq5WabUXFxcrGa20yKUFvrCwQDqdJhqNsrCwsGYsU1NTRCIRZmZmmJmZIR6PMzMzo2T4ODwxilxuj30+H7t27eIP/uAPqKysVC6UjbBarVitVl577TVefvllLBYLJpOJYDDIyMgImUyGs2fPqq1koSCEwGg04nA4aGtro7y8nCNHjuDz+Th27BilpaXKvTA7O8vMzAw//elPuXz5MqOjo8oFUYgYjUaKi4tpb2/nG9/4hnKTTE1Nce3aNRKJBHv27FllYd7rtQwGA88++yz79+9n165dTE5O8uMf/5jp6Wnm5+efGEVuNpvx+/00NjZy/PhxKisrKS8vX7V+stks8/PznD59mu7ubtLpNEtLS9y8eZOxsTFlke4EhBC4XC5KS0tpbGzE7/fj8/nweDy0t7fT1taGw+GgtLQUo9GIyWRSSlV+yR2HEEJ9rax0ls+XNRby9/VcSt3d3Vy/fp3+/n5u3LhBMBgkHo+TyWSebkVeVFSE2WzG4XBQW1uL1+vl0KFD1NbW4vF4Vm2fM5kMmqZhMBjWtcSMRiNCCFWWbrVaKSsrU1vuQqGkpITy8nJcLheNjY2UlZXR2dmJ0+mkoaFBuQ7i8Tg3b94kkUhw+/ZtpqenuXXrFjMzMwWbMiaLVCoqKqitreXAgQM4nU7S6TTJZFI9T96Qs9ksiUSCbDbL0tLSqsWUTqfRNE2l0WUyGbLZLE6nE7PZzJEjRzAYDHR3d3Pjxo2CLkmXO9mqqipefvllGhoalD9crhUpo+7ubiYnJ+nu7ub27dvquuW82UkyKCoqYteuXTQ3N9Pc3Izf71exs4qKClwulzLccrkcCwsLpFIp5ubm1hhuUonLn1ciFbhUylardZWRYDKZMBqNlJSUsHv3bjweDzU1NQwODuL1epmYmKC3t/exlHlBK3I5Aevr63n11VdpbW3lzTffxGazrVLWmqaxuLhILpdTrpS7PwzpB5PY7XYMBgMOh2NVD4mdjtvtZu/evbS2tvLFL34Rr9fLrl27MJlMCCFIp9OEQiGmp6d59913GRoa4qOPPmJycrLgdh13U1FRwe7duzl48CDPP/88VVVVeDweEonEuu6hdDpNOBxW/lBpRcntci6Xo6qqCpvNBiwrBr/frypfX3jhBX7wgx8wPDys0hcLkeLiYqqqqti3bx9/+Id/iNfrxev1qvWwMrPnvffeo6enhzNnzjA29uszH3bivDEajRw/fpyXXnqJlpYW/H4/wLoKOZVKEY/HmZqa4vbt2xsq1Y3qELLZLMFgkGQySXV1NU6nUz1fpj7X1dXR2dmpZHXz5k0++eQTzp8/z61bt55eRS7LyJuamti/fz9VVVVYLBaleBcWFggEAiSTSeWza2lpwel04nQ6191aSwujv7+f/v5+BgYGCkLBlZSU4HK5aGhoYM+ePTQ0NFBZWam2jVKBJ5NJFby8desWwWBQWaWFjt1uV3EAqXBX3rSj0SjXrl2jv79fBbAjkYjyh65cSGazGZPJRDwex2azEYvFWFpa4sSJE7hcLuU3XbnlLjQMBgMmk4nKykpOnDhBS0sLbrd7lSUuc+z7+vqYnJzk5s2bDA4Okkwmd/yayOVy9Pf3Y7PZlIKdnp4mGo2uGfvi4qIKWI6NjT20UtU0TdVWjI6OKm+AwWCgqqqK8vJyteuRWTDSk3D79u3Hnj8Frchra2v56le/SlNTEy+//DJms3lVUGZmZoZ33nmHiYkJwuEwJpOJN998k4aGBlpaWlTO9Epk2uGpU6d4++23GR0dLQgl5/F46OjooKOjg9dffx2fz0dbW5tyGc3NzXHhwgVmZmYIBAJMTU3x8ccfMz09XRDX9yCUl5fT2tpKW1sb7e3taxbH+Pg4b7/9NslkclXm0t2L2mQysWvXLtxuN9XV1VitVvr6+pidnaW8vJz29nb1v4WMyWSitLSUjo4Ovv3tb1NWVkZFRcUq2SwtLTE7O8vPf/5zbt26xalTp9TubaeTyWT48MMPuXTpEkePHqW5uZnz58/T09OzpnOn9GvL4P6jfLbyf+SN3WKxYDabOXDgAG1tbdjtdpqbmzGbzVgsFjweD/v27SMQCDydilxGmPfu3UtTUxOVlZUqUBGPx0kmkwQCAWZmZkilUhiNRurq6pT/eGXU+W4WFhaYm5sjEokQCoV2fOqh1WrFZrNRW1vLrl27aGpqwufzUVpaisFgQNM05ufnicViTE9PEw6HGRwcJBKJKCvU4XBgMplW3QjXQ/r6YHniy9fMNzIAZbFYsNlsalcmc4LHx8e5fv06vb29KrPgXqlx2WxWWVeZTAaz2Uw4HCaZTDI8PExvby+Dg4OMjY0xPDy843zDD4qcN9XV1ZSWlmK329dY4v39/UxOThIIBBgdHSWZTBaEEgfU3M/lcgwNDbG4uKiKle6+gUvFnslkHjpGZDKZMBgMq3rNyMwYm82mdsh+v5+ioiKSySRTU1OEQiEGBgYe2z8OBajIhRDs27eP3//936ehoYFDhw6pkvL5+XmCwSADAwP81V/9FQsLC7S3t+P1ejl+/Dher5e6uro1PvSVzMzMMDIywsDAwD19ZTuF8vJyWlpaeOaZZ/jN3/xNfD4fTU1NKqibSqUIh8MEg0Fu3rzJ6Ogop0+fJhaLkclkMBqN1NbW4nQ68Xq99wzuygwAGfi7fv06H3/8cd4tU5lx4HQ6VQteWP4sBwcHuXDhAt///veZm5t7oEWazWYZGRlRNwhABcLPnDlDNBrl4sWL9PT0qNz7fMvgUfB6vZw4cUIVhlksFoQQ5HI5FhcXCYfD/OQnPyEQCHDy5MmC3L3FYjFisRgzMzMIIVQf/fV4lM9QukisViu1tbWq+LCyspK2tjYqKyvx+/14PB7VXXRoaIiuri4uXLjAO++8owqKHoeCUuQOhwOXy0VNTQ11dXV4vV7V1CmVShGNRhkeHmZmZoba2loAmpubcbvdeL1eXC6XUvp3I9OoxsbG6O3tJRwO7+hJK/1vsmrR6/Wu2m3IrVoikVCZBjK/VQZ8XS6XshhcLhcejweLxbLhezocDux2u4ojaJpGNBplbm6OiYkJMplMXopAjEYjFotF3YzsdjuAsqDHx8eZnZ0lkUg88GKVi13OAekPT6fTKrthfn5eZbcUEjKvvry8XFmK8vpg+Zrn5uaYnp5mZGSEsbExkslkQe465Gdzv3m53gEqK5HrTRYbGo1GbDYbJpMJv99PSUmJCnLKbDH5vHg8TiqVIplMMj8/TyAQ4MaNGwwODj6wcXE/CkqRNzY2cuzYMV544QWOHDmitjTJZJJQKMTQ0BC/+MUvcLvdfOc738HtdlNcXKy23dKqWo9oNEo0GuX999/nZz/7GaFQaJuv7uGwWCyUlJRQW1tLZ2enSrNaqcQBRkZG+Iu/+Avi8bjaOre2tmKz2fjc5z5HRUUFe/fuxe12Y7fb75lbLX1/2WyWbDZLKBTi9ddf5+LFi/zgBz9QzZK2W7HJQG9zczPPPPOM+oyHh4f5xS9+wdDQEOFwWN18HpWVgc1CDG5KHA4HVVVV7N27l1dffRWXy7XK1Tg/P8+NGzfo7+/n9OnTjI+PF0SV5uMgA79SYd+NdJmUlpbyzDPP4PF4aG1tpbS0lObmZpVAYTablZsmGAwSiUS4efMmw8PD9Pf3q749Msi+uLi4KeulYBS5tCBlYr+0KmH5bmkymSgpKVHlxPKOWFRUpCoXs9ksVqtVFYysDOpEo1EmJyeZmppienp6xzfQKikpwe124/f7Va8UmWK4EmlBWCwWFUuQ/SKampooLy/H5/Op7eG9fOQSGRiSvuZQKERzczMTExNKztuJVKxGo1GVTktWHj32KAtGxg1k1W9NTQ3V1dU0NjaqHZzMctjpbjhYlpVsXVBVVYXD4VD9U+5+nqxmlNW/O3mH+qhIS7y8vJy6urp15xAsK3q73Y7D4aC9vR2Xy0VtbS02mw2n04nFYmF+fp5EIqEasMkuof39/QSDQUZHR5mcnCSZTBKPxzf1OgpCkct0nba2Nn7rt34Ll8u1SmHJ/gder5f29nblt5LBjmg0ysmTJ4nFYuzevZuysjKamppUrqemafT29vLpp5/S29vL1NTUjl+UNTU17N+/n5dffnlNa9aVVFdX881vfhOTyURbW5sqgJCLdKUl8qBWpvw8PB6P6mFSVlbGhQsXCAQCO+YmaLfbqa2tJRaLPZIFLRe40+nkC1/4Ai0tLTz33HO0tLTwuc99jlAoxN///d/z9ttvK3fLTkb2FWlvb+ett96ioaEBn8+3yq0CqN4j6XSaw4cP4/f7uXr16qYrn52A2WymuLiYV155hT/5kz9Ru7v1kHKSu3uZXy9v5ufPn2d8fJy+vj7C4TBzc3PK/y3djptRxbkeBaHIJdKSvjvjRFoPsi+G7Cu9sLBAKBRibm6OkZERFhYWaGlpWeUjl77xqakp1Zd8p1seQghKS0uprq7G6/WqG9J6WCwWqqqqVPm17OAGv+5gJyek7B99dxaG2WzGaDRitVopLi5Wj0uZy92Bw+HIi8tB7g5mZ2cZGxtTZdeyfa1sRfCgyCKO4uJitdtpbGyksbFRZQSl02nMZjMulwuTyVQQrgfpNpCuFRmAuztmJD9TOcdyuRyBQGBVBexON3QehqKiIrWbdzgcuN3u+84XmZopb+DRaFRlMY2MjDA9PU0ikVi358pWUBCKXCqbZDLJ5OQkPp9vVWP7XC5HOp1W6UORSET59t5//31isRgmkwmPx8MXv/hFGhsbsVqtaJrG+Pg4kUiEs2fP8sEHHzA3N5fHK70/0jWyd+9evva1r1FRUXHP58tgphBCBXplZoIMYEnl3d3dzdTUlAoYS2RBw4EDB9i/f/9WX+JDE4vFSCQS/PKXv2R6epqXXnqJr3zlK6pt8fT09EMpcovFwvPPP09tbS1vvPEGjY2NeL1eHA6HcunJpks+n4+SkpKCCASazWblHjp8+LBK07wbi8Wi2lwUFRUpH/nIyAjDw8NKQe10g+dBkL1RMpkMi4uLWCwW1cZ4JXfH16RLRtM0dWrWpUuXVjWb286bXUEockDliEt/5Mp+0plMhlQqRTabZWFhgenpaQYGBhgbG2NgYIBUKkVdXR0GgwGLxaICoDJfOBgMMjU1xezs7I5p+LMRMsgpG/7LDI2NkC1bJTI/WO5CFhcX1dfg4KDKM5+dnVX/I/2jOzWnXo5P5rXLqkOLxYLb7cbj8VBRUcH8/PyqYg/pV5fIxWqz2airq6OxsZGGhgbq6urWBIJlUFm6qQoh+CmVj9lsxmazbRgPkTd9AJ/PB0B9fb1aM9FoVNUhFPrhEVKRy3bN0WiURCKxbqxJpi3Lz17OHVmDYbVaV93Ut7MzZkEocmmRf/LJJ4yMjFBXV8fu3bvV3+SHMD8/TygUYmFhQTWwSaVSuFwu1YvF7/djNBpZWloilUrxs5/9jLNnz3Lz5s1HrujaTpqbm2ltbaWjo2NVFd5GyECfJBqNcurUKSYmJjhz5gyRSIRIJKIq2qSLZeX/zM3NEY/HlczXYye0MSgtLaWmpkZV7MoWDi6Xi/r6eoaGhjh58qRyg5jNZioqKjAajWiahtFoVKX9X/jCF1SLg/UsV3mt+b7mrcRkMlFVVYXX66WyspJUKqV2a++++y4DAwPcunVrRxSFPSqZTIZkMsmHH37I9evXVbfLlYq8qKiI0tJSjh49itPpxOfz4XQ6OXz4ME6nU2XR7dmzh1AoxOnTp1UdiqyC3WrrvCAUuWR2dlY1+V/ZRjIej6uqs4mJCSU06Q+Uyfq1tbUqQr+y6i8QCBCNRgvC7yej5fKU8o2Qu5NMJrMqf3p2dlbtVnp6epient6w46HsN5LL5TAajetuw2VPm9nZ2bwXxsjMJWk5S1dCRUUFuVwOi8XCwMCAulbZLEpaVkajkaqqKnXYhtfrBdZX2jJ4JU/BKQSFLjvwrTdv5K4mlUqtcsNJa1PWD5SUlDAzM0Nvby/pdJrJyUlVXFYI6+dupJEoe4SvR1FRkcqCc7lcxGIx1b4hk8moTqsyf3xsbExlysld4Fb7ygtKkctt3PDw8KoTa6R/6+6qLavVyoEDB6ivr+fIkSM0NjbicDjIZrP09PQQDAbVXbMQglVCCA4dOsTv/u7vUllZue5zpAU+NjbG2bNnCQaDfPTRR6RSKTRNI51OMzMzw+LiIjMzMxuWq8v3amlp4cSJExw8eJDq6upVz5G7mkuXLvHd736X6enpvLqmpMKRRU3SlSYbQVVXV9PR0bHKtbIyjVUqMJPJtGHmguT27dsMDg7S29urTobZ6dTX13Ps2DF27dq1yuKUc2F0dJR3330Xk8nEs88+q3Yysg2EwWBQ8am33nqLmZkZfvSjH9HV1cXAwADhcDiPV7d15HI5otEo586dU+mJFouFDz74AJfLxdGjR/H7/Tz77LM0NzdTXl5OMpnkV7/6FX19fZw6dYqrV69u6RgLSpFLJZVMJlf1l94IWXUlt4dut1u1tA2Hw4yMjBTMIpRtdmXj/5V+75UBG1l5GA6HCQQC9Pf388knn6ieExshfX7SCjOZTNTU1NDS0kJrayutra2rMlZgWZFHo1EmJibo7u7Oe3xBplKudDdJn7DsW+/z+dbtJ72SB/F3RyIRBgYG1M2rEPzEsvve3Tcp6U6bmZnh+vXrWCyWVZXC6XRaZYRJC13uWOrr6wmFQkxOTubpqraHdDq9yng0GAzMzc2pIqFkMsmePXvw+XxUVlZSVFREJBJBCEFfXx/FxcUqsWArKChF/qDIAJ/P5+Oll16iubmZ0tJSNE0jFoupbm4XL15kZGQk38O9L0IIOjs7aWxspL29XfVKh2VramFhgdHRUXp6ehgaGuLChQvEYjHVH/nuFq13IxXdvn378Pv9HD9+nMbGRnXCkjzAWLpWpEvhypUr/PznP6enp6cgsjY2Cxmv+dGPfqQCxoXgWjGbzTidTqxW66qbldypzc3Ncf36dVKpFD09PTgcDjo6OvB4PBw7dkylYdrtdpWSeuzYMSoqKojH44RCoSfq6Lt7kcvlVJ74e++9h91u5/bt21RVVXHixAmampqoqamhvr4eIQTV1dVcvnyZrq6uLRnPE6nIZeZBaWmpyjqQpbOyQCgQCNDb21sQk04Igd/vp6WlBY/Hsyp7QrqVIpEItzd52icAAA/ySURBVG/fpqenh1OnTikFL7k7dUpmMMhzPWUcob6+nhdeeIGOjg6Vc76yOlKWFc/PzzM6OsqVK1d2zJmmcnxLS0ssLi6q3hjAGsUlg7Py+90W9cpUzbst9Fwup07JKSRWZtncjWyUFYlEVD6+1WollUrh8/koLy8nlUpRXl6uXAtGo5Hq6mqMRqM6UrEQdiabgcwjX1paUm2Rc7kcPp+Pmpoa7HY7bW1tVFRU0NLSQiKR2FKj8YlU5D6fT3VHlH2lzWYzqVSKM2fOEAgEVJOnnaCA7ocQgvr6eg4dOqTSwSQyQCV9nNlslj179qhI+d3XJxed3W5n3759qoWt0WikublZNSWTKZqw3Md7amqKmzdv0tfXpzrKDQ8P09PTw8LCwo4IdH322WeEw2EuXLjAmTNnOHDgAK+88oracUhmZ2fp6uoimUwSiUSIxWJ0dXWpU4Tsdjvf+MY3aGxspLq6WnVTlBRCquF6JJNJgsEg1dXVaJp23+tYWlpicHCQiYkJVb/x7W9/m/b2dtUyuKKiAofDobJ9NqsJVKGRy+UYHx9nZmaGv/3bv+XkyZN885vf5MUXX8Rms63KptoK7qvIhRC1wH8B/IAGfE/TtP8ghCgD/hZoAIaAf6Fp2uxGr7OdOBwOjh49Sn19PT6fT3VIzGQyDAwM0NPTs9UZFp1CiPfZRJm4XK5Vx45JZDGUTLmU27iNMJvNVFZWUlZWxiuvvLKqus/j8azyg0uLVZ6a0tXVxSeffKKa/iSTyYcpoNp0mdzN5OSk6pczOTmJyWTiueeeI5fLrcq4SSQS9Pf3Mzs7y+joKNPT07z33nvqwGm3283+/ftVZ8mVMl9pxW8CrUKI22zT+pG99h8kvgSozAuAiYkJysrKCIVC1NTUqJ2srICV3+910PmDsp0y2Syk2zYWi7GwsIDNZuP1119naWkJi8WizgfdKh5E6hngX2madlUI4QCu3FmQ/y3woaZp3xVC/BnwZ8C/3bKRPgCyJ7XP56OqqkrljKfTaYaGhpicnOTixYt0d3dvdQVnN/AhmyiT+fl5ZmdnmZ6eVk2vbDabKi0+evQolZWV91UwshxZlu6vzNqQLptEIqEKhKampjh79iyfffYZY2NjTE5OqtLkh3RLbbpMNkI27srlcoRCIRW8ldc5NzfHjRs3WFhYUFWKK/ukzM/P84//+I9cvnyZb33rW3R0dGC32zEajVy/fp3h4WEGBwc3Y6hxTdNat2v9TE1N0dXVRWNj447eiW6HTCwWCxaLRbnh7rzvY7+uwWCgs7OT+vp6GhoasNlsm3Jzux/3fQdN0yaAiTs/x4UQvUA18BXg83ee9n3gY/KsyGXeeGlpqer9YTAYWFhYYHJyUrWSlOdwbjGbKhNpcUejUWX52Gw25TZwOp00NTU99vvIys9EIsHg4CCBQIBf/epXnD9/Xh0o8RhsyzyRubuJRILR0dE1rUnlfNhoDiwtLXH58mUCgQCvvfYaDQ0NWCwWDAYDw8PDfPrpp6syGB6DyJ3v2yIX6Q6LRJbfVlZGr2QHuY22VCayulX2mN8sfVBUVERtbS179uzB6/WqebPVN86HulUIIRqAg8AFwH9HyQNMsux6yQuy0KGhoYGvf/3rNDQ0UFZWppq6y61zf38/U1NT22WNbJpMcrkc58+fJxgM0t7eTl1dHfv27aOzsxOr1brqoNeHQVqt8XicxcVFRkZGiEaj9PT0EAqF1DFfw8PDm1Xwsa3zZHFxkdnZ2VUn/QD3jY0YDAaqq6upqKhQR+bJ6tfr169z6dKlzUq3kwn8eVs/MndenjT1pS99SaWTypthnuIfWyKTsrIynE4nzz33HEeOHOHcuXP88pe/XGWZPwpCCNXA7uDBgzz77LN4vV6y2SyDg4NcvnyZYDC4iVeymgdW5EIIO/AT4E81TYvdlQWgCSHWXRlCiD8C/uhxB3ovpHVaW1vLl7/8Zfx+P06nU7WxjUQinD9/nr6+Pubm5rZFkW+mTDRNo7u7m+7ubsbHx9UBrlVVVTidThWsfBhFLn29Mhc8Ho9z48YNxsfHOXv2LENDQ4RCIeU33gy2e57c72zOjZCFLzImYTAY1Onr/f393LhxY1UvmsclH+tn5fo1mUy43W5qa2s5ceIEg4ODTE5OMjMzc9/6g61iq2TidDqprq7m+PHj/PZv/zaZTIZTp06p+pJHpaioSJ0P0N7ezr59+1Qfd3lj3KRd3Lo8kCIXQphYVuI/0jTtH+48HBJCVGqaNiGEqATWLevSNO17wPfuvM6WaFCLxYLf71dVZ/IknGQySU9PD4ODg4RCIVVKvB1slUxW+qh7enqoqKigtrYWi8Vyz/M21xkD8/PzLC4ucvv2bebm5hgfHycej29ZoVS+58mDYjKZ6OjooLW1lWw2y+joqNrRdXV1EY1GN6v4yQTbJ5d0Ok0ymeT27du8++676gBzmYbqcrk4fPgwTU1NqslYIpFQ7rSSkhIOHjxIRUXFlgbuYOtk4vP5VM+lkpISDh8+zLe+9S0mJydVsziZ/SVdiUtLS2sOaxZCqIO+GxsbcbvdHD16lOrqalpaWiguLlZN6K5cuUJ3d/eWVr4+SNaKAP4a6NU07c9X/OmnwB8A373z/b9uyQgfAHn6TUVFhTroAJa31j09PQQCAaanp7e7Mf6WyCQcDhMOh+nr6wNQKZZWq/Wh0pukS2V+fp5Lly5t2Gdik8nrPHlQpCLv7OxkcXGR0dFR/vmf/5lz585t9lt57nzfFrnIPvP9/f380z/9E0eOHKGjo0Ply0u3QC6X4+jRoyrTSyowWWi3Ud+dTWbTZSKEwOPx0NzcrLLZDh06RH19Pbdu3eLChQsMDAyQTCZV+wnZFkSeiiUpKirCbrer5ll1dXW88cYbKsC5Mp5y5coVbty4sZmXsoYHschfAH4PuC6EkGVJ/45lBf53Qog/BIaBf7E1Q9yY4uJiSktLaW1t5Td+4zdobGzEbDaru6eseBwZGdnu3NZOYI5tkEksFmNoaEgduvGgyK2kTF3cBrZNJo+LdK3U1dWRTqdJpVI0NjaqMxg3sZ2v806q3batH03TmJ6e5rPPPlOHAtfU1HDkyBGsVit2u135zWWztJW9aaT7TvbcDgQChMNhbt++vWk7la2SiewdLrPb3G63ymjy+/0899xztLW1sXv3btXuYn5+nrGxMVVoJmUhi6EcDgf79+/H4/FQUlJCMplUTfjOnTvH9evXt6V9wYNkrZwFNgplv7K5w3k45OHDBw4c4Otf/7pqOSotifn5eQYGBggEAtvdT6Vb07QvbMcb3atr2w5j22TyuBgMBqqqqmhublYGQXt7O+FwmBs3bmymIr+ladozm/ViD4KmaYRCIcLhMNeuXeP06dPKJSCVkexZsxGyiVQsFuOjjz6ip6eHa9euEYlENsWfrmla62O/yAb09/czPDysdrDV1dWqnH7//v1rMnlmZ2f57LPPVIqqvD6z2UxDQ4MqhrJYLExMTBCPxzl//jyBQICPP/6YmzdvbktDvoKs7JSWgdfr5dChQ7S3t6v2pfL0G1k+K7eTOzlvVkdnO5FrIZ1OE4/HGRgY4J133qG8vJxdu3ZRXFxMSUmJ2uVJN4q8qS0uLqqzba9evcrIyAixWKwg1ph0j0hFW15eTmVlJT6fj4aGBjweD7W1tSp5wGg04vV6lYUurzGbzRIKhQgGg1y5coV0Oq2K5KQVHolEtq29b0EqctmJrb6+njfeeIOamhocDseqsyhTqZQK5m33sUs6hY/M6rn7sScF6VpbXFzk008/pbu7G6/XywsvvKAyO6xWK+Xl5co6z2azKtb04YcfMjo6SiQSKYgDWSSyA+GVK1e4evWqatHb0tLCM888Q2dnJ1/60pdUwZDBYKChoWHN68RiMd555x1GRkY4d+4c4XCYeDyueq9sdyO1glTkTqeTmpoaGhsbqampUWcLynL12dlZLl68yPDwsBKwrsh1dNZHrhtZBFZSUsL09DQWi0UV1cnnSb96KBQiHo+vslILCRlHW1paQgihEgikEr67P8/dyCSB6elpdbiGrHbOh0wKUpFXVVXx4osvcvjwYTo7OzGbzapCKxaLEQgE+Mu//EtGR0eVf7wQJ5uOznYhe+pcvnx5VcfHjXq3F8qpSPdD1hoEAgEGBwcpKirihz/8IXDvKlfZMXPlzi2f8ihIRQ6sOg0+Ho8ry3toaIjh4WEmJiaYm5vT/eM6D408lHtyclIdCJBIJEgmkwVxktTj8LTuXKViLtQ2vAWryOWJOKlUimAwyPvvv8/IyAgnT54kGo0SCoUK9hxBnfySTqfp7e3FYDBQWVmJ0WhkbGyM8fFx1epWR2cnUZCKPJFIqMb3MtDQ19dHKBRSwRddies8KrJoRp59ajAYmJycVPNKR2enIbbT7bBZpdcya0UehCrTDTcqp80DVx40Pzjf5ejbSMHIRAixqoJRCMH8/LwKYm3i3HpgmdwZ11MxVzRNe+AWjE+LTLjPXClIi1yeiqOjsxXIPjQ6OoXCljdM0NHR0dHZWnRFrqOjo1PgbLdrZRpI3vn+JFDO+tdS/xCv8aTJBNaXiy6Tx5MJPHly0WWylkfSKdsa7AQQQlze7kZBW8VmXcuTJBPYnOvRZbK1r7MT0GWylke9Ft21oqOjo1Pg6IpcR0dHp8DJhyL/Xh7ec6vYrGt5kmQCm3M9uky29nV2ArpM1vJI17LtPnIdHR0dnc1Fd63o6OjoFDjbpsiFEK8LIW4KIfqFEH+2Xe+7WQghaoUQJ4UQPUKIG0KIf3nn8X8vhBgTQnTd+friQ75uwcpFl8ladJmsz1bIRZfJCmTviK38AgxAAGgCzMA1YM92vPcmXkMlcOjOzw7gFrAH+PfAv34a5aLLRJdJvuSiy2T113ZZ5EeAfk3TBjRNWwJ+DHxlm957U9A0bULTtKt3fo4DvUD1Y75sQctFl8ladJmszxbIRZfJCrZLkVcDoyt+D/L4kztvCCEagIPAhTsPfUcI8ZkQ4m+EEO6HeKknRi66TNaiy2R9NkkuukxWoAc7HxIhhB34CfCnmqbFgP8HaAYOABPA/5XH4eUFXSZr0WWyPrpc1rIZMtkuRT4G1K74vebOYwWFEMLEssB/pGnaPwBomhbSNC2raVoO+H9Z3vI9KAUvF10ma9Flsj6bLBddJivYLkV+CWgVQjQKIczAW8BPt+m9NwWxfBLrXwO9mqb9+YrHK1c87WtA90O8bEHLRZfJWnSZrM8WyEWXyQq2pfuhpmkZIcR3gF+yHG3+G03TbmzHe28iLwC/B1wXQnTdeezfAb8jhDgAaMAQ8N8/6As+AXLRZbIWXSbrs6ly0WWyGr2yU0dHR6fA0YOdOjo6OgWOrsh1dHR0Chxdkevo6OgUOLoi19HR0SlwdEWuo6OjU+DoilxHR0enwNEVuY6Ojk6BoytyHR0dnQLn/wekyN/NP42ueAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjAHuOg7xDKk",
        "colab_type": "text"
      },
      "source": [
        "#### **Test Accuracy:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAn8MFZnp-YR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87fcd004-37be-4fe6-aef1-031ba77ccd7e"
      },
      "source": [
        "print(f'Test Accuracy: {(MNIST_net.predict(X_test)==y_test).mean()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}